{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPWZkgGzafZqj4cNMXTqR5d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StratosFair/Mean_Escape_Time/blob/main/OU_process_2D/Models/boundary_adapted%5BWIP%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports & FP64\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.integrate as integrate\n",
        "import scipy.special as special\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.set_default_dtype(torch.float64)"
      ],
      "metadata": {
        "id": "zWfcku6GqTt1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solving for the MET of an Ornstein-Uhlenbeck process in a disk with \"Adapted Architecture PINNs\" : comparison with exact solution"
      ],
      "metadata": {
        "id": "wDHeduNpyl5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Setup\n",
        "\n",
        "For $\\theta,\\sigma,r >0$ some fixed parameters, we let $\\Omega := B_r \\equiv \\{x\\in\\mathbb R^d : \\|x\\|< r \\}$, and define the process ($d=2$ in our illustration) :\n",
        "\n",
        "$$\\begin{cases} dX_t &= -\\theta X_t dt + \\sigma dB_t \\\\\n",
        "X_0 &= x \\in \\Omega \\end{cases} $$\n",
        "\n",
        "For all $x\\in\\Omega$, let\n",
        "\n",
        "$$T(x) := \\inf\\{t\\ge 0 : X_t \\in\\partial\\Omega\\} $$\n",
        "\n",
        "and let its first moment be denoted\n",
        "\n",
        "$$\\tau(x) := \\mathbb E[T(x)] $$\n",
        "\n",
        "We can show under some regularity conditions on $\\Omega$ that $\\tau$ is the (unique) solution of the BVP :\n",
        "\n",
        "$$\\begin{cases} -\\mathcal{L}u(x) &= 1 \\text{ for all } x\\in\\Omega \\\\\n",
        "u(x) &= 0 \\text{ for all } x\\in\\partial\\Omega \\end{cases} $$\n",
        "\n",
        "where $\\mathcal L$ is the infinitesimal generator of the Ornstein-Uhlenbeck process, given by\n",
        "$$\\mathcal Lu : x \\mapsto -\\theta x \\cdot \\nabla u(x) + \\frac{\\sigma^2}{2}\\Delta u(x) $$"
      ],
      "metadata": {
        "id": "EpGLkz71ym36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nicely enough, for this problem, we can compare our solution with the known closed-form solution (see https://arxiv.org/abs/2208.04029) :\n",
        "\n",
        "$$ \\tau(x) := \\frac{1}{\\lambda^{d/2}\\sigma^2}\\int_\\rho^r z^{1-d} e^{\\lambda z^2} \\gamma(d/2, \\lambda z^2)\\ dz $$\n",
        "\n",
        "where $\\lambda := \\theta/\\sigma^2 $, $\\rho := \\|x\\| $ and $\\gamma$ is the upper incomplete gamma function :\n",
        "$$\\gamma(n,y) := \\int_0^y t^{n-1} e^{-t}\\ dt.  $$"
      ],
      "metadata": {
        "id": "ah5DvZTq7uP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem Parameters & True Solution\n",
        "R = 2.0\n",
        "theta = 1.0\n",
        "sigma = 1.0\n",
        "\n",
        "def true_tau(x, y, theta=theta, sigma=sigma, R=R):\n",
        "    lam = theta / sigma**2\n",
        "    rho = np.sqrt(x**2 + y**2)\n",
        "    integrand = lambda t: np.exp(lam * t**2) * special.gammainc(1, lam * t**2) / t\n",
        "    I, _ = integrate.quad(integrand, rho, R)\n",
        "    return I / theta\n",
        "\n",
        "true_tau_vec = np.vectorize(true_tau)"
      ],
      "metadata": {
        "id": "1lyRR4sIqUbT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Neural Network architecture and loss function\n",
        "\n",
        "Unlike the standard PINN, we will take an hypothesis space of Neural Networks which satisfy the boundary conditions explicitly. For this problem, we have homogenous zero Dirichlet boundary conditions, which we can enforce by multiplying our Neural Networks with an appropriate \"smooth distance approximation\" (see https://arxiv.org/abs/2104.08426). In accordance with the mentioned paper, we will take\n",
        "$$\\varphi : x\\mapsto \\frac{R^2 - \\|x\\|^2}{2R}$$\n",
        "as our smooth distance approximation.  \n",
        "\n",
        "\n",
        "With this modification, our objective to minimize becomes\n",
        "$$\\hat u := \\arg\\min_{u\\in\\mathcal{NN}_\\varphi}\\ \\frac1n \\sum_{i=1}^n (\\mathcal L u(x_i^c) + 1)^2 $$\n",
        "where $x_i^c$ are sampled i.i.d. with uniform distribution on $\\Omega$,\n",
        "$$\\mathcal{NN}_\\varphi:=\\left\\{x\\mapsto \\varphi(x) \\cdot T_L\\circ \\sigma \\circ T_{L-1}\\circ\\cdots\\circ \\sigma\\circ T_1 (x)\\right\\}, $$\n",
        "for\n",
        "$$T_\\ell : \\mathbb{R}^{\\ell-1}\\to\\mathbb{R}^\\ell $$\n",
        "affine-linear maps of appropriate input-output dimensions, where\n",
        "$$\\sigma :x \\mapsto \\begin{cases}x^2 &\\text{ if } x\\ge 0\\\\ 0 &\\text{ if } x\\le 0\\end{cases} \\quad \\text{OR }\\ \\ \\sigma : x\\mapsto \\operatorname{tanh}(x),$$\n",
        "is either the ReQU or hyperbolic tangent activation function, which is understood element-wise when applied to vectors."
      ],
      "metadata": {
        "id": "ipb5U7KL7yL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining PINN w/ matching boundary condition\n",
        "power = None #exponent k for relu^k, none for tanh\n",
        "width = 50\n",
        "depth = 3\n",
        "gain = 1.0 #magnitude of weights at initialization\n",
        "\n",
        "#define ReLU^k activation\n",
        "\n",
        "class RePU(nn.Module):\n",
        "    def __init__(self, power = power):\n",
        "        super(RePU, self).__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.pow(torch.relu(x), self.power)\n",
        "\n",
        "#define the smooth distance approximation\n",
        "def smooth_distance(x, exponent):\n",
        "    norm_x = torch.linalg.norm(x, dim=-1)\n",
        "    dist =  (R**2 - norm_x**2)/(2*R)\n",
        "    return dist**exponent\n",
        "\n",
        "#define hypothesis space\n",
        "class BoundaryPINN(nn.Module):\n",
        "    def __init__(self, power = power, width = width, depth = depth):\n",
        "        super(BoundaryPINN,self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(2, width))\n",
        "        if isinstance(power, int):\n",
        "            self.layers.append(RePU(power))\n",
        "            for _ in range(depth-1) :\n",
        "                self.layers.append(nn.Linear(width, width))\n",
        "                self.layers.append(RePU(power))\n",
        "        else :\n",
        "            self.layers.append(nn.Tanh())\n",
        "            for _ in range(depth-1) :\n",
        "                self.layers.append(nn.Linear(width, width))\n",
        "                self.layers.append(nn.Tanh())\n",
        "        self.layers.append(nn.Linear(width, 1))\n",
        "        self.mlp = nn.Sequential(*self.layers)\n",
        "\n",
        "        #for the exponent in the distance function\n",
        "        self.exponent = nn.Parameter(torch.tensor(1.0, dtype= torch.float64))\n",
        "\n",
        "        #for dynamic weighting, first biased towards data fidelity\n",
        "        self.log_sigma_pde = nn.Parameter(torch.tensor(-0.5,\\\n",
        "                                                        dtype=torch.float64))\n",
        "        self.log_sigma_sobolev = nn.Parameter(torch.tensor(np.log(100),\n",
        "                                                        dtype=torch.float64))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "        distance =  smooth_distance(x, self.exponent)\n",
        "        return output * distance.unsqueeze(-1)\n",
        "\n",
        "    def raw_forward(self, x):\n",
        "        #returns the raw neural network output without multiplication by\n",
        "        #distance to boundary\n",
        "        return self.mlp(x)\n",
        "\n",
        "#weight initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_normal_(m.weight, gain=gain)\n",
        "        m.bias.data.fill_(gain)"
      ],
      "metadata": {
        "id": "kNAMxUlRqa3D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining PDE Residual & Loss, enabling grad on x\n",
        "def pde_residual(model, x, theta, sigma, forcing = None):\n",
        "    # ensure x is on the right device, float64, and requires grad\n",
        "    x = x.clone().detach().to(device).requires_grad_(True)\n",
        "\n",
        "    # forward through PINN\n",
        "    tau = model(x)                                # [N]\n",
        "    # first derivatives\n",
        "    grads = torch.autograd.grad(\n",
        "        tau, x,\n",
        "        grad_outputs=torch.ones_like(tau),\n",
        "        create_graph=True,\n",
        "    )[0]                                        # [N,2]\n",
        "    tau_x, tau_y = grads[:,0], grads[:,1]\n",
        "\n",
        "    # second derivatives\n",
        "    tau_xx = torch.autograd.grad(\n",
        "        tau_x, x,\n",
        "        grad_outputs=torch.ones_like(tau_x),\n",
        "        create_graph=True\n",
        "    )[0][:,0]\n",
        "    tau_yy = torch.autograd.grad(\n",
        "        tau_y, x,\n",
        "        grad_outputs=torch.ones_like(tau_y),\n",
        "        create_graph=True\n",
        "    )[0][:,1]\n",
        "\n",
        "    # OU‐generator L[tau]\n",
        "    phys = -theta*(x[:,0]*tau_x + x[:,1]*tau_y) \\\n",
        "           + 0.5*sigma**2*(tau_xx + tau_yy)\n",
        "\n",
        "    if forcing is None:\n",
        "        # default constant −1\n",
        "        f_vals = -1.0\n",
        "    elif isinstance(forcing, (int, float)):\n",
        "        # constant forcing = that number\n",
        "        f_vals = float(forcing)\n",
        "    elif callable(forcing):\n",
        "        # call it on x\n",
        "        f_vals = forcing(x)\n",
        "        # flatten to [N]\n",
        "        if f_vals.dim() > 1:\n",
        "            f_vals = f_vals.view(-1)\n",
        "    else:\n",
        "        raise ValueError(f\"forcing must be None, float, or callable, got {type(forcing)}\")\n",
        "\n",
        "    # turn any scalar into a tensor of shape [N]\n",
        "    if isinstance(f_vals, float) or isinstance(f_vals, int):\n",
        "        f_vals = x.new_full((x.shape[0],), float(f_vals))\n",
        "\n",
        "    # 7) residual = L[tau] - f(x)\n",
        "    r = phys - f_vals              # shape [N]\n",
        "    return r\n",
        "\n",
        "def loss_fn(res):\n",
        "    return torch.mean(res.pow(2))"
      ],
      "metadata": {
        "id": "RDSBmM64qd26"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def empirical_h2_norm(model, x_batch):\n",
        "    \"\"\"\n",
        "    Empirical H² norm of the *raw* NN output u(x) = model.raw_forward(x)\n",
        "    computed exactly like your pde_residual (component-wise gradients).\n",
        "\n",
        "    Returns a scalar tensor (mean over the batch).\n",
        "    \"\"\"\n",
        "    # ------------------------------------------------------------------\n",
        "    # 1) Prepare the input exactly as in pde_residual\n",
        "    # ------------------------------------------------------------------\n",
        "    x = x_batch.clone().detach().requires_grad_(True)   # shape [N,2]\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 2) 0-th order term :  u²\n",
        "    # ------------------------------------------------------------------\n",
        "    u = model.raw_forward(x)                # [N,1]\n",
        "    u_sq = torch.mean(u.pow(2))              # scalar\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 3) 1-st order term :  |∇u|²  =  u_x² + u_y²\n",
        "    # ------------------------------------------------------------------\n",
        "    grad_u = torch.autograd.grad(\n",
        "        u, x,\n",
        "        grad_outputs=torch.ones_like(u),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]                                     # [N,2]\n",
        "    u_x, u_y = grad_u[:, 0], grad_u[:, 1]\n",
        "\n",
        "    grad_sq = torch.mean(u_x.pow(2) + u_y.pow(2))   # scalar\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 4) 2-nd order term :  ||∇²u||_F²  =  u_xx² + u_xy² + u_yx² + u_yy²\n",
        "    # ------------------------------------------------------------------\n",
        "    #   u_xx\n",
        "    u_xx = torch.autograd.grad(\n",
        "        u_x, x,\n",
        "        grad_outputs=torch.ones_like(u_x),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0][:, 0]                               # [N]\n",
        "\n",
        "    #   u_yy\n",
        "    u_yy = torch.autograd.grad(\n",
        "        u_y, x,\n",
        "        grad_outputs=torch.ones_like(u_y),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0][:, 1]                               # [N]\n",
        "\n",
        "    #   mixed derivatives u_xy = ∂/∂y (u_x)  and  u_yx = ∂/∂x (u_y)\n",
        "    #   (they are equal for C² functions, but we compute both to be safe)\n",
        "    u_xy = torch.autograd.grad(\n",
        "        u_x, x,\n",
        "        grad_outputs=torch.ones_like(u_x),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0][:, 1]                               # ∂/∂y of u_x  → [N]\n",
        "\n",
        "    u_yx = torch.autograd.grad(\n",
        "        u_y, x,\n",
        "        grad_outputs=torch.ones_like(u_y),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0][:, 0]                               # ∂/∂x of u_y  → [N]\n",
        "\n",
        "    hessian_fro_sq = torch.mean(\n",
        "        u_xx.pow(2) + u_xy.pow(2) + u_yx.pow(2) + u_yy.pow(2)\n",
        "    )                                        # scalar\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 5) Assemble the full H² norm (mean over the batch)\n",
        "    # ------------------------------------------------------------------\n",
        "    h2 = u_sq + grad_sq + hessian_fro_sq\n",
        "    return h2"
      ],
      "metadata": {
        "id": "fxKIqN7gO4PQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Computing an approximate data penalty regularization\n",
        "\n",
        "We sample a small subset $x_1, \\ldots, x_{n_{data}} \\in \\Omega$ of points for which we compute approximate values $\\tau^{MC}(x_1), \\ldots, \\tau^{MC}(x_{n_{data}}) $ by Monte Carlo. we will then use them to define a \"data-fidelity\" penalty term in the loss function:\n",
        "$$\\text{Penalty}(\\hat\\tau_{NN}) = \\frac{\\lambda_{data}}{n_{data}} \\sum_{i=1}^n \\left(\\tau^{MC}(x_i) - \\hat\\tau_{NN}(x_i)\\right)^2, $$\n",
        "where $\\lambda_{data}$ is a positive constant."
      ],
      "metadata": {
        "id": "xA77CH6N03KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating some approximate true solutions in the domain\n",
        "\n",
        "#sampling anchor points, biased towards center (failure zone)\n",
        "def sample_disk(batch_size, R, radius_frac=0.6):\n",
        "    \"\"\"\n",
        "    Sample uniformly in the DISK of radius (radius_frac * R).\n",
        "    This is the simplest way to concentrate points near the centre.\n",
        "    \"\"\"\n",
        "    r = torch.sqrt(torch.rand(batch_size, device=device))  # uniform‐disk radius ∝ sqrt(U)\n",
        "    theta = 2 * torch.pi * torch.rand(batch_size, device=device)\n",
        "    x = torch.stack([r * torch.cos(theta), r * torch.sin(theta)], dim=1)\n",
        "    return radius_frac * R * x  # now uniform in disk of radius radius_frac*R\n",
        "\n",
        "\n",
        "# 2) monte-carlo (euler-maruyama) approximation of tau at these points\n",
        "@torch.no_grad()\n",
        "def mc_exit_time(x0, theta, sigma, R,\n",
        "                 n_paths=128, dt=1e-3, max_steps=20000):\n",
        "    \"\"\"\n",
        "    x0: [B,2] starting points\n",
        "    Returns tau_hat: [B] = mean exit time from each x0.\n",
        "    Any path still 'alive' after max_steps is counted as having exit time = max_steps*dt.\n",
        "    \"\"\"\n",
        "    B = x0.shape[0]\n",
        "    P = n_paths\n",
        "\n",
        "    # Expand to per‐path positions\n",
        "    X = x0.unsqueeze(1).expand(B, P, 2).clone()  # [B,P,2]\n",
        "    t = torch.zeros(B, P, device=device)         # running clock\n",
        "    alive = torch.ones(B, P, dtype=torch.bool, device=device)\n",
        "    sqrt_dt = sigma * (dt ** 0.5)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        if not alive.any():\n",
        "            break\n",
        "\n",
        "        # Drift + noise, but only update those still alive\n",
        "        X_alive = X[alive]\n",
        "        drift    = -theta * X_alive * dt\n",
        "        noise    = sqrt_dt * torch.randn_like(X_alive)\n",
        "        X[alive] += drift + noise\n",
        "        t[alive] += dt\n",
        "\n",
        "        # Mark newly‐exited paths\n",
        "        just_exited = (X[alive].pow(2).sum(dim=1) >= R*R)\n",
        "        idx_alive   = alive.nonzero(as_tuple=False)  # [[i1,j1], [i2,j2], ...]\n",
        "        exited_idx  = idx_alive[just_exited]\n",
        "        alive[exited_idx[:,0], exited_idx[:,1]] = False\n",
        "\n",
        "    # Paths still alive get t = max_steps*dt automatically from the loop\n",
        "    tau_hat = t.mean(dim=1)  # [B]\n",
        "    return tau_hat  # no gradient flows back"
      ],
      "metadata": {
        "id": "wWM1g0wlzSPW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining Uniform + Adaptive Sampling functions in the disk\n",
        "def sample_uniform_disk(N, R=R):\n",
        "    theta = 2*np.pi * torch.rand(N, dtype=torch.float64, device=device)\n",
        "    rho = R    * torch.sqrt(torch.rand(N, dtype=torch.float64, device=device))\n",
        "    return torch.stack([rho*torch.cos(theta), rho*torch.sin(theta)], dim=1)  # [N,2]\n",
        "\n",
        "def make_adaptive_batch(\n",
        "    model, pool_size, batch_size,\n",
        "    forcing, theta, sigma,\n",
        "    hard_frac=0.1,    # fraction of batch from highest residuals\n",
        "    use_soft=False,   # if True, sample soft rather than top‐K\n",
        "    beta=1.0          # exponent for soft sampling\n",
        "):\n",
        "    # candidate pool\n",
        "    Xp = sample_uniform_disk(pool_size)\n",
        "\n",
        "    # 2) Compute residuals _with_ gradient‐tracking so second‐derivatives work\n",
        "    r_all = pde_residual(model, Xp, theta, sigma, forcing)  # [pool_size]\n",
        "    # 3) Detach and take absolute\n",
        "    Rvals = r_all.detach().abs()                             # [pool_size]\n",
        "\n",
        "    # number of hard points\n",
        "    K     = int(batch_size * hard_frac)\n",
        "    if use_soft or beta < 10.0 :\n",
        "        # 2a) soft sampling: probabilities ∝ (residual^beta)\n",
        "        weights = Rvals.pow(beta)\n",
        "        weights = weights / (weights.sum() + 1e-12)\n",
        "        hard_idx = torch.multinomial(weights,\n",
        "                                     num_samples=K,\n",
        "                                     replacement=False)\n",
        "    else:\n",
        "        # 2b) hard sampling: pure top‐K\n",
        "        hard_idx = torch.topk(Rvals, K).indices # [K]\n",
        "\n",
        "    all_idx  = torch.arange(pool_size, device=device)\n",
        "    mask     = torch.ones(pool_size, dtype=torch.bool, device=device)\n",
        "    mask[hard_idx] = False\n",
        "    rest_idx = all_idx[mask]\n",
        "    n_rest   = batch_size - K\n",
        "    rnd      = rest_idx[torch.randperm(rest_idx.numel(), device=device)[:n_rest]]\n",
        "\n",
        "    batch_idx = torch.cat([hard_idx, rnd], dim=0)                # [batch_size]\n",
        "    return Xp[batch_idx]                                      # [batch_size,2]"
      ],
      "metadata": {
        "id": "IJI5xaOUqom1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Training loop"
      ],
      "metadata": {
        "id": "gkRR91T6_atw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 6.1) Training Parameters"
      ],
      "metadata": {
        "id": "Qo8UoTLI_lue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "pool_size        = 4096\n",
        "n_batches        = 16\n",
        "batch_size       = pool_size // n_batches\n",
        "n_epochs         = 40_000\n",
        "learning_rate    = 5e-3\n",
        "beta = 1 # 0 -> no adaptive sampling, infty -> top K sampling\n",
        "\n",
        "\n",
        "# model instantiation\n",
        "model = BoundaryPINN().to(device)\n",
        "model.apply(init_weights)\n",
        "\n",
        "opt   = torch.optim.Adam(list(model.parameters()), lr=learning_rate)\n",
        "\n",
        "# scheduler: every `step_size` epochs multiply lr by gamma\n",
        "num_decay_stages = 100\n",
        "epochs_between_decay = n_epochs // num_decay_stages\n",
        "num_decays           = n_epochs // epochs_between_decay\n",
        "final_factor = 1e-3\n",
        "gamma = math.exp(math.log(final_factor) / num_decays)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    opt,\n",
        "    step_size=epochs_between_decay,  # decay every N epochs\n",
        "    gamma=gamma                      # LR *= gamma at each decay\n",
        ")"
      ],
      "metadata": {
        "id": "Oq-NigAc8Ag2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2) Generating the \"data-fidelity\" samples"
      ],
      "metadata": {
        "id": "7iyHryGJ_pU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample x_data\n",
        "x_data = sample_disk(batch_size = 0, #batch_size//2,\n",
        "                     R = R,\n",
        "                     radius_frac = 0.8)\n",
        "\n",
        "# precompute tau_data once (theta, sigma are fixed PDE params)\n",
        "tau_data = mc_exit_time(\n",
        "    x0       = x_data,\n",
        "    theta    = theta,\n",
        "    sigma    = sigma,\n",
        "    R        = R,\n",
        "    n_paths  = 256,\n",
        "    dt       = 1e-3,\n",
        "    max_steps= 10_000\n",
        ")"
      ],
      "metadata": {
        "id": "kepv7k3TzfBU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4) Start the Training with Adam"
      ],
      "metadata": {
        "id": "RSYL1T67_73u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop w/ Homotopy & Loss Logging\n",
        "\n",
        "# prep for best‐model tracking\n",
        "best_loss       = float(\"inf\")\n",
        "best_model_path = \"best_adam.pth\"\n",
        "best_epoch = 0\n",
        "eval_Xb = sample_uniform_disk(pool_size, R=R) #to evaluate the model\n",
        "\n",
        "total_history = []\n",
        "pde_history   = []\n",
        "\n",
        "model.train()\n",
        "for ep in range(1, n_epochs+1):\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    Xb = sample_uniform_disk(batch_size)# → [batch_size,2]\n",
        "\n",
        "    # 3) PDE residual + loss\n",
        "    r        = pde_residual(model, Xb, theta, sigma, forcing = None)\n",
        "    loss_pde = loss_fn(r)\n",
        "\n",
        "    ## 4) data‐loss (fixed x_data, tau_data)\n",
        "    #tau_pred  = model(x_data).squeeze(-1)\n",
        "    #loss_data = (tau_pred - tau_data).pow(2).mean()\n",
        "\n",
        "    # 4) Sobolev penalty on the *raw* NN output\n",
        "    h2_reg = empirical_h2_norm(model, Xb)   # <-- new term\n",
        "\n",
        "    # 3) total loss\n",
        "    #total_loss = pde_loss + lambda_h2 * h2_reg\n",
        "\n",
        "    # 5) total + backward + step\n",
        "    loss_total = torch.exp(-2 * model.log_sigma_pde) * loss_pde \\\n",
        "               + torch.exp(-2 * model.log_sigma_sobolev) * h2_reg \\\n",
        "               + model.log_sigma_pde + model.log_sigma_sobolev\n",
        "\n",
        "    # 6) backward + step\n",
        "    loss_total.backward()\n",
        "    opt.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    #eval\n",
        "    r_eval        = pde_residual(model, eval_Xb, theta, sigma, forcing = None)  #evaluate the model on the true PDE\n",
        "    loss_eval     = torch.exp(-2 * model.log_sigma_pde) * loss_fn(r_eval).item() \\\n",
        "                  + torch.exp(-2 * model.log_sigma_sobolev) * h2_reg\\\n",
        "                  + model.log_sigma_pde + model.log_sigma_sobolev\n",
        "\n",
        "    if loss_eval < best_loss:\n",
        "        best_loss = loss_eval - \\\n",
        "                (model.log_sigma_pde.detach() + model.log_sigma_sobolev.detach().item())\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        best_epoch = ep\n",
        "\n",
        "\n",
        "    # 7) logging & histories\n",
        "    total_history.append(loss_total.item() - \\\n",
        "            (model.log_sigma_pde.detach() + model.log_sigma_sobolev.detach()).item())\n",
        "    pde_history.append(loss_pde.item())\n",
        "\n",
        "    if ep % 100 == 0:\n",
        "        print(\n",
        "            f\"[ep {ep:4d}/{n_epochs}] \"\n",
        "            f\"loss_pde={loss_pde:.2e}  \"\n",
        "            f\"h2_reg={h2_reg:.2e}\"\n",
        "        )\n",
        "    if ep % 500 == 0:\n",
        "        lr = scheduler.get_last_lr()[0]\n",
        "        print(f\"  → epoch {ep:4d},  lr={lr:.2e}\")"
      ],
      "metadata": {
        "id": "wMX05ifLq36Z",
        "collapsed": true,
        "outputId": "cdc05205-c9ea-41ad-f148-deecfbfb5d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ep  100/40000] loss_pde=1.12e+00  h2_reg=7.90e-02\n",
            "[ep  200/40000] loss_pde=9.79e-01  h2_reg=3.99e-03\n",
            "[ep  300/40000] loss_pde=3.38e+00  h2_reg=8.41e+00\n",
            "[ep  400/40000] loss_pde=2.58e+00  h2_reg=3.22e+00\n",
            "[ep  500/40000] loss_pde=8.79e-01  h2_reg=2.53e+00\n",
            "  → epoch  500,  lr=4.67e-03\n",
            "[ep  600/40000] loss_pde=9.22e-01  h2_reg=2.42e+00\n",
            "[ep  700/40000] loss_pde=8.10e-01  h2_reg=2.29e+00\n",
            "[ep  800/40000] loss_pde=7.35e-01  h2_reg=2.13e+00\n",
            "[ep  900/40000] loss_pde=8.27e-01  h2_reg=1.95e+00\n",
            "[ep 1000/40000] loss_pde=6.89e-01  h2_reg=1.75e+00\n",
            "  → epoch 1000,  lr=4.35e-03\n",
            "[ep 1100/40000] loss_pde=6.97e-01  h2_reg=1.53e+00\n",
            "[ep 1200/40000] loss_pde=6.47e-01  h2_reg=1.31e+00\n",
            "[ep 1300/40000] loss_pde=5.27e-01  h2_reg=1.11e+00\n",
            "[ep 1400/40000] loss_pde=6.07e-01  h2_reg=9.14e-01\n",
            "[ep 1500/40000] loss_pde=6.23e-01  h2_reg=7.24e-01\n",
            "  → epoch 1500,  lr=4.06e-03\n",
            "[ep 1600/40000] loss_pde=5.82e-01  h2_reg=5.43e-01\n",
            "[ep 1700/40000] loss_pde=5.97e-01  h2_reg=3.82e-01\n",
            "[ep 1800/40000] loss_pde=6.76e-01  h2_reg=2.33e-01\n",
            "[ep 1900/40000] loss_pde=7.22e-01  h2_reg=1.04e-01\n",
            "[ep 2000/40000] loss_pde=8.72e-01  h2_reg=1.84e-02\n",
            "  → epoch 2000,  lr=3.54e-03\n",
            "[ep 2100/40000] loss_pde=9.83e-01  h2_reg=3.16e-04\n",
            "[ep 2200/40000] loss_pde=9.96e-01  h2_reg=3.36e-05\n",
            "[ep 2300/40000] loss_pde=9.99e-01  h2_reg=1.20e-05\n",
            "[ep 2400/40000] loss_pde=1.00e+00  h2_reg=5.80e-06\n",
            "[ep 2500/40000] loss_pde=1.00e+00  h2_reg=2.39e-06\n",
            "  → epoch 2500,  lr=3.30e-03\n",
            "[ep 2600/40000] loss_pde=1.00e+00  h2_reg=1.13e-06\n",
            "[ep 2700/40000] loss_pde=1.00e+00  h2_reg=6.16e-07\n",
            "[ep 2800/40000] loss_pde=1.00e+00  h2_reg=8.68e-07\n",
            "[ep 2900/40000] loss_pde=1.00e+00  h2_reg=2.28e-07\n",
            "[ep 3000/40000] loss_pde=1.00e+00  h2_reg=1.45e-07\n",
            "  → epoch 3000,  lr=3.08e-03\n",
            "[ep 3100/40000] loss_pde=1.00e+00  h2_reg=1.42e-07\n",
            "[ep 3200/40000] loss_pde=1.00e+00  h2_reg=9.13e-08\n",
            "[ep 3300/40000] loss_pde=9.99e-01  h2_reg=6.80e-07\n",
            "[ep 3400/40000] loss_pde=1.00e+00  h2_reg=6.18e-08\n",
            "[ep 3500/40000] loss_pde=1.00e+00  h2_reg=5.40e-08\n",
            "  → epoch 3500,  lr=2.88e-03\n",
            "[ep 3600/40000] loss_pde=9.96e-01  h2_reg=1.38e-05\n",
            "[ep 3700/40000] loss_pde=1.00e+00  h2_reg=3.19e-08\n",
            "[ep 3800/40000] loss_pde=1.00e+00  h2_reg=3.60e-08\n",
            "[ep 3900/40000] loss_pde=9.99e-01  h2_reg=8.87e-07\n",
            "[ep 4000/40000] loss_pde=1.00e+00  h2_reg=2.25e-08\n",
            "  → epoch 4000,  lr=2.51e-03\n",
            "[ep 4100/40000] loss_pde=1.00e+00  h2_reg=1.89e-08\n",
            "[ep 4200/40000] loss_pde=1.00e+00  h2_reg=9.82e-07\n",
            "[ep 4300/40000] loss_pde=1.00e+00  h2_reg=1.19e-08\n",
            "[ep 4400/40000] loss_pde=1.00e+00  h2_reg=1.09e-08\n",
            "[ep 4500/40000] loss_pde=9.77e-01  h2_reg=5.80e-04\n",
            "  → epoch 4500,  lr=2.34e-03\n",
            "[ep 4600/40000] loss_pde=1.00e+00  h2_reg=1.52e-08\n",
            "[ep 4700/40000] loss_pde=1.00e+00  h2_reg=7.99e-09\n",
            "[ep 4800/40000] loss_pde=1.00e+00  h2_reg=2.72e-08\n",
            "[ep 4900/40000] loss_pde=1.00e+00  h2_reg=7.22e-09\n",
            "[ep 5000/40000] loss_pde=1.00e+00  h2_reg=6.17e-09\n",
            "  → epoch 5000,  lr=2.18e-03\n",
            "[ep 5100/40000] loss_pde=1.00e+00  h2_reg=5.47e-09\n",
            "[ep 5200/40000] loss_pde=1.00e+00  h2_reg=1.09e-05\n",
            "[ep 5300/40000] loss_pde=1.00e+00  h2_reg=4.30e-09\n",
            "[ep 5400/40000] loss_pde=1.00e+00  h2_reg=4.18e-09\n",
            "[ep 5500/40000] loss_pde=1.00e+00  h2_reg=2.69e-08\n",
            "  → epoch 5500,  lr=2.04e-03\n",
            "[ep 5600/40000] loss_pde=1.00e+00  h2_reg=3.66e-09\n",
            "[ep 5700/40000] loss_pde=1.00e+00  h2_reg=5.45e-09\n",
            "[ep 5800/40000] loss_pde=1.00e+00  h2_reg=8.95e-08\n",
            "[ep 5900/40000] loss_pde=1.00e+00  h2_reg=3.22e-09\n",
            "[ep 6000/40000] loss_pde=1.00e+00  h2_reg=2.34e-08\n",
            "  → epoch 6000,  lr=1.77e-03\n",
            "[ep 6100/40000] loss_pde=1.00e+00  h2_reg=2.23e-09\n",
            "[ep 6200/40000] loss_pde=1.00e+00  h2_reg=1.67e-09\n",
            "[ep 6300/40000] loss_pde=9.96e-01  h2_reg=1.37e-05\n",
            "[ep 6400/40000] loss_pde=1.00e+00  h2_reg=1.21e-09\n",
            "[ep 6500/40000] loss_pde=1.00e+00  h2_reg=1.53e-09\n",
            "  → epoch 6500,  lr=1.66e-03\n",
            "[ep 6600/40000] loss_pde=9.99e-01  h2_reg=5.23e-07\n",
            "[ep 6700/40000] loss_pde=1.00e+00  h2_reg=1.09e-09\n",
            "[ep 6800/40000] loss_pde=1.00e+00  h2_reg=1.09e-09\n",
            "[ep 6900/40000] loss_pde=1.00e+00  h2_reg=8.15e-08\n",
            "[ep 7000/40000] loss_pde=1.00e+00  h2_reg=9.65e-10\n",
            "  → epoch 7000,  lr=1.55e-03\n",
            "[ep 7100/40000] loss_pde=1.00e+00  h2_reg=1.21e-09\n",
            "[ep 7200/40000] loss_pde=9.99e-01  h2_reg=2.19e-06\n",
            "[ep 7300/40000] loss_pde=1.00e+00  h2_reg=6.94e-10\n",
            "[ep 7400/40000] loss_pde=1.00e+00  h2_reg=5.15e-10\n",
            "[ep 7500/40000] loss_pde=1.00e+00  h2_reg=3.65e-10\n",
            "  → epoch 7500,  lr=1.44e-03\n",
            "[ep 7600/40000] loss_pde=9.99e-01  h2_reg=2.47e-07\n",
            "[ep 7700/40000] loss_pde=1.00e+00  h2_reg=4.97e-10\n",
            "[ep 7800/40000] loss_pde=1.00e+00  h2_reg=4.09e-10\n",
            "[ep 7900/40000] loss_pde=9.91e-01  h2_reg=7.00e-05\n",
            "[ep 8000/40000] loss_pde=1.00e+00  h2_reg=7.45e-10\n",
            "  → epoch 8000,  lr=1.26e-03\n",
            "[ep 8100/40000] loss_pde=1.00e+00  h2_reg=3.49e-10\n",
            "[ep 8200/40000] loss_pde=1.00e+00  h2_reg=2.66e-09\n",
            "[ep 8300/40000] loss_pde=1.00e+00  h2_reg=8.12e-09\n",
            "[ep 8400/40000] loss_pde=1.00e+00  h2_reg=2.47e-10\n",
            "[ep 8500/40000] loss_pde=1.00e+00  h2_reg=6.70e-09\n",
            "  → epoch 8500,  lr=1.17e-03\n",
            "[ep 8600/40000] loss_pde=1.00e+00  h2_reg=2.37e-08\n",
            "[ep 8700/40000] loss_pde=1.00e+00  h2_reg=1.84e-10\n",
            "[ep 8800/40000] loss_pde=9.98e-01  h2_reg=5.01e-06\n",
            "[ep 8900/40000] loss_pde=1.00e+00  h2_reg=4.13e-10\n",
            "[ep 9000/40000] loss_pde=1.00e+00  h2_reg=1.63e-10\n",
            "  → epoch 9000,  lr=1.09e-03\n",
            "[ep 9100/40000] loss_pde=1.00e+00  h2_reg=1.62e-10\n",
            "[ep 9200/40000] loss_pde=1.01e+00  h2_reg=3.32e-05\n",
            "[ep 9300/40000] loss_pde=1.00e+00  h2_reg=9.51e-10\n",
            "[ep 9400/40000] loss_pde=1.00e+00  h2_reg=1.66e-10\n",
            "[ep 9500/40000] loss_pde=1.00e+00  h2_reg=2.46e-10\n",
            "  → epoch 9500,  lr=1.02e-03\n",
            "[ep 9600/40000] loss_pde=1.00e+00  h2_reg=1.25e-08\n",
            "[ep 9700/40000] loss_pde=1.00e+00  h2_reg=1.25e-10\n",
            "[ep 9800/40000] loss_pde=1.00e+00  h2_reg=1.59e-10\n",
            "[ep 9900/40000] loss_pde=1.00e+00  h2_reg=2.29e-07\n",
            "[ep 10000/40000] loss_pde=1.00e+00  h2_reg=1.16e-10\n",
            "  → epoch 10000,  lr=8.89e-04\n",
            "[ep 10100/40000] loss_pde=1.00e+00  h2_reg=1.05e-10\n",
            "[ep 10200/40000] loss_pde=1.00e+00  h2_reg=2.34e-07\n",
            "[ep 10300/40000] loss_pde=1.00e+00  h2_reg=9.76e-11\n",
            "[ep 10400/40000] loss_pde=1.00e+00  h2_reg=2.59e-05\n",
            "[ep 10500/40000] loss_pde=1.00e+00  h2_reg=8.44e-11\n",
            "  → epoch 10500,  lr=8.30e-04\n",
            "[ep 10600/40000] loss_pde=1.00e+00  h2_reg=7.44e-11\n",
            "[ep 10700/40000] loss_pde=1.00e+00  h2_reg=1.84e-10\n",
            "[ep 10800/40000] loss_pde=1.00e+00  h2_reg=2.35e-08\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4135525067.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m#eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mr_eval\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mpde_residual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_Xb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforcing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#evaluate the model on the true PDE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mloss_eval\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_sigma_pde\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                   \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_sigma_sobolev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh2_reg\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3923879642.py\u001b[0m in \u001b[0;36mpde_residual\u001b[0;34m(model, x, theta, sigma, forcing)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     )[0][:,0]\n\u001b[0;32m---> 22\u001b[0;31m     tau_yy = torch.autograd.grad(\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mtau_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    501\u001b[0m         )\n\u001b[1;32m    502\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         result = _engine_run_backward(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5) L-BFGS Fine-Tuning"
      ],
      "metadata": {
        "id": "MYTAjR_mAHHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LBFGS fine-tuning\n",
        "\n",
        "# Reload best Adam snapshot\n",
        "print(f\"\\nLoading best Adam model (loss={best_loss:.2e}, epoch={best_epoch}) …\")\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# define L-BFGS parameters\n",
        "lbfgs = torch.optim.LBFGS(\n",
        "    model.parameters(),\n",
        "    lr=0.1,\n",
        "    max_iter=100,\n",
        "    history_size=100,\n",
        "    tolerance_grad=1e-9,\n",
        "    tolerance_change=1e-9,\n",
        "    line_search_fn=\"strong_wolfe\"\n",
        ")\n",
        "\n",
        "# freeze one big PDE batch for the closure\n",
        "Xb_ft = sample_uniform_disk(pool_size, R=R)\n",
        "\n",
        "lbfgs_total = []\n",
        "lbfgs_pde   = []\n",
        "\n",
        "#closure function\n",
        "def closure():\n",
        "    closure.calls += 1\n",
        "    lbfgs.zero_grad()\n",
        "    # PDE term\n",
        "    r_ft    = pde_residual(model, Xb_ft, theta, sigma, forcing = None)\n",
        "    pde_l   = loss_fn(r_ft)\n",
        "\n",
        "    # data term\n",
        "    #tau_p   = model(x_data).squeeze(-1)\n",
        "    #data_l  = (tau_p - tau_data).pow(2).mean()\n",
        "\n",
        "    h2_reg =  empirical_h2_norm(model, Xb_ft)\n",
        "\n",
        "    total_l = torch.exp(-2 * model.log_sigma_pde) * pde_l \\\n",
        "            + torch.exp(-2 * model.log_sigma_sobolev) * h2_reg \\\n",
        "            + model.log_sigma_pde + model.log_sigma_sobolev\n",
        "\n",
        "    # record\n",
        "    lbfgs_total.append(total_l.item() - \\\n",
        "            (model.log_sigma_pde.detach() + model.log_sigma_sobolev.detach()).item())\n",
        "    lbfgs_pde.append(  pde_l.item()   )\n",
        "\n",
        "    total_l.backward()\n",
        "    return total_l\n",
        "\n",
        "closure.calls = 0"
      ],
      "metadata": {
        "id": "6Xu_iuYS2b2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# report losses before/after lbfgs\n",
        "# make sure model is in eval mode so e.g. Dropout/BatchNorm won’t move\n",
        "model.eval()\n",
        "\n",
        "# --- PDE loss (needs grad to compute ∇u inside pde_residual) ---\n",
        "r_before = pde_residual(model, Xb_ft, theta, sigma, forcing = None)\n",
        "before_pde = loss_fn(r_before).item()\n",
        "\n",
        "# --- empirical Sobolev norm (obviously needs grad too) ---\n",
        "before_h2 = empirical_h2_norm(model, Xb_ft).item()\n",
        "\n",
        "print(f\"Before L-BFGS → PDE {before_pde:.2e}, SOBOLEV {before_h2:.2e}\")\n",
        "\n",
        "# … run your L-BFGS …\n",
        "print(\"→ Running L-BFGS …\")\n",
        "loss_after = lbfgs.step(closure)\n",
        "print(f\"L-BFGS did {closure.calls} closure calls, final total loss = {loss_after:.2e}\")\n",
        "\n",
        "# after L-BFGS, same pattern:\n",
        "model.eval()\n",
        "r_after = pde_residual(model, Xb_ft, theta, sigma, forcing = None)\n",
        "after_pde = loss_fn(r_after).item()\n",
        "after_h2 = empirical_h2_norm(model, Xb_ft).item()\n",
        "print(f\" After L-BFGS → PDE {after_pde:.2e}, SOBOLEV {after_h2:.2e}\")"
      ],
      "metadata": {
        "id": "Tq8eGZNl31KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Plotting the results"
      ],
      "metadata": {
        "id": "CpHta0VwANgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1) Training curves"
      ],
      "metadata": {
        "id": "ShkQeYZSAQVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting three separate loss evolutions\n",
        "n = min(n_epochs, len(total_history))\n",
        "total_history = total_history + lbfgs_total\n",
        "pde_history   = pde_history + lbfgs_pde\n",
        "\n",
        "# a) Total Loss\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.semilogy(np.arange(n), total_history[:n], \\\n",
        "             color = 'blue', label = \"ADAM\")\n",
        "plt.semilogy(np.arange(n, len(total_history)), total_history[n:], \\\n",
        "             color = 'green', label = \"L-BFGS\")\n",
        "plt.title(\"Total Loss vs Epoch / L-BFGS step\")\n",
        "plt.xlabel(\"step (Adam epochs or L-BFGS iter)\")\n",
        "plt.ylabel(\"loss_total\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "# b) PDE Loss\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.semilogy(np.arange(n), pde_history[:n], \\\n",
        "             color = 'blue', label = \"ADAM\")\n",
        "plt.semilogy(np.arange(n, len(pde_history)), pde_history[n:], \\\n",
        "             color = 'green', label = \"L-BFGS\")\n",
        "plt.title(\"PDE Loss vs Epoch / L-BFGS step\")\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"loss_pde\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "# # c) Data Loss\n",
        "# plt.figure(figsize=(6,4))\n",
        "# plt.semilogy(np.arange(n_hom), data_history[:n_hom], \\\n",
        "#              color = 'blue', label = \"ADAM with homotopy\")\n",
        "# plt.semilogy(np.arange(n_hom, n_epochs), data_history[n_hom: n_epochs], \\\n",
        "#              color = 'orange', label = \"Adam on true PDE\")\n",
        "# plt.semilogy(np.arange(n_epochs, len(pde_history)), data_history[n_epochs:], \\\n",
        "#              color = 'green', label = \"L-BFGS on true PDE\")\n",
        "# plt.title(\"Data Loss vs Epoch / L-BFGS step\")\n",
        "# plt.xlabel(\"step\")\n",
        "# plt.ylabel(\"loss_data\")\n",
        "# plt.legend()\n",
        "# plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QC2cFlG3q6mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2) Learned Solution vs True Solution"
      ],
      "metadata": {
        "id": "dqHCGWxwAW-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Visualize PINN vs True tau\n",
        "\n",
        "n_pts = 200\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(-R,R,n_pts),\n",
        "    np.linspace(-R,R,n_pts)\n",
        ")\n",
        "mask = xx**2 + yy**2 <= R**2\n",
        "pts = np.vstack([xx[mask], yy[mask]]).T\n",
        "\n",
        "with torch.no_grad():\n",
        "    inp = torch.from_numpy(pts).to(device)\n",
        "    pred = model(inp).cpu().numpy().squeeze(-1)\n",
        "\n",
        "Zp = np.zeros_like(xx); Zt = np.zeros_like(xx)\n",
        "Zp[mask], Zt[mask] = pred, true_tau_vec(xx[mask], yy[mask])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n",
        "cf1 = ax1.contourf(xx, yy, Zp, levels=50, cmap='viridis')\n",
        "ax1.set_title('PINN tau')\n",
        "fig.colorbar(cf1, ax=ax1, shrink=0.8)\n",
        "\n",
        "cf2 = ax2.contourf(xx, yy, Zt, levels=50, cmap='viridis')\n",
        "ax2.set_title('True tau')\n",
        "fig.colorbar(cf2, ax=ax2, shrink=0.8)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mICNQZu_q96H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3) Relative error"
      ],
      "metadata": {
        "id": "37SwVkTGAcBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# average relative L2 error\n",
        "#    ‖tau_pinn − tau_true‖₂ / ‖tau_true‖₂\n",
        "pred_vals = pred\n",
        "true_vals = true_tau_vec(pts[:,0], pts[:,1])\n",
        "tau_l2_norm =  np.linalg.norm(true_vals)\n",
        "glob_rel_L2 = np.linalg.norm(pred_vals - true_vals) / tau_l2_norm\n",
        "print(f\"Global relative L2 error: {glob_rel_L2:.3e}\")\n",
        "\n",
        "# pointwise relative L2 error\n",
        "Z_err = np.abs(Zp - Zt)                     # raw error\n",
        "#Z_err_norm = Z_err / tau_l2_norm\n",
        "#err_flat = Z_err[mask]              # flatten to disk\n",
        "#L2_norm = np.sqrt(np.mean(err_flat**2))\n",
        "#Z_err_norm = np.zeros_like(Z_err)\n",
        "#Z_err_norm[mask] = Z_err[mask] / L2_norm\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "cf = plt.contourf(xx, yy, Z_err, levels=50, cmap='RdBu_r')\n",
        "plt.colorbar(cf, label=r'$(|\\tau_{PINN}-\\tau|/\\|\\tau\\|_{L^2}$')\n",
        "plt.title('Normalized L²‐Error Field')\n",
        "plt.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GZSdlI0pyicy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}