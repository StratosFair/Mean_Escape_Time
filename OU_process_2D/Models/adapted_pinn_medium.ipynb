{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOuHHG86H96VlcMJdt9AM9M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StratosFair/Mean_Escape_Time/blob/main/OU_process_2D/Models/adapted_pinn_medium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports & FP64\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.integrate as integrate\n",
        "import scipy.special as special\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.set_default_dtype(torch.float64)"
      ],
      "metadata": {
        "id": "zWfcku6GqTt1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solving for the MET of an Ornstein-Uhlenbeck process in a disk with \"Adapted Architecture PINNs\" : comparison with exact solution"
      ],
      "metadata": {
        "id": "wDHeduNpyl5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Setup\n",
        "\n",
        "For $\\theta,\\sigma,r >0$ some fixed parameters, we let $\\Omega := B_r \\equiv \\{x\\in\\mathbb R^d : \\|x\\|< r \\}$, and define the process ($d=2$ in our illustration) :\n",
        "\n",
        "$$\\begin{cases} dX_t &= -\\theta X_t dt + \\sigma dB_t \\\\\n",
        "X_0 &= x \\in \\Omega \\end{cases} $$\n",
        "\n",
        "For all $x\\in\\Omega$, let\n",
        "\n",
        "$$T(x) := \\inf\\{t\\ge 0 : X_t \\in\\partial\\Omega\\} $$\n",
        "\n",
        "and let its first moment be denoted\n",
        "\n",
        "$$\\tau(x) := \\mathbb E[T(x)] $$\n",
        "\n",
        "We can show under some regularity conditions on $\\Omega$ that $\\tau$ is the (unique) solution of the BVP :\n",
        "\n",
        "$$\\begin{cases} -\\mathcal{L}u(x) &= 1 \\text{ for all } x\\in\\Omega \\\\\n",
        "u(x) &= 0 \\text{ for all } x\\in\\partial\\Omega \\end{cases} $$\n",
        "\n",
        "where $\\mathcal L$ is the infinitesimal generator of the Ornstein-Uhlenbeck process, given by\n",
        "$$\\mathcal Lu : x \\mapsto -\\theta x \\cdot \\nabla u(x) + \\frac{\\sigma^2}{2}\\Delta u(x) $$"
      ],
      "metadata": {
        "id": "EpGLkz71ym36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nicely enough, for this problem, we can compare our solution with the known closed-form solution (see https://arxiv.org/abs/2208.04029) :\n",
        "\n",
        "$$ \\tau(x) := \\frac{1}{\\lambda^{d/2}\\sigma^2}\\int_\\rho^r z^{1-d} e^{\\lambda z^2} \\gamma(d/2, \\lambda z^2)\\ dz $$\n",
        "\n",
        "where $\\lambda := \\theta/\\sigma^2 $, $\\rho := \\|x\\| $ and $\\gamma$ is the upper incomplete gamma function :\n",
        "$$\\gamma(n,y) := \\int_0^y t^{n-1} e^{-t}\\ dt.  $$"
      ],
      "metadata": {
        "id": "ah5DvZTq7uP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem Parameters & True Solution\n",
        "R = 2.0\n",
        "theta = 1.0\n",
        "sigma = 1.0\n",
        "\n",
        "def true_tau(x, y, theta=theta, sigma=sigma, R=R):\n",
        "    lam = theta / sigma**2\n",
        "    rho = np.sqrt(x**2 + y**2)\n",
        "    integrand = lambda t: np.exp(lam * t**2) * special.gammainc(1, lam * t**2) / t\n",
        "    I, _ = integrate.quad(integrand, rho, R)\n",
        "    return I / theta\n",
        "\n",
        "true_tau_vec = np.vectorize(true_tau)"
      ],
      "metadata": {
        "id": "1lyRR4sIqUbT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Neural Network architecture and loss function\n",
        "\n",
        "Unlike the standard PINN, we will take an hypothesis space of Neural Networks which satisfy the boundary conditions explicitly. For this problem, we have homogenous zero Dirichlet boundary conditions, which we can enforce by multiplying our Neural Networks with an appropriate \"smooth distance approximation\" (see https://arxiv.org/abs/2104.08426). In accordance with the mentioned paper, we will take\n",
        "$$\\varphi : x\\mapsto \\frac{r^2 - \\|x\\|^2}{2r}$$\n",
        "as our smooth distance approximation.  \n",
        "\n",
        "\n",
        "With this modification, our objective to minimize becomes\n",
        "$$\\hat u := \\arg\\min_{u\\in\\mathcal{NN}_\\varphi}\\ \\frac1n \\sum_{i=1}^n (\\mathcal L u(x_i^c) + 1)^2 $$\n",
        "where $x_i^c$ are sampled i.i.d. with uniform distribution on $\\Omega$,\n",
        "$$\\mathcal{NN}_\\varphi:=\\left\\{x\\mapsto \\varphi(x) \\cdot T_L\\circ \\sigma \\circ T_{L-1}\\circ\\cdots\\circ \\sigma\\circ T_1 (x)\\right\\}, $$\n",
        "for\n",
        "$$T_\\ell : \\mathbb{R}^{\\ell-1}\\to\\mathbb{R}^\\ell $$\n",
        "affine-linear maps of appropriate input-output dimensions, where\n",
        "$$\\sigma :x \\mapsto \\begin{cases}x^2 &\\text{ if } x\\ge 0\\\\ 0 &\\text{ if } x\\le 0\\end{cases} \\quad \\text{OR }\\ \\ \\sigma : x\\mapsto \\operatorname{tanh}(x),$$\n",
        "is either the ReQU or hyperbolic tangent activation function, which is understood element-wise when applied to vectors."
      ],
      "metadata": {
        "id": "ipb5U7KL7yL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining PINN w/ matching boundary condition\n",
        "power = 2 #exponent k for relu^k, none for tanh\n",
        "width = 50\n",
        "depth = 3\n",
        "gain = 1.0 #magnitude of weights at initialization\n",
        "\n",
        "#define ReLU^k activation\n",
        "\n",
        "class RePU(nn.Module):\n",
        "    def __init__(self, power = power):\n",
        "        super(RePU, self).__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.pow(torch.relu(x), self.power)\n",
        "\n",
        "#define the smooth distance approximation\n",
        "def smooth_distance(x):\n",
        "    norm_x = torch.linalg.norm(x, dim=-1)\n",
        "    return (R**2 - norm_x**2)/(2*R)\n",
        "\n",
        "#define hypothesis space\n",
        "class BoundaryPINN(nn.Module):\n",
        "    def __init__(self, power = power, width = width, depth = depth):\n",
        "        super(BoundaryPINN,self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(2, width))\n",
        "        if isinstance(power, int):\n",
        "            self.layers.append(RePU(power))\n",
        "            for _ in range(depth-1) :\n",
        "                self.layers.append(nn.Linear(width, width))\n",
        "                self.layers.append(RePU(power))\n",
        "        else :\n",
        "            self.layers.append(nn.Tanh())\n",
        "            for _ in range(depth-1) :\n",
        "                self.layers.append(nn.Linear(width, width))\n",
        "                self.layers.append(nn.Tanh())\n",
        "        self.layers.append(nn.Linear(width, 1))\n",
        "\n",
        "        #for dynamic weighting, first biased towards data fidelity\n",
        "        self.log_sigma_pde = nn.Parameter(torch.tensor(0.0))\n",
        "        self.log_sigma_data = nn.Parameter(torch.tensor(-np.log(2),\\\n",
        "                                                        dtype=torch.float64))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "        distance =  smooth_distance(x)\n",
        "        return output * distance.unsqueeze(-1)\n",
        "\n",
        "#weight initialization\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_normal_(m.weight, gain=gain)\n",
        "        m.bias.data.fill_(gain)"
      ],
      "metadata": {
        "id": "kNAMxUlRqa3D"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining PDE Residual & Loss, enabling grad on x\n",
        "def pde_residual(model, x, theta, sigma, forcing = None):\n",
        "    # ensure x is on the right device, float64, and requires grad\n",
        "    x = x.clone().detach().to(device).requires_grad_(True)\n",
        "\n",
        "    # forward through PINN\n",
        "    tau = model(x)                                # [N]\n",
        "    # first derivatives\n",
        "    grads = torch.autograd.grad(\n",
        "        tau, x,\n",
        "        grad_outputs=torch.ones_like(tau),\n",
        "        create_graph=True,\n",
        "    )[0]                                        # [N,2]\n",
        "    tau_x, tau_y = grads[:,0], grads[:,1]\n",
        "\n",
        "    # second derivatives\n",
        "    tau_xx = torch.autograd.grad(\n",
        "        tau_x, x,\n",
        "        grad_outputs=torch.ones_like(tau_x),\n",
        "        create_graph=True\n",
        "    )[0][:,0]\n",
        "    tau_yy = torch.autograd.grad(\n",
        "        tau_y, x,\n",
        "        grad_outputs=torch.ones_like(tau_y),\n",
        "        create_graph=True\n",
        "    )[0][:,1]\n",
        "\n",
        "    # OU‐generator L[tau]\n",
        "    phys = -theta*(x[:,0]*tau_x + x[:,1]*tau_y) \\\n",
        "           + 0.5*sigma**2*(tau_xx + tau_yy)\n",
        "\n",
        "    if forcing is None:\n",
        "        # default constant −1\n",
        "        f_vals = -1.0\n",
        "    elif isinstance(forcing, (int, float)):\n",
        "        # constant forcing = that number\n",
        "        f_vals = float(forcing)\n",
        "    elif callable(forcing):\n",
        "        # call it on x\n",
        "        f_vals = forcing(x)\n",
        "        # flatten to [N]\n",
        "        if f_vals.dim() > 1:\n",
        "            f_vals = f_vals.view(-1)\n",
        "    else:\n",
        "        raise ValueError(f\"forcing must be None, float, or callable, got {type(forcing)}\")\n",
        "\n",
        "    # turn any scalar into a tensor of shape [N]\n",
        "    if isinstance(f_vals, float) or isinstance(f_vals, int):\n",
        "        f_vals = x.new_full((x.shape[0],), float(f_vals))\n",
        "\n",
        "    # 7) residual = L[tau] - f(x)\n",
        "    r = phys - f_vals              # shape [N]\n",
        "    return r\n",
        "\n",
        "def loss_fn(res):\n",
        "    return torch.mean(res.pow(2))"
      ],
      "metadata": {
        "id": "RDSBmM64qd26"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Computing an approximate data penalty regularization\n",
        "\n",
        "We sample a small subset $x_1, \\ldots, x_{n_{data}} \\in \\Omega$ of points for which we compute approximate values $\\tau^{MC}(x_1), \\ldots, \\tau^{MC}(x_{n_{data}}) $ by Monte Carlo. we will then use them to define a \"data-fidelity\" penalty term in the loss function:\n",
        "$$\\text{Penalty}(\\hat\\tau_{NN}) = \\frac{\\lambda_{data}}{n_{data}} \\sum_{i=1}^n \\left(\\tau^{MC}(x_i) - \\hat\\tau_{NN}(x_i)\\right)^2, $$\n",
        "where $\\lambda_{data}$ is a positive constant."
      ],
      "metadata": {
        "id": "xA77CH6N03KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating some approximate true solutions in the domain\n",
        "\n",
        "#sampling anchor points, biased towards center (failure zone)\n",
        "def sample_disk(batch_size, R, radius_frac=0.6):\n",
        "    \"\"\"\n",
        "    Sample uniformly in the DISK of radius (radius_frac * R).\n",
        "    This is the simplest way to concentrate points near the centre.\n",
        "    \"\"\"\n",
        "    r = torch.sqrt(torch.rand(batch_size, device=device))  # uniform‐disk radius ∝ sqrt(U)\n",
        "    theta = 2 * torch.pi * torch.rand(batch_size, device=device)\n",
        "    x = torch.stack([r * torch.cos(theta), r * torch.sin(theta)], dim=1)\n",
        "    return radius_frac * R * x  # now uniform in disk of radius radius_frac*R\n",
        "\n",
        "\n",
        "# 2) monte-carlo (euler-maruyama) approximation of tau at these points\n",
        "@torch.no_grad()\n",
        "def mc_exit_time(x0, theta, sigma, R,\n",
        "                 n_paths=128, dt=1e-3, max_steps=20000):\n",
        "    \"\"\"\n",
        "    x0: [B,2] starting points\n",
        "    Returns tau_hat: [B] = mean exit time from each x0.\n",
        "    Any path still 'alive' after max_steps is counted as having exit time = max_steps*dt.\n",
        "    \"\"\"\n",
        "    B = x0.shape[0]\n",
        "    P = n_paths\n",
        "\n",
        "    # Expand to per‐path positions\n",
        "    X = x0.unsqueeze(1).expand(B, P, 2).clone()  # [B,P,2]\n",
        "    t = torch.zeros(B, P, device=device)         # running clock\n",
        "    alive = torch.ones(B, P, dtype=torch.bool, device=device)\n",
        "    sqrt_dt = sigma * (dt ** 0.5)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        if not alive.any():\n",
        "            break\n",
        "\n",
        "        # Drift + noise, but only update those still alive\n",
        "        X_alive = X[alive]\n",
        "        drift    = -theta * X_alive * dt\n",
        "        noise    = sqrt_dt * torch.randn_like(X_alive)\n",
        "        X[alive] += drift + noise\n",
        "        t[alive] += dt\n",
        "\n",
        "        # Mark newly‐exited paths\n",
        "        just_exited = (X[alive].pow(2).sum(dim=1) >= R*R)\n",
        "        idx_alive   = alive.nonzero(as_tuple=False)  # [[i1,j1], [i2,j2], ...]\n",
        "        exited_idx  = idx_alive[just_exited]\n",
        "        alive[exited_idx[:,0], exited_idx[:,1]] = False\n",
        "\n",
        "    # Paths still alive get t = max_steps*dt automatically from the loop\n",
        "    tau_hat = t.mean(dim=1)  # [B]\n",
        "    return tau_hat  # no gradient flows back"
      ],
      "metadata": {
        "id": "wWM1g0wlzSPW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Homotopy Regularization\n",
        "\n",
        "We define a smooth function $g(x)$ for regularizing the constant forcing term as\n",
        "\n",
        "$$\n",
        "g(x) = \\mathrm{magnitude} \\cdot \\sigma\\!\\left(\\frac{1}{\\sigma_{\\mathrm{raw}}}\\left[\\sum_{j=1}^{M} a_j\\cos\\Big(\\langle \\omega_j, x \\rangle + b_j\\Big) - \\mu\\right]\\right)\n",
        "\\cdot \\frac{\\max\\{R^2 - \\|x\\|^2,\\,0\\}}{2R},\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "- $a_j$, $\\omega_j$, and $b_j$ are random coefficients (with $\\omega_j$ scaled by a base frequency, and $b_j$ uniformly drawn from $[0,2\\pi]$),\n",
        "- The **raw Fourier sum** is\n",
        "  $$\n",
        "  G_{\\mathrm{raw}}(x) = \\sum_{j=1}^{M} a_j\\cos\\!\\Big(\\langle \\omega_j, x \\rangle + b_j\\Big),\n",
        "  $$\n",
        "  which is then normalized by subtracting its mean $\\mu$ and dividing by its standard deviation $\\sigma_{\\mathrm{raw}}$;\n",
        "- $\\sigma(\\cdot)$ is the sigmoid function that bounds the output to $(0,1)$;\n",
        "- The radial mask\n",
        "  $$\n",
        "  \\frac{\\max\\{R^2 - \\|x\\|^2,\\,0\\}}{2R}\n",
        "  $$\n",
        "  ensures that $g(x)$ smoothly decays to zero as $\\|x\\|$ approaches $R$.\n",
        "\n",
        "**Quick Intuition:** The function combines a randomized Fourier series with normalization, a sigmoid activation, and a radial decay. This results in a smooth, bounded perturbation to the forcing term, enhancing the regularity needed for improved convergence of the PINN solver."
      ],
      "metadata": {
        "id": "u7GNsuMn7qwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Fourier Homotopy g(x) to regularize the forcing term\n",
        "class RandomFourierHomotopy:\n",
        "    def __init__(self, M=200, freq_scale=None, R=R, magnitude=8.0):\n",
        "        \"\"\"\n",
        "        M           : number of random Fourier modes\n",
        "        freq_scale  : base frequency scale (if None, defaults to π/R)\n",
        "        R           : radius of the disk\n",
        "        magnitude   : overall output scale\n",
        "        \"\"\"\n",
        "        self.M         = M\n",
        "        self.R         = R\n",
        "        self.magnitude = magnitude\n",
        "\n",
        "        if freq_scale is None:\n",
        "            # pick a lower base frequency so the field is smooth\n",
        "            freq_scale = math.pi / R\n",
        "\n",
        "        # sample random directions & phases\n",
        "        # omega: [M,2], b: [M], a: [M]\n",
        "        self.omega = torch.randn(M, 2) * freq_scale\n",
        "        self.b     = 2 * math.pi * torch.rand(M)\n",
        "        # normalize by sqrt(M) so that raw variance ~1\n",
        "        self.a     = torch.randn(M) / math.sqrt(M)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        x: [N,2] tensor of (x,y) points\n",
        "        returns: [N] tensor of forcing values\n",
        "        \"\"\"\n",
        "        # 1) compute raw random Fourier sum\n",
        "        #    proj: [N,M] = x·omega^T + b\n",
        "        proj  = x @ self.omega.t() + self.b\n",
        "        g_raw = (self.a * torch.cos(proj)).sum(dim=1)    # [N]\n",
        "\n",
        "        # 2) normalize to zero mean, unit std dev (over this batch)\n",
        "        mean   = g_raw.mean()\n",
        "        stddev = g_raw.std(unbiased=False).clamp(min=1e-6)\n",
        "        g_norm = (g_raw - mean) / stddev                # [N]\n",
        "\n",
        "        # 3) nonlinearity: tanh keeps it in (–1,1) but nonzero everywhere\n",
        "        g_nl = torch.sigmoid(g_norm)                       # [N]\n",
        "\n",
        "        # 4) radial mask to fade to zero at |x|=R\n",
        "        r2   = (x**2).sum(dim=1)                        # [N]\n",
        "        mask = ((self.R**2 - r2).clamp(min=0)\n",
        "                / (2 * self.R))                       # [N]\n",
        "\n",
        "        # 5) scale to the desired magnitude\n",
        "        return self.magnitude * g_nl * mask             # [N]"
      ],
      "metadata": {
        "id": "9-A3bb5AqX1E"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Adaptive sampling [TODO: add citation]\n",
        "The function `make_adaptive_batch` creates a mini-batch that focuses training on areas with high PDE residuals:\n",
        "\n",
        "1. **Residual Computation:**  \n",
        "   A candidate pool `Xp` is sampled uniformly over the disk. For each point $x$, the PDE residual is computed via  \n",
        "   $$\n",
        "   r(x) = \\texttt{pde_residual(model, x, theta, sigma, forcing)}\n",
        "   $$  \n",
        "   and the absolute residual is defined as  \n",
        "   $$\n",
        "   R(x) = |r(x)|.\n",
        "   $$\n",
        "\n",
        "2. **Adaptive Selection:**  \n",
        "   A fraction of the batch (defined by `hard_frac`) is formed using high-error points. There are two modes:\n",
        "   - **Hard Sampling:** Directly select the top $K = \\texttt{hard_frac} \\times \\texttt{batch_size}$ points with the highest $R(x)$.\n",
        "   - **Soft Sampling:** Sample points with probability proportional to  \n",
        "     $$\n",
        "     p(x) \\propto R(x)^\\beta.\n",
        "     $$\n",
        "     The parameter $\\beta$ controls the selection sharpness:\n",
        "     - A small $\\beta$ makes the sampling nearly uniform.\n",
        "     - A large $\\beta$ heavily favors points with high residuals.\n",
        "\n",
        "3. **Batch Formation:**  \n",
        "   The final batch is a combination of these adaptively chosen points and randomly selected points from the remaining pool.\n",
        "\n",
        "This method helps concentrate training on regions where the model underperforms while still maintaining overall coverage."
      ],
      "metadata": {
        "id": "OH0uiQg_8lBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining Uniform + Adaptive Sampling functions in the disk\n",
        "def sample_uniform_disk(N, R=R):\n",
        "    theta = 2*np.pi * torch.rand(N, dtype=torch.float64, device=device)\n",
        "    rho = R    * torch.sqrt(torch.rand(N, dtype=torch.float64, device=device))\n",
        "    return torch.stack([rho*torch.cos(theta), rho*torch.sin(theta)], dim=1)  # [N,2]\n",
        "\n",
        "def make_adaptive_batch(\n",
        "    model, pool_size, batch_size,\n",
        "    forcing, theta, sigma,\n",
        "    hard_frac=0.1,    # fraction of batch from highest residuals\n",
        "    use_soft=False,   # if True, sample soft rather than top‐K\n",
        "    beta=1.0          # exponent for soft sampling\n",
        "):\n",
        "    # candidate pool\n",
        "    Xp = sample_uniform_disk(pool_size)\n",
        "\n",
        "    # 2) Compute residuals _with_ gradient‐tracking so second‐derivatives work\n",
        "    r_all = pde_residual(model, Xp, theta, sigma, forcing)  # [pool_size]\n",
        "    # 3) Detach and take absolute\n",
        "    Rvals = r_all.detach().abs()                             # [pool_size]\n",
        "\n",
        "    # number of hard points\n",
        "    K     = int(batch_size * hard_frac)\n",
        "    if use_soft or beta < 10.0 :\n",
        "        # 2a) soft sampling: probabilities ∝ (residual^beta)\n",
        "        weights = Rvals.pow(beta)\n",
        "        weights = weights / (weights.sum() + 1e-12)\n",
        "        hard_idx = torch.multinomial(weights,\n",
        "                                     num_samples=K,\n",
        "                                     replacement=False)\n",
        "    else:\n",
        "        # 2b) hard sampling: pure top‐K\n",
        "        hard_idx = torch.topk(Rvals, K).indices # [K]\n",
        "\n",
        "    all_idx  = torch.arange(pool_size, device=device)\n",
        "    mask     = torch.ones(pool_size, dtype=torch.bool, device=device)\n",
        "    mask[hard_idx] = False\n",
        "    rest_idx = all_idx[mask]\n",
        "    n_rest   = batch_size - K\n",
        "    rnd      = rest_idx[torch.randperm(rest_idx.numel(), device=device)[:n_rest]]\n",
        "\n",
        "    batch_idx = torch.cat([hard_idx, rnd], dim=0)                # [batch_size]\n",
        "    return Xp[batch_idx]                                      # [batch_size,2]"
      ],
      "metadata": {
        "id": "IJI5xaOUqom1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Training loop"
      ],
      "metadata": {
        "id": "gkRR91T6_atw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 6.1) Training Parameters"
      ],
      "metadata": {
        "id": "Qo8UoTLI_lue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "pool_size        = 4096\n",
        "batch_size       = 512\n",
        "n_epochs         = 20_000\n",
        "learning_rate    = 5e-2\n",
        "beta = 1 # 0 -> no adaptive sampling, infty -> top K sampling\n",
        "\n",
        "\n",
        "# model instantiation\n",
        "model = BoundaryPINN().to(device)\n",
        "model.apply(init_weights)\n",
        "\n",
        "opt   = torch.optim.Adam(list(model.parameters()), lr=learning_rate)\n",
        "\n",
        "# scheduler: every `step_size` epochs multiply lr by gamma\n",
        "step_size = 500        # decay lr every step_size epochs\n",
        "n_steps   = n_epochs // step_size\n",
        "damping   = 1e-2\n",
        "gamma     = math.exp(math.log(damping) / n_steps)        # mutiply lr by gamma at each step\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(opt,\n",
        "                                            step_size=step_size,\n",
        "                                            gamma=gamma)"
      ],
      "metadata": {
        "id": "Oq-NigAc8Ag2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2) Generating the \"data-fidelity\" samples"
      ],
      "metadata": {
        "id": "7iyHryGJ_pU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sample x_data\n",
        "x_data = sample_disk(batch_size = batch_size//2,\n",
        "                     R = R,\n",
        "                     radius_frac = 0.8)\n",
        "\n",
        "# precompute tau_data once (theta, sigma are fixed PDE params)\n",
        "tau_data = mc_exit_time(\n",
        "    x0       = x_data,\n",
        "    theta    = theta,\n",
        "    sigma    = sigma,\n",
        "    R        = R,\n",
        "    n_paths  = 256,\n",
        "    dt       = 1e-3,\n",
        "    max_steps= 10_000\n",
        ")"
      ],
      "metadata": {
        "id": "kepv7k3TzfBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3) Parametrizing the homotopy function and schedule"
      ],
      "metadata": {
        "id": "upAlBbgt_zz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parametrizing the homotopy\n",
        "sigma_true = sigma       # true diffusion coefficient\n",
        "theta_true = theta       # true drift coefficient\n",
        "r_true     = theta_true / sigma_true # true ratio\n",
        "\n",
        "# \"easy\" and true coefficient ratio\n",
        "r0 = 0.25 if r_true > 0.25 else r_true\n",
        "r1 = r_true\n",
        "\n",
        "# \"easy\" and true forcing term\n",
        "g_hom = RandomFourierHomotopy(M=50)\n",
        "f_0 = lambda x : -1.0 - g_hom(x)\n",
        "f_1 = lambda x : x.new_full((x.shape[0],), -1.0)\n",
        "\n",
        "#helper functions\n",
        "nu_step = 0.1\n",
        "def make_nu(epoch, n_hom, step = 0.1):\n",
        "    \"\"\"Returns nu in steps of `step` from 0 up to 1 after n_hom epochs.\"\"\"\n",
        "    if epoch >= n_hom:\n",
        "        return 1.0\n",
        "    continuous_nu = float(epoch) / n_hom\n",
        "    discrete_nu = math.floor(continuous_nu / step) * step\n",
        "    return discrete_nu\n",
        "\n",
        "def interp(a0, a1, nu):\n",
        "    return a0 * (1.0 - nu) + a1 * nu"
      ],
      "metadata": {
        "id": "KrqgYNkqxnko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot homotopy forcing g_hom(x)\n",
        "n_pts = 200\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(-R,R,n_pts),\n",
        "    np.linspace(-R,R,n_pts)\n",
        ")\n",
        "mask = xx**2 + yy**2 <= R**2\n",
        "grid_pts = np.vstack([xx[mask], yy[mask]]).T\n",
        "\n",
        "with torch.no_grad():\n",
        "    g_vals = g_hom(torch.from_numpy(grid_pts).to(device)).cpu().numpy()\n",
        "\n",
        "Zg = np.zeros_like(xx)\n",
        "Zg[mask] = g_vals\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "cg = plt.contourf(xx, yy, Zg, levels=50, cmap='coolwarm')\n",
        "plt.colorbar(cg, shrink=0.8)\n",
        "plt.title('Homotopy forcing $g_{\\\\rm hom}(x)$')\n",
        "plt.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M3o_xL7B0mPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pde_residual_homotopy(model, X, nu):\n",
        "    \"\"\"\n",
        "    Single‐nu homotopy:\n",
        "      - theta_nu = (interp(r0,r1,nu)) * sigma_true\n",
        "      - f_nu(x)  = interp( f_0(x), f_1(x), nu )\n",
        "    If nu=None, uses theta_true and f_1 exactly.\n",
        "    \"\"\"\n",
        "    # 1) drift/diff ratio\n",
        "    if nu is None:\n",
        "        theta_nu = theta_true\n",
        "    else:\n",
        "        r_nu     = interp(r0, r1, nu)\n",
        "        theta_nu = r_nu * sigma_true\n",
        "\n",
        "    # 2) forcing\n",
        "    if nu is None:\n",
        "        forcing_nu = f_1\n",
        "    else:\n",
        "        forcing_nu = lambda x : interp(f_0(x), f_1(x), nu)\n",
        "\n",
        "    return pde_residual(model, X, theta_nu, sigma_true, forcing=forcing_nu)"
      ],
      "metadata": {
        "id": "vIE2jqz5zK5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4) Start the Training with Adam"
      ],
      "metadata": {
        "id": "RSYL1T67_73u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop w/ Homotopy & Loss Logging\n",
        "\n",
        "# prep for best‐model tracking\n",
        "best_loss       = float(\"inf\")\n",
        "best_model_path = \"best_adam.pth\"\n",
        "best_epoch = 0\n",
        "eval_Xb = sample_uniform_disk(pool_size, R=R) #to evaluate the model\n",
        "\n",
        "total_history = []\n",
        "pde_history   = []\n",
        "data_history  = []\n",
        "\n",
        "n_hom =  int(0.6 * n_epochs)   # epochs to ramp nu from 0 to 1\n",
        "\n",
        "print(\"\\n=== Training with single‐nu homotopy ramp ===\")\n",
        "model.train()\n",
        "for ep in range(1, n_epochs+1):\n",
        "    nu = make_nu(ep, n_hom, nu_step)   # in [0,1], then stays at 1\n",
        "\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # 2) build one adaptive interior batch using current nu\n",
        "    if nu is None:\n",
        "        forcing_nu = f_1\n",
        "        theta_nu = theta_true\n",
        "    else:\n",
        "        forcing_nu = lambda x : interp(f_0(x), f_1(x), nu)\n",
        "        r_nu     = interp(r0, r1, nu)\n",
        "        theta_nu = r_nu * sigma_true\n",
        "\n",
        "    Xb = make_adaptive_batch(\n",
        "        model, pool_size, batch_size,\n",
        "        forcing_nu, theta_nu, sigma_true, beta=beta\n",
        "    )  # → [batch_size,2]\n",
        "\n",
        "    # 3) PDE residual + loss\n",
        "    r        = pde_residual_homotopy(model, Xb, nu)  # uses theta_nu, f_nu under the hood\n",
        "    loss_pde = loss_fn(r)\n",
        "\n",
        "    # 4) data‐loss (fixed x_data, tau_data)\n",
        "    tau_pred  = model(x_data).squeeze(-1)\n",
        "    loss_data = (tau_pred - tau_data).pow(2).mean()\n",
        "\n",
        "    # 5) total + backward + step\n",
        "    loss_total = torch.exp(-model.log_sigma_pde) * loss_pde \\\n",
        "               + torch.exp(-model.log_sigma_data) * loss_data \\\n",
        "               + model.log_sigma_pde + model.log_sigma_data\n",
        "\n",
        "    # 6) backward + step\n",
        "    loss_total.backward()\n",
        "    opt.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    #eval\n",
        "    r_eval        = pde_residual_homotopy(model, eval_Xb , None)  #evaluate the model on the true PDE\n",
        "    loss_eval     = torch.exp(-model.log_sigma_pde) * loss_fn(r_eval).item() \\\n",
        "                  + torch.exp(-model.log_sigma_data) * loss_data\\\n",
        "                  + model.log_sigma_pde + model.log_sigma_data\n",
        "\n",
        "    if loss_eval < best_loss:\n",
        "        best_loss = loss_eval\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        best_epoch = ep\n",
        "\n",
        "\n",
        "    # 7) logging & histories\n",
        "    total_history.append(loss_total.item())\n",
        "    pde_history.append(loss_pde.item())\n",
        "    data_history.append(loss_data.item())\n",
        "\n",
        "    if ep % 100 == 0:\n",
        "        print(\n",
        "            f\"[ep {ep:4d}/{n_epochs}] \"\n",
        "            f\"nu={nu:.3f}  \"\n",
        "            f\"loss_pde={loss_pde:.2e}  \"\n",
        "            f\"loss_data={loss_data:.2e}\"\n",
        "        )\n",
        "    if ep % 500 == 0:\n",
        "        lr = scheduler.get_last_lr()[0]\n",
        "        print(f\"  → epoch {ep:4d},  lr={lr:.2e}\")"
      ],
      "metadata": {
        "id": "wMX05ifLq36Z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.5) L-BFGS Fine-Tuning"
      ],
      "metadata": {
        "id": "MYTAjR_mAHHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#LBFGS fine-tuning\n",
        "\n",
        "# Reload best Adam snapshot\n",
        "print(f\"\\nLoading best Adam model (loss={best_loss:.2e}, epoch={best_epoch}) …\")\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# define L-BFGS parameters\n",
        "lbfgs = torch.optim.LBFGS(\n",
        "    model.parameters(),\n",
        "    lr=1.0,\n",
        "    max_iter=500,\n",
        "    history_size=500,\n",
        "    tolerance_grad=0,\n",
        "    tolerance_change=0,\n",
        "    line_search_fn=\"strong_wolfe\"\n",
        ")\n",
        "\n",
        "# freeze one big PDE batch for the closure\n",
        "Xb_ft = make_adaptive_batch(\n",
        "    model, pool_size, pool_size,\n",
        "    f_1, theta, sigma, beta=beta\n",
        ").detach()\n",
        "\n",
        "lbfgs_total = []\n",
        "lbfgs_pde   = []\n",
        "lbfgs_data  = []\n",
        "\n",
        "\n",
        "\n",
        "#closure function\n",
        "def closure():\n",
        "    closure.calls += 1\n",
        "    lbfgs.zero_grad()\n",
        "    # PDE term\n",
        "    r_ft    = pde_residual_homotopy(model, Xb_ft, nu=None)\n",
        "    pde_l   = loss_fn(r_ft)\n",
        "    # data term\n",
        "    tau_p   = model(x_data).squeeze(-1)\n",
        "    data_l  = (tau_p - tau_data).pow(2).mean()\n",
        "    total_l = torch.exp(-model.log_sigma_pde) * pde_l \\\n",
        "            + torch.exp(-model.log_sigma_data) * data_l \\\n",
        "            + model.log_sigma_pde + model.log_sigma_data\n",
        "\n",
        "    # record\n",
        "    lbfgs_total.append(total_l.item())\n",
        "    lbfgs_pde.append(  pde_l.item()   )\n",
        "    lbfgs_data.append( data_l.item()   )\n",
        "\n",
        "    total_l.backward()\n",
        "    return total_l\n",
        "\n",
        "closure.calls = 0"
      ],
      "metadata": {
        "id": "6Xu_iuYS2b2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# report losses before/after lbfgs\n",
        "# make sure model is in eval mode so e.g. Dropout/BatchNorm won’t move\n",
        "model.eval()\n",
        "\n",
        "# --- PDE loss (needs grad to compute ∇u inside pde_residual) ---\n",
        "r_before = pde_residual_homotopy(model, Xb_ft, nu=None)\n",
        "before_pde = loss_fn(r_before).item()\n",
        "\n",
        "# --- data loss (pure forward, safe under no_grad) ---\n",
        "with torch.no_grad():\n",
        "    pred = model(x_data).squeeze(-1)\n",
        "    before_data = (pred - tau_data).pow(2).mean().item()\n",
        "\n",
        "print(f\"Before L-BFGS → PDE {before_pde:.2e}, DATA {before_data:.2e}\")\n",
        "\n",
        "# … run your L-BFGS …\n",
        "print(\"→ Running L-BFGS …\")\n",
        "loss_after = lbfgs.step(closure)\n",
        "print(f\"L-BFGS did {closure.calls} closure calls, final total loss = {loss_after:.2e}\")\n",
        "\n",
        "# after L-BFGS, same pattern:\n",
        "model.eval()\n",
        "r_after = pde_residual_homotopy(model, Xb_ft, nu=None)\n",
        "after_pde = loss_fn(r_after).item()\n",
        "with torch.no_grad():\n",
        "    pred = model(x_data).squeeze(-1)\n",
        "    after_data = (pred - tau_data).pow(2).mean().item()\n",
        "print(f\" After L-BFGS → PDE {after_pde:.2e}, DATA {after_data:.2e}\")"
      ],
      "metadata": {
        "id": "Tq8eGZNl31KB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Plotting the results"
      ],
      "metadata": {
        "id": "CpHta0VwANgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1) Training curves"
      ],
      "metadata": {
        "id": "ShkQeYZSAQVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting three separate loss evolutions\n",
        "total_history = total_history + lbfgs_total\n",
        "pde_history   = pde_history + lbfgs_pde\n",
        "data_history  = data_history + lbfgs_data\n",
        "\n",
        "# a) Total Loss\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.semilogy(np.arange(n_hom), total_history[:n_hom], \\\n",
        "             color = 'blue', label = \"ADAM with homotopy\")\n",
        "plt.semilogy(np.arange(n_hom, n_epochs), total_history[n_hom: n_epochs], \\\n",
        "             color = 'orange', label = \"Adam on true PDE\")\n",
        "plt.semilogy(np.arange(n_epochs, len(total_history)), total_history[n_epochs:], \\\n",
        "             color = 'green', label = \"L-BFGS on true PDE\")\n",
        "plt.title(\"Total Loss vs Epoch / L-BFGS step\")\n",
        "plt.xlabel(\"step (Adam epochs or L-BFGS iter)\")\n",
        "plt.ylabel(\"loss_total\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "# b) PDE Loss\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.semilogy(np.arange(n_hom), pde_history[:n_hom], \\\n",
        "             color = 'blue', label = \"ADAM with homotopy\")\n",
        "plt.semilogy(np.arange(n_hom, n_epochs), pde_history[n_hom: n_epochs], \\\n",
        "             color = 'orange', label = \"Adam on true PDE\")\n",
        "plt.semilogy(np.arange(n_epochs, len(pde_history)), pde_history[n_epochs:], \\\n",
        "             color = 'green', label = \"L-BFGS on true PDE\")\n",
        "plt.title(\"PDE Loss vs Epoch / L-BFGS step\")\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"loss_pde\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "# c) Data Loss\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.semilogy(np.arange(n_hom), data_history[:n_hom], \\\n",
        "             color = 'blue', label = \"ADAM with homotopy\")\n",
        "plt.semilogy(np.arange(n_hom, n_epochs), data_history[n_hom: n_epochs], \\\n",
        "             color = 'orange', label = \"Adam on true PDE\")\n",
        "plt.semilogy(np.arange(n_epochs, len(pde_history)), data_history[n_epochs:], \\\n",
        "             color = 'green', label = \"L-BFGS on true PDE\")\n",
        "plt.title(\"Data Loss vs Epoch / L-BFGS step\")\n",
        "plt.xlabel(\"step\")\n",
        "plt.ylabel(\"loss_data\")\n",
        "plt.legend()\n",
        "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QC2cFlG3q6mA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2) Learned Solution vs True Solution"
      ],
      "metadata": {
        "id": "dqHCGWxwAW-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9) Visualize PINN vs True tau\n",
        "\n",
        "n_pts = 200\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(-R,R,n_pts),\n",
        "    np.linspace(-R,R,n_pts)\n",
        ")\n",
        "mask = xx**2 + yy**2 <= R**2\n",
        "pts = np.vstack([xx[mask], yy[mask]]).T\n",
        "\n",
        "with torch.no_grad():\n",
        "    inp = torch.from_numpy(pts).to(device)\n",
        "    pred = model(inp).cpu().numpy().squeeze(-1)\n",
        "\n",
        "Zp = np.zeros_like(xx); Zt = np.zeros_like(xx)\n",
        "Zp[mask], Zt[mask] = pred, true_tau_vec(xx[mask], yy[mask])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n",
        "cf1 = ax1.contourf(xx, yy, Zp, levels=50, cmap='viridis')\n",
        "ax1.set_title('PINN tau')\n",
        "fig.colorbar(cf1, ax=ax1, shrink=0.8)\n",
        "\n",
        "cf2 = ax2.contourf(xx, yy, Zt, levels=50, cmap='viridis')\n",
        "ax2.set_title('True tau')\n",
        "fig.colorbar(cf2, ax=ax2, shrink=0.8)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mICNQZu_q96H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3) Relative error"
      ],
      "metadata": {
        "id": "37SwVkTGAcBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# average relative L2 error\n",
        "#    ‖tau_pinn − tau_true‖₂ / ‖tau_true‖₂\n",
        "pred_vals = pred\n",
        "true_vals = true_tau_vec(pts[:,0], pts[:,1])\n",
        "tau_l2_norm =  np.linalg.norm(true_vals)\n",
        "glob_rel_L2 = np.linalg.norm(pred_vals - true_vals) / tau_l2_norm\n",
        "print(f\"Global relative L2 error: {glob_rel_L2:.3e}\")\n",
        "\n",
        "# pointwise relative L2 error\n",
        "Z_err = np.abs(Zp - Zt)                     # raw error\n",
        "#Z_err_norm = Z_err / tau_l2_norm\n",
        "#err_flat = Z_err[mask]              # flatten to disk\n",
        "#L2_norm = np.sqrt(np.mean(err_flat**2))\n",
        "#Z_err_norm = np.zeros_like(Z_err)\n",
        "#Z_err_norm[mask] = Z_err[mask] / L2_norm\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "cf = plt.contourf(xx, yy, Z_err, levels=50, cmap='RdBu_r')\n",
        "plt.colorbar(cf, label=r'$(|\\tau_{PINN}-\\tau|/\\|\\tau\\|_{L^2}$')\n",
        "plt.title('Normalized L²‐Error Field')\n",
        "plt.axis('equal')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GZSdlI0pyicy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}