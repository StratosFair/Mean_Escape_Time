{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3a4mgs8vPhwPuGZQq86G5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StratosFair/Mean_Escape_Time/blob/main/Duffin_oscillator/Models/curriculum_pinn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JioiNFwzetqk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import ReLU\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solving for the MET of a Duffing oscillator process in a disk with \"Boundary Adapted PINNs\": computation of solutions using Finite Element Method\n",
        "\n",
        "Based on the paper [A neural network solution of first-passage problems](https://link.springer.com/article/10.1007/s10483-024-3189-8) (Jiamin Qian, Lincong Chen & J. Q. Sun, Oct. 2024), the 2-dimensional Duffing oscillator is defined by:\n",
        "$$ d \\begin{pmatrix} X_1(t)\\\\\n",
        "X_2(t) \\end{pmatrix} = \\begin{pmatrix} X_2\\\\\n",
        "-X_1 - X_1^3 - 2\\zeta X_2 \\end{pmatrix} dt + \\begin{pmatrix} \\sqrt{2\\varepsilon}dB_1(t)\\\\\n",
        "\\sqrt{2\\zeta} dB_2(t) \\end{pmatrix}$$\n",
        "\n",
        "The infinitesimal generator of this process is given for sufficiently smooth $f$ by\n",
        "$$\\mathscr Lf: x \\mapsto b(x) \\cdot \\nabla f(x) + a(x) : \\nabla^2 f(x)$$\n",
        "where\n",
        "$$b : x = (x_1, x_2)^T \\mapsto \\begin{pmatrix} x_2\\\\ -x_1 - x_1^3 - 2\\zeta x_2 \\end{pmatrix}  $$\n",
        "and\n",
        "$$a:x \\mapsto \\frac12 \\sigma(x)\\sigma(x)^T = \\begin{pmatrix} \\varepsilon & 0\\\\ 0 & \\zeta\\end{pmatrix} $$\n",
        "\n",
        "Let $\\Omega := B_r \\equiv \\{x\\in\\mathbb R^d : \\|x\\|< r \\}$, and for all $x\\in\\Omega$, let\n",
        "\n",
        "$$T(x) := \\inf\\{t\\ge 0 : X_t \\in\\partial\\Omega\\} $$\n",
        "\n",
        "and let its first moment be denoted\n",
        "\n",
        "$$\\tau(x) := \\mathbb E[T(x)] $$\n",
        "\n",
        "We can show under some regularity conditions on the coefficients $a$ and $b$ that $\\tau:\\Omega \\to \\mathbb R$ is the (unique) solution of the BVP :\n",
        "\n",
        "$$\\begin{cases}\\mathscr L\\tau = -1 \\quad \\text{in } \\Omega,\\\\\n",
        "\\tau= 0 \\quad\\text{ on }\\partial\\Omega\\end{cases} $$"
      ],
      "metadata": {
        "id": "Rh-yuT-m-8z6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Setting up the Neural Network architecture and loss function\n",
        "\n",
        "Unlike the standard PINN, we will take an hypothesis space of Neural Networks which satisfy the boundary conditions explicitly. For this problem, we have homogenous zero Dirichlet boundary conditions, which we can enforce by multiplying our Neural Networks with an appropriate \"smooth distance approximation\" (see https://arxiv.org/abs/2104.08426). In accordance with the mentioned paper, we will take\n",
        "$$\\varphi : x\\mapsto \\frac{r^2 - \\|x\\|^2}{2r}$$\n",
        "as our smooth distance approximation.  \n",
        "\n",
        "\n",
        "With this modification, our objective to minimize becomes\n",
        "$$\\hat u := \\arg\\min_{u\\in\\mathcal{NN}}\\ \\frac1n \\sum_{i=1}^n (\\mathscr L u(x_i^c) + 1)^2 $$\n",
        "where $x_i^c$ are sampled i.i.d. with uniform distribution on $\\Omega$,\n",
        "$$\\sigma_k :x \\mapsto \\begin{cases}x^k &\\text{ if } x\\ge 0\\\\ 0 &\\text{ if } x\\le 0\\end{cases} $$\n",
        "is the ReLU$^k$ activation, and\n",
        "$\\mathcal{NN} $ is a space of feedforward Neural Networks with $\\sigma_k$ activation."
      ],
      "metadata": {
        "id": "ipb5U7KL7yL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters of the PDE\n",
        "radius = 2\n",
        "zeta = 0.08\n",
        "eps = 0.001\n",
        "\n",
        "#define our NN architecture\n",
        "power = 2 #exponent k for relu^k\n",
        "width = 50\n",
        "depth = 3\n",
        "magnitude = 0.8 #magnitude of weights at initialization\n",
        "\n",
        "#define ReLU^k activation\n",
        "\n",
        "class RePU(nn.Module):\n",
        "    def __init__(self, power = power):\n",
        "        super(RePU, self).__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.pow(torch.relu(x), self.power)\n",
        "\n",
        "\n",
        "#define the smooth distance approximation\n",
        "def smooth_distance(x):\n",
        "    norm_x = torch.linalg.norm(x, dim=-1)\n",
        "    return (radius**2 - norm_x**2)/(2*radius)\n",
        "\n",
        "\n",
        "#define hypothesis space\n",
        "class CurriculumPINN(nn.Module):\n",
        "    def __init__(self, power = 2, width = width):\n",
        "        super(CurriculumPINN,self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(2, width))\n",
        "        self.layers.append(RePU(power))\n",
        "        for _ in range(depth-1) :\n",
        "            self.layers.append(nn.Linear(width, width))\n",
        "            self.layers.append(RePU(power))\n",
        "        self.layers.append(nn.Linear(width, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "        distance =  smooth_distance(x)\n",
        "        return output**2 * distance.unsqueeze(-1)"
      ],
      "metadata": {
        "id": "8-itNA2vPyqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#all functions needed for training\n",
        "\n",
        "def derivative(dy: torch.Tensor, x: torch.Tensor, order: int = 1) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function calculates the derivative of the model at x\n",
        "    \"\"\"\n",
        "    for i in range(order):\n",
        "        dy = torch.autograd.grad(\n",
        "            dy, x, grad_outputs = torch.ones_like(dy), create_graph=True, retain_graph=True\n",
        "        )[0]\n",
        "    return dy\n",
        "\n",
        "def u_function(model: CurriculumPINN, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function evaluates the model on the input x\n",
        "    \"\"\"\n",
        "    model_input = torch.stack((x, y), axis = 1)\n",
        "    return model(model_input)\n",
        "\n",
        "\n",
        "def residual(model, x_c, y_c, zeta, eps):\n",
        "    u = u_function(model, x_c, y_c)\n",
        "    u_x = derivative(u, x_c, order=1)\n",
        "    u_y = derivative(u, y_c, order=1)\n",
        "    u_xx = derivative(u, x_c, order=2)\n",
        "    u_yy = derivative(u, y_c, order=2)\n",
        "    res = y_c * u_x \\\n",
        "        - (x_c + x_c**3 + 2*zeta*y_c) * u_y\n",
        "        + eps * u_xx + zeta * u_yy \\\n",
        "        + 1\n",
        "    return res\n",
        "\n",
        "def loss_function(model: CurriculumPINN, x_c: torch.Tensor, y_c: torch.Tensor, zeta, eps) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function evaluates the physics governing the model on the input x\n",
        "    \"\"\"\n",
        "    res = residual(model, x_c, y_c, zeta, eps)\n",
        "    return torch.mean(res**2)\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight, gain=magnitude)\n",
        "        m.bias.data.fill_(magnitude)"
      ],
      "metadata": {
        "id": "Wjer_ESbR0ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Verification of the collocation points for training\n",
        "\n",
        "sampling $N= N_c$ collocation points in $\\Omega$."
      ],
      "metadata": {
        "id": "lVUscVjoR6hB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_c = 2048 #number of points in the domain\n",
        "\n",
        "#definition of X_c_train : N_c points in the disk\n",
        "t = np.random.uniform(0,2*np.pi, N_c)\n",
        "rho = np.sqrt(np.random.uniform(0,radius**2, N_c)) #uniform distribution on the disk\n",
        "x_c = rho * np.cos(t)\n",
        "y_c = rho * np.sin(t)\n",
        "X_c_train = np.vstack( (x_c, y_c) )\n",
        "\n",
        "#shuffling X_c_train\n",
        "index = np.arange(0, N_c)\n",
        "np.random.shuffle(index)\n",
        "X_c_train = X_c_train[:,index]"
      ],
      "metadata": {
        "id": "XC_qfCnWSFyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the collocation points\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(cx,cy,'k-', alpha=.6)\n",
        "plt.scatter(x_c,y_c)"
      ],
      "metadata": {
        "id": "A8PSKrxGSGkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingData(Dataset):\n",
        "    def __init__(self, X_c):\n",
        "        self.X_c = X_c\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X_c.shape[1]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X_c = self.X_c[:, index]\n",
        "        return X_c"
      ],
      "metadata": {
        "id": "5ryH2hjQSMp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Training loop function"
      ],
      "metadata": {
        "id": "c6QXroqASO7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, optimizer, zeta, eps, losses):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    n_display = (N_c//batch_size)//3\n",
        "\n",
        "    for i, X_c in enumerate(dataloader):\n",
        "\n",
        "        x_c = X_c[0,:]\n",
        "        y_c = X_c[1,:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_function(model, x_c, y_c, zeta, eps)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if (i+1) % n_display == 0 :\n",
        "            last_loss = running_loss / n_display # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            losses.append(last_loss)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ],
      "metadata": {
        "id": "_0YrVEK1S7mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Curriculum Training"
      ],
      "metadata": {
        "id": "ZQUHMobzS_jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_curriculum = 1\n",
        "zetas = [zeta] * nb_curriculum\n",
        "epsis = np.linspace(theta * nb_curriculum, eps, nb_curriculum).tolist()\n",
        "losses = [[] for _ in range(nb_curriculum)]\n",
        "\n",
        "#training parameters\n",
        "batch_size = 32\n",
        "learning_rate_adam = 5e-4\n",
        "learning_rate_sgd = 5e-2\n",
        "method = 'Adam' #'SGD' or 'Adam', Adam strongly recommended\n",
        "n_epochs = 1500\n",
        "n_decreases = 10\n",
        "damping = 1e-2\n",
        "gamma = damping**(1/n_decreases) #damp the learning rate by 1e-2 by the end of training\n",
        "\n",
        "#first training batch\n",
        "\n",
        "#convert numpy array to tensor and load it\n",
        "X_c_train_tensor = torch.from_numpy(X_c_train).requires_grad_(True).float()\n",
        "dataset = TrainingData(X_c_train_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "0kbQdCiZTAQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(nb_curriculum):\n",
        "\n",
        "    print(\"------CURRICULUM TRAINING: {0}/{1}------\".format(i+1, nb_curriculum))\n",
        "\n",
        "    #update parameter values\n",
        "    zeta = zetas[i]\n",
        "    eps = epsis[i]\n",
        "\n",
        "    #load model and set optimizer\n",
        "    model = CurriculumPINN()\n",
        "    if i > 0:\n",
        "        model.load_state_dict(\\\n",
        "        torch.load(f'curriculum_pinn_{method}.pt'))\n",
        "    else:\n",
        "        model.apply(init_weights)\n",
        "\n",
        "    if method == 'Adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_adam)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=n_epochs//n_decreases,\n",
        "                                                gamma=gamma)\n",
        "\n",
        "    elif method == 'SGD':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate_sgd)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=n_epochs//n_decreases,\n",
        "                                                gamma=gamma)\n",
        "\n",
        "    #train model for current curriculum\n",
        "    epoch_number = 0\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        epoch_number += 1\n",
        "        print('EPOCH {}:'.format(epoch_number))\n",
        "\n",
        "        # Make sure gradient tracking is on, and do a pass over the data\n",
        "        model.train(True)\n",
        "        avg_loss = train_one_epoch(model, optimizer, zeta, eps, losses[i])\n",
        "        scheduler.step()\n",
        "        print('LOSS train {}'.format(avg_loss))\n",
        "\n",
        "    #save trained model and reset optimizer\n",
        "    torch.save(model.state_dict(), f'curriculum_pinn_{method}.pt')\n",
        "\n",
        "    #regenerate collocation points\n",
        "    #definition of X_c_train : N_c points in the disk\n",
        "    t = np.random.uniform(0,2*np.pi, N_c)\n",
        "    rho = np.sqrt(np.random.uniform(0,radius**2, N_c)) #uniform distribution on the disk\n",
        "    x_c = rho * np.cos(t)\n",
        "    y_c = rho * np.sin(t)\n",
        "    X_c_train = np.vstack( (x_c, y_c) )\n",
        "    #shuffling X_c_train\n",
        "    index = np.arange(0, N_c)\n",
        "    np.random.shuffle(index)\n",
        "    X_c_train = X_c_train[:,index]\n",
        "\n",
        "    #convert numpy array to tensor and load it\n",
        "    X_c_train_tensor = torch.from_numpy(X_c_train).requires_grad_(True).float()\n",
        "    dataset = TrainingData(X_c_train_tensor)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "elzpjZUHTcFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Visualizing the training in each curriculum"
      ],
      "metadata": {
        "id": "AO4_yN84TeWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,3)\n",
        "axs[0].loglog(losses[nb_curriculum//3])\n",
        "axs[0].set_title('({2}): {0}/{1}'.format(nb_curriculum//3,nb_curriculum,method))\n",
        "axs[1].loglog(losses[2 * nb_curriculum//3])\n",
        "axs[1].set_title('({2}): {0}/{1}'.format(2 * nb_curriculum//3,nb_curriculum,method))\n",
        "axs[2].loglog(losses[nb_curriculum-1])\n",
        "axs[2].set_title('({1}): {0}/{0}'.format(nb_curriculum,method))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fHEYUG0FTjQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Visualizing the learned solution"
      ],
      "metadata": {
        "id": "RIy6mdU8ToZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model\n",
        "model = CurriculumPINN()\n",
        "model.load_state_dict(\\\n",
        "torch.load(f'curriculum_pinn_{method}.pt'))\n",
        "\n",
        "#evaluate the model on a uniform grid\n",
        "n_points = 150\n",
        "tt = np.linspace(-1, 1, n_points) * radius\n",
        "xx, yy = np.meshgrid(tt, tt)  # create unit square grid\n",
        "xx, yy = np.where(xx**2 + yy**2 <= radius**2, xx, 0), np.where(xx**2 + yy**2 <= radius**2 , yy, 0) #(https://stackoverflow.com/questions/15733530/)\n",
        "#zz_true = true_solution_vectorized(xx,yy)\n",
        "\n",
        "input = torch.from_numpy(np.vstack((xx.ravel(),yy.ravel())).T).float()#.requires_grad_(False)\n",
        "learned_sol = model(input)\n",
        "\n",
        "#plot\n",
        "learned_sol_np = learned_sol.detach().numpy().reshape(xx.shape)\n",
        "#learned_sol_smooth = ndimage.gaussian_filter(learned_sol_np, sigma=0.1, order=0) #smoothing for visualization\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(cx, cy,'k-', alpha=.2)\n",
        "contour = ax.contourf(xx, yy, learned_sol_np, levels=200)\n",
        "ax.set_title(f'Curriculum PINN solution ({method})')\n",
        "cb = fig.colorbar(contour, ax=ax)"
      ],
      "metadata": {
        "id": "H8ajidIPTrPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verify that the model is zero on the boundary\n",
        "circle = torch.from_numpy(np.vstack((cx.ravel(),cy.ravel())).T).float().requires_grad_(False)\n",
        "torch.sum(model(circle)**2) #== 0"
      ],
      "metadata": {
        "id": "zbAtgo_HTvdC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}