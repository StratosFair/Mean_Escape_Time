{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk8OQ/w7j/nZO30nJba/7h"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JioiNFwzetqk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import ReLU\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from functools import partial\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solving for the MET of a particle in double well potential landscape in a disk with \"Boundary Adapted PINNs\": comparison with exact solution"
      ],
      "metadata": {
        "id": "r-TH0s2h7GIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Setup\n",
        "\n",
        "For $\\theta,\\sigma,r >0$ some fixed parameters, we let $\\Omega := B_r \\equiv \\{x\\in\\mathbb R^d : \\|x\\|< r \\}$, $x_1, x_2\\in\\Omega$ be fixed and define the process ($d=2$ in our illustration) :\n",
        "\n",
        "$$\\begin{cases} dX_t &= -\\theta (X_t - x_1 + X_t - x_2) dt + \\sigma dB_t \\\\\n",
        "X_0 &= x \\in \\Omega \\end{cases} $$\n",
        "\n",
        "For all $x\\in\\Omega$, let\n",
        "\n",
        "$$T(x) := \\inf\\{t\\ge 0 : X_t \\in\\partial\\Omega\\} $$\n",
        "\n",
        "and let its first moment be denoted\n",
        "\n",
        "$$\\tau(x) := \\mathbb E[T(x)] $$\n",
        "\n",
        "We can show under some regularity conditions on $\\Omega$ that $\\tau$ is the (unique) solution of the BVP :\n",
        "\n",
        "$$\\begin{cases} -\\mathcal{L}u(x) &= 1 \\text{ for all } x\\in\\Omega \\\\\n",
        "u(x) &= 0 \\text{ for all } x\\in\\partial\\Omega \\end{cases} $$\n",
        "\n",
        "where $\\mathcal L$ is the infinitesimal generator of the Ornstein-Uhlenbeck process, given by\n",
        "$$\\mathcal Lu : x \\mapsto -\\theta(2x - x_1 - x_2)\\cdot \\nabla u(x) + \\frac{\\sigma^2}{2}\\Delta u(x) $$"
      ],
      "metadata": {
        "id": "EHGrDIf1D4x8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Setting up the Neural Network architecture and loss function\n",
        "\n",
        "Unlike the standard PINN, we will take an hypothesis space of Neural Networks which satisfy the boundary conditions explicitly. For this problem, we have homogenous zero Dirichlet boundary conditions, which we can enforce by multiplying our Neural Networks with an appropriate \"smooth distance approximation\" (see https://arxiv.org/abs/2104.08426). In accordance with the mentioned paper, we will take\n",
        "$$\\varphi : x\\mapsto \\frac{r^2 - \\|x\\|^2}{2r}$$\n",
        "as our smooth distance approximation.  \n",
        "\n",
        "\n",
        "With this modification, our objective to minimize becomes\n",
        "$$\\hat u := \\arg\\min_{u\\in\\mathcal{NN}_\\varphi}\\ \\frac1n \\sum_{i=1}^n (\\mathcal L u(x_i^c) + 1)^2 $$\n",
        "where $x_i^c$ are sampled i.i.d. with uniform distribution on $\\Omega$,\n",
        "$$\\mathcal{NN}_\\varphi:=\\left\\{x\\mapsto \\varphi(x) \\cdot T_L\\circ \\sigma^k \\circ T_{L-1}\\circ\\cdots\\circ \\sigma^k\\circ T_1 (x)\\right\\}, $$\n",
        "for\n",
        "$$T_\\ell : \\mathbb{R}^{\\ell-1}\\to\\mathbb{R}^\\ell $$\n",
        "affine-linear maps of appropriate input-output dimensions, where\n",
        "$$\\sigma_k :x \\mapsto \\begin{cases}x^k &\\text{ if } x\\ge 0\\\\ 0 &\\text{ if } x\\le 0\\end{cases} $$\n",
        "is the ReLU$^k$ activation, which is understood element-wise when applied to vectors."
      ],
      "metadata": {
        "id": "ipb5U7KL7yL-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "oLIJ-Qt7etqq"
      },
      "outputs": [],
      "source": [
        "#pde parameters\n",
        "radius = 2.0\n",
        "sigma = 1.0\n",
        "theta = 0.5\n",
        "x_1 = torch.tensor(1/np.sqrt(2), dtype=torch.float64)\n",
        "y_1 = torch.tensor(1/np.sqrt(2), dtype=torch.float64)\n",
        "x_2 = torch.tensor(-1/np.sqrt(2), dtype=torch.float64)\n",
        "y_2 = torch.tensor(-1/np.sqrt(2), dtype=torch.float64)\n",
        "\n",
        "radius_double = torch.tensor(2.0, dtype=torch.float64)\n",
        "sigma_double = torch.tensor(1.0, dtype=torch.float64)\n",
        "theta_double = torch.tensor(0.5, dtype=torch.float64)\n",
        "\n",
        "\n",
        "#define our NN architecture\n",
        "power = 2 #exponent k for relu^k\n",
        "width = 50\n",
        "depth = 3\n",
        "magnitude = 0.8 #magnitude of weights at initialization\n",
        "\n",
        "#define ReLU^k activation\n",
        "\n",
        "class RePU(nn.Module):\n",
        "    def __init__(self, power = power):\n",
        "        super(RePU, self).__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.pow(torch.relu(x), self.power)\n",
        "\n",
        "\n",
        "#define the smooth distance approximation\n",
        "def smooth_distance(x):\n",
        "    norm_x = torch.linalg.norm(x, dim=-1)\n",
        "    return (radius_double**2 - norm_x**2)/(2*radius_double)\n",
        "\n",
        "\n",
        "#define hypothesis space\n",
        "class BoundaryPINN(nn.Module):\n",
        "    def __init__(self, power = power, width = width, depth = depth):\n",
        "        super(BoundaryPINN,self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(2, width))\n",
        "        self.layers.append(RePU(power))\n",
        "        for _ in range(depth-1) :\n",
        "            self.layers.append(nn.Linear(width, width))\n",
        "            self.layers.append(RePU(power))\n",
        "        self.layers.append(nn.Linear(width, 1))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "        distance =  smooth_distance(x)\n",
        "        return output**2 * distance.unsqueeze(-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "bsJorxBFqwUg"
      },
      "outputs": [],
      "source": [
        "#all functions needed for training\n",
        "\n",
        "def derivative(dy: torch.Tensor, x: torch.Tensor, order: int = 1) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function calculates the derivative of the model at x\n",
        "    \"\"\"\n",
        "    for i in range(order):\n",
        "        dy = torch.autograd.grad(\n",
        "            dy, x, grad_outputs = torch.ones_like(dy, dtype=torch.float64),\n",
        "            create_graph=True, retain_graph=True\n",
        "        )[0]\n",
        "    return dy\n",
        "\n",
        "def u_function(model: BoundaryPINN, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function evaluates the model on the input x\n",
        "    \"\"\"\n",
        "    model_input = torch.stack((x, y), axis = 1)\n",
        "    return model(model_input)\n",
        "\n",
        "\n",
        "def residual(model, x_c, y_c, theta, sigma):\n",
        "    u = u_function(model, x_c, y_c)\n",
        "    u_x = derivative(u, x_c, order=1)\n",
        "    u_y = derivative(u, y_c, order=1)\n",
        "    u_xx = derivative(u, x_c, order=2)\n",
        "    u_yy = derivative(u, y_c, order=2)\n",
        "    res = - theta * ((2 * x_c-x_1 - x_2) * u_x \\\n",
        "                     + (2 * y_c- y_1 - y_2) * u_y) \\\n",
        "        + sigma**2 * (u_xx + u_yy)/2 \\\n",
        "        + 1\n",
        "    return res\n",
        "\n",
        "def loss_function(model: BoundaryPINN, x_c: torch.Tensor, y_c: torch.Tensor, theta, sigma) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function evaluates the physics governing the model on the input x\n",
        "    \"\"\"\n",
        "    res = residual(model, x_c, y_c, theta, sigma)\n",
        "    return torch.mean(res**2)\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight, gain=magnitude)\n",
        "        m.bias.data.fill_(magnitude)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Verification of the collocation points for training\n",
        "\n",
        "sampling $n = N_c$ collocation points in $\\Omega$."
      ],
      "metadata": {
        "id": "fEr9LHDwDPhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "1IRZCzngetqr"
      },
      "outputs": [],
      "source": [
        "N_c = 2048 #number of points in the domain\n",
        "\n",
        "#definition of X_c_train : N_c points in the disk\n",
        "t = np.random.uniform(0,2*np.pi, N_c)\n",
        "rho = np.sqrt(np.random.uniform(0,radius**2, N_c)) #uniform distribution on the disk\n",
        "x_c = rho * np.cos(t)\n",
        "y_c = rho * np.sin(t)\n",
        "X_c_train = np.vstack( (x_c, y_c) )\n",
        "\n",
        "#shuffling X_c_train\n",
        "index = np.arange(0, N_c)\n",
        "np.random.shuffle(index)\n",
        "X_c_train = X_c_train[:,index]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the collocation points\n",
        "\n",
        "#circle\n",
        "a = np.linspace(0, 2*np.pi, 50)\n",
        "cx,cy = np.cos(a) * radius , np.sin(a)*radius\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.plot(cx,cy,'k-', alpha=.6)\n",
        "plt.scatter(x_c,y_c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "mTB4fMOHq-oK",
        "outputId": "5a30e5e1-0440-4543-f0cc-4e447422ab09"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7d39e29bb310>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingData(Dataset):\n",
        "    def __init__(self, X_c):\n",
        "        self.X_c = X_c\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X_c.shape[1]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X_c = self.X_c[:, index]\n",
        "        return X_c"
      ],
      "metadata": {
        "id": "EQE-azXarLvi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Training loop function"
      ],
      "metadata": {
        "id": "6WRykQDFD55F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#adam\n",
        "def train_one_epoch(model, optimizer, theta, sigma, losses):\n",
        "    running_loss = 0.\n",
        "    last_loss = 0.\n",
        "    n_display = (N_c//batch_size)//3\n",
        "\n",
        "    for i, X_c in enumerate(dataloader):\n",
        "\n",
        "        x_c = X_c[0,:]\n",
        "        y_c = X_c[1,:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_function(model, x_c, y_c, theta, sigma)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if (i+1) % n_display == 0 :\n",
        "            last_loss = running_loss / n_display # loss per batch\n",
        "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
        "            losses.append(last_loss)\n",
        "            running_loss = 0.\n",
        "\n",
        "    return last_loss"
      ],
      "metadata": {
        "id": "RZzB2lSVrVHb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#l-bfgs\n",
        "def training_loop_lbfgs(model, optimizer, theta, sigma, losses):\n",
        "\n",
        "    iter = 0\n",
        "\n",
        "    for X_c in dataloader_bfgs:\n",
        "        x_c = X_c[0,:]  # Collocation points (x-coordinates)\n",
        "        y_c = X_c[1,:]  # Collocation points (y-coordinates)\n",
        "\n",
        "    # Define the closure function required by L-BFGS\n",
        "    def closure():\n",
        "        nonlocal iter\n",
        "        optimizer.zero_grad()  # Clear previous gradients\n",
        "\n",
        "        # Compute total loss\n",
        "        loss = loss_function(model, x_c, y_c, theta, sigma)\n",
        "        loss.backward()  # Backpropagation to compute gradients\n",
        "\n",
        "        # Log the loss for the current iteration\n",
        "        print(f'L-BFGS Iteration {iter}: Loss = {loss.item()}')\n",
        "        losses.append(loss.item())\n",
        "        iter += 1\n",
        "        return loss\n",
        "\n",
        "    # Perform one optimization step\n",
        "    optimizer.step(closure)\n",
        "\n",
        "    final_loss = loss_function(model, x_c, y_c, theta, sigma).item()\n",
        "\n",
        "    # Gather data and report\n",
        "    print('Full-batch loss: {}'.format(final_loss))\n",
        "    losses.append(final_loss)  # Append the final loss to the losses list\n",
        "\n",
        "    return final_loss"
      ],
      "metadata": {
        "id": "l5V-CvBf_h9t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Curriculum training"
      ],
      "metadata": {
        "id": "wVZZ5j7GjO1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_curriculum = 1\n",
        "thetas = [theta] * nb_curriculum\n",
        "sigmas = [sigma] * nb_curriculum #np.linspace(theta * nb_curriculum, sigma, nb_curriculum).tolist()\n",
        "losses = [[] for _ in range(nb_curriculum)]\n",
        "losses_lbfgs = [[] for _ in range(nb_curriculum)]\n",
        "\n",
        "#training parameters\n",
        "n_batches = 8 #<= N_c\n",
        "batch_size = max(4, N_c//n_batches)\n",
        "learning_rate_adam = 5e-4\n",
        "method = 'Adam' #'SGD' or 'Adam', Adam strongly recommended\n",
        "n_epochs = 10_000\n",
        "n_decreases = 200\n",
        "damping = 1e-3\n",
        "gamma = damping**(1/n_decreases) #damp the learning rate by a factor of 'damping' by the end of training\n",
        "\n",
        "#first training batch\n",
        "#convert numpy array to tensor and load it\n",
        "X_c_train_tensor = torch.from_numpy(X_c_train).requires_grad_(True).double()\n",
        "dataset = TrainingData(X_c_train_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "dataloader_bfgs = DataLoader(dataset, batch_size=len(dataset), shuffle=True)"
      ],
      "metadata": {
        "id": "qQ_ljTlxjM_G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#download a pretrained model\n",
        "#!wget https://raw.githubusercontent.com/StratosFair/Mean_Escape_Time/main/OU_process/Models/boundary_pinn_theta=0.5_sigma=1.pt -O boundary_pinn_pretrained.pt"
      ],
      "metadata": {
        "id": "hHOqYLFiOlHA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(nb_curriculum):\n",
        "\n",
        "    print(\"------CURRICULUM TRAINING: {0}/{1}------\".format(i+1, nb_curriculum))\n",
        "\n",
        "    #update parameter values\n",
        "    theta = thetas[i]\n",
        "    sigma = sigmas[i]\n",
        "\n",
        "    #load model and set optimizer\n",
        "    if i > 0:\n",
        "        checkpoint = torch.load(f'boundary_pinn_curriculum_{i}_lbfgs.pt')\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model = model.double()\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Scale the learning rate back up\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = learning_rate_adam  # Reset to original learning rate\n",
        "\n",
        "    else:\n",
        "        model = BoundaryPINN(power=power, width=width, depth=depth).double()\n",
        "\n",
        "        if os.path.isfile('boundary_pinn_pretrained.pt'):\n",
        "            pretrained_checkpoint = torch.load('boundary_pinn_pretrained.pt')\n",
        "            model.load_state_dict(pretrained_checkpoint['model_state_dict'])\n",
        "            model = model.double()\n",
        "\n",
        "        else:\n",
        "            model.apply(init_weights)\n",
        "\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_adam)\n",
        "\n",
        "    # Define or continue scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer, step_size=n_epochs // n_decreases, gamma=gamma\n",
        "    )\n",
        "\n",
        "    #train model for current curriculum\n",
        "    for epoch in range(n_epochs):\n",
        "        print('EPOCH {}:'.format(epoch + 1))\n",
        "\n",
        "        # Make sure gradient tracking is on, and do a pass over the data\n",
        "        model.train(True)\n",
        "        avg_loss = train_one_epoch(model, optimizer, theta, sigma, losses[i])\n",
        "        scheduler.step()\n",
        "        print('LOSS train {}'.format(avg_loss))\n",
        "\n",
        "    #save trained model\n",
        "    if n_epochs>0:\n",
        "        torch.save({'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    }, f'boundary_pinn_curriculum_{i+1}.pt')\n",
        "\n",
        "    # ---- SWITCH TO L-BFGS FOR FINE-TUNING ----\n",
        "        print(\"Switching to L-BFGS for fine-tuning...\")\n",
        "\n",
        "    # Initialize L-BFGS optimizer\n",
        "    lbfgs_optimizer = torch.optim.LBFGS(model.parameters(),\n",
        "                                        max_iter=1000, # Allow more iterations\n",
        "                                        tolerance_grad=1e-10,      # Stricter gradient tolerance\n",
        "                                        tolerance_change=1e-12,   # Stricter tolerance for parameter changes\n",
        "                                        line_search_fn=\"strong_wolfe\"  # More robust line search\n",
        "                                        )\n",
        "\n",
        "    # Train one epoch with L-BFGS\n",
        "    final_loss = training_loop_lbfgs(model, lbfgs_optimizer, theta, sigma, losses_lbfgs[i])\n",
        "\n",
        "    # Log the final loss\n",
        "    print(f\"L-BFGS fine-tuning loss: {final_loss}\")\n",
        "\n",
        "    # Save the model after L-BFGS fine-tuning\n",
        "    torch.save({'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': lbfgs_optimizer.state_dict(),\n",
        "            }, f'boundary_pinn_curriculum_{i + 1}_lbfgs.pt')"
      ],
      "metadata": {
        "id": "elzpjZUHTcFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e1514f-4aaa-458c-dd83-9e32ee1050ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------CURRICULUM TRAINING: 1/1------\n",
            "EPOCH 1:\n",
            "  batch 2 loss: 5.473665558332595\n",
            "  batch 4 loss: 1.5421808258980443\n",
            "  batch 6 loss: 1.2745351280645048\n",
            "  batch 8 loss: 11.319458927790247\n",
            "LOSS train 11.319458927790247\n",
            "EPOCH 2:\n",
            "  batch 2 loss: 3.3799690314019815\n",
            "  batch 4 loss: 0.7049068702254857\n",
            "  batch 6 loss: 1.5966280826338106\n",
            "  batch 8 loss: 1.7587756640698702\n",
            "LOSS train 1.7587756640698702\n",
            "EPOCH 3:\n",
            "  batch 2 loss: 2.0875575405862556\n",
            "  batch 4 loss: 2.2421637429631627\n",
            "  batch 6 loss: 9.269608954469629\n",
            "  batch 8 loss: 11.894096359484745\n",
            "LOSS train 11.894096359484745\n",
            "EPOCH 4:\n",
            "  batch 2 loss: 0.7515397989839555\n",
            "  batch 4 loss: 0.9561280733119842\n",
            "  batch 6 loss: 0.6504900933437566\n",
            "  batch 8 loss: 1.3700549119725793\n",
            "LOSS train 1.3700549119725793\n",
            "EPOCH 5:\n",
            "  batch 2 loss: 1.007185406317862\n",
            "  batch 4 loss: 1.20046529521811\n",
            "  batch 6 loss: 1.6004147179661858\n",
            "  batch 8 loss: 1.6559068932761738\n",
            "LOSS train 1.6559068932761738\n",
            "EPOCH 6:\n",
            "  batch 2 loss: 2.376841198507519\n",
            "  batch 4 loss: 1.512094122387353\n",
            "  batch 6 loss: 1.3349808730932586\n",
            "  batch 8 loss: 0.8045234635837221\n",
            "LOSS train 0.8045234635837221\n",
            "EPOCH 7:\n",
            "  batch 2 loss: 0.9068877010847802\n",
            "  batch 4 loss: 1.3372883573148628\n",
            "  batch 6 loss: 1.8930819245619832\n",
            "  batch 8 loss: 0.9867334171607809\n",
            "LOSS train 0.9867334171607809\n",
            "EPOCH 8:\n",
            "  batch 2 loss: 0.8575731453170825\n",
            "  batch 4 loss: 0.9480358903071\n",
            "  batch 6 loss: 1.120478467561731\n",
            "  batch 8 loss: 0.961028515905942\n",
            "LOSS train 0.961028515905942\n",
            "EPOCH 9:\n",
            "  batch 2 loss: 0.7858262812563419\n",
            "  batch 4 loss: 0.8527706156869466\n",
            "  batch 6 loss: 1.1217329532337001\n",
            "  batch 8 loss: 0.5896145953998748\n",
            "LOSS train 0.5896145953998748\n",
            "EPOCH 10:\n",
            "  batch 2 loss: 0.8303857788030404\n",
            "  batch 4 loss: 0.7129848515998849\n",
            "  batch 6 loss: 0.7792974661385936\n",
            "  batch 8 loss: 0.8563089381493636\n",
            "LOSS train 0.8563089381493636\n",
            "EPOCH 11:\n",
            "  batch 2 loss: 1.2369033201429893\n",
            "  batch 4 loss: 0.7146075433940959\n",
            "  batch 6 loss: 0.7284941475787235\n",
            "  batch 8 loss: 0.6825347489555537\n",
            "LOSS train 0.6825347489555537\n",
            "EPOCH 12:\n",
            "  batch 2 loss: 0.8697133282805141\n",
            "  batch 4 loss: 1.0789259573657828\n",
            "  batch 6 loss: 0.5336429120012729\n",
            "  batch 8 loss: 0.9064576723366933\n",
            "LOSS train 0.9064576723366933\n",
            "EPOCH 13:\n",
            "  batch 2 loss: 1.009120944900124\n",
            "  batch 4 loss: 1.5370898977436953\n",
            "  batch 6 loss: 0.5555481468142389\n",
            "  batch 8 loss: 0.641236955309933\n",
            "LOSS train 0.641236955309933\n",
            "EPOCH 14:\n",
            "  batch 2 loss: 0.4623906838392189\n",
            "  batch 4 loss: 0.7405405038936435\n",
            "  batch 6 loss: 0.585230169915385\n",
            "  batch 8 loss: 1.095445169696906\n",
            "LOSS train 1.095445169696906\n",
            "EPOCH 15:\n",
            "  batch 2 loss: 0.9496854791498901\n",
            "  batch 4 loss: 1.464347041902178\n",
            "  batch 6 loss: 1.4174410705967189\n",
            "  batch 8 loss: 0.8156521952845865\n",
            "LOSS train 0.8156521952845865\n",
            "EPOCH 16:\n",
            "  batch 2 loss: 21.072618188683677\n",
            "  batch 4 loss: 1.5648740703667845\n",
            "  batch 6 loss: 1.6429882246561844\n",
            "  batch 8 loss: 1.7074268951355225\n",
            "LOSS train 1.7074268951355225\n",
            "EPOCH 17:\n",
            "  batch 2 loss: 0.8425198825328775\n",
            "  batch 4 loss: 37.85181913584196\n",
            "  batch 6 loss: 6.570989415308899\n",
            "  batch 8 loss: 3.3896464485197164\n",
            "LOSS train 3.3896464485197164\n",
            "EPOCH 18:\n",
            "  batch 2 loss: 1.5299832523778667\n",
            "  batch 4 loss: 1.6165818790581585\n",
            "  batch 6 loss: 1.1230233372927199\n",
            "  batch 8 loss: 0.9259345285787198\n",
            "LOSS train 0.9259345285787198\n",
            "EPOCH 19:\n",
            "  batch 2 loss: 0.44890201035679467\n",
            "  batch 4 loss: 4.764320867628516\n",
            "  batch 6 loss: 1.2885130360891286\n",
            "  batch 8 loss: 13.392398718964333\n",
            "LOSS train 13.392398718964333\n",
            "EPOCH 20:\n",
            "  batch 2 loss: 4.765264698721195\n",
            "  batch 4 loss: 1.1943419960665391\n",
            "  batch 6 loss: 25.00826026782419\n",
            "  batch 8 loss: 2.9999367770090073\n",
            "LOSS train 2.9999367770090073\n",
            "EPOCH 21:\n",
            "  batch 2 loss: 1.9538259878081181\n",
            "  batch 4 loss: 1.0608514683827297\n",
            "  batch 6 loss: 6.07780594146595\n",
            "  batch 8 loss: 2.1152299424664287\n",
            "LOSS train 2.1152299424664287\n",
            "EPOCH 22:\n",
            "  batch 2 loss: 1.1833680102824657\n",
            "  batch 4 loss: 1.2615478549848196\n",
            "  batch 6 loss: 0.7935964663767225\n",
            "  batch 8 loss: 1.0810199469618562\n",
            "LOSS train 1.0810199469618562\n",
            "EPOCH 23:\n",
            "  batch 2 loss: 1.111507993170313\n",
            "  batch 4 loss: 0.8905217234044782\n",
            "  batch 6 loss: 0.92892479049391\n",
            "  batch 8 loss: 1.1160119684280116\n",
            "LOSS train 1.1160119684280116\n",
            "EPOCH 24:\n",
            "  batch 2 loss: 1.4931368404797918\n",
            "  batch 4 loss: 0.6218723972832254\n",
            "  batch 6 loss: 1.1420270923168214\n",
            "  batch 8 loss: 1.4831021971748548\n",
            "LOSS train 1.4831021971748548\n",
            "EPOCH 25:\n",
            "  batch 2 loss: 1.1668971336852505\n",
            "  batch 4 loss: 0.5317570549864735\n",
            "  batch 6 loss: 1.2539693091381545\n",
            "  batch 8 loss: 1.078195539152573\n",
            "LOSS train 1.078195539152573\n",
            "EPOCH 26:\n",
            "  batch 2 loss: 0.8790514665073592\n",
            "  batch 4 loss: 0.8199721777279139\n",
            "  batch 6 loss: 1.1626480666841155\n",
            "  batch 8 loss: 1.0231884114540604\n",
            "LOSS train 1.0231884114540604\n",
            "EPOCH 27:\n",
            "  batch 2 loss: 0.85949296348848\n",
            "  batch 4 loss: 1.06698619627215\n",
            "  batch 6 loss: 0.9481271160942777\n",
            "  batch 8 loss: 0.9034414265254982\n",
            "LOSS train 0.9034414265254982\n",
            "EPOCH 28:\n",
            "  batch 2 loss: 0.8870481613216388\n",
            "  batch 4 loss: 0.9145120498048267\n",
            "  batch 6 loss: 0.9121031144946521\n",
            "  batch 8 loss: 1.71532426910766\n",
            "LOSS train 1.71532426910766\n",
            "EPOCH 29:\n",
            "  batch 2 loss: 1.7074766927991392\n",
            "  batch 4 loss: 1.137787089310141\n",
            "  batch 6 loss: 0.8712168749426267\n",
            "  batch 8 loss: 1.1497651593233507\n",
            "LOSS train 1.1497651593233507\n",
            "EPOCH 30:\n",
            "  batch 2 loss: 0.8302475841839295\n",
            "  batch 4 loss: 0.7856395831990641\n",
            "  batch 6 loss: 1.14613233848877\n",
            "  batch 8 loss: 1.0095509583024982\n",
            "LOSS train 1.0095509583024982\n",
            "EPOCH 31:\n",
            "  batch 2 loss: 1.0065330677590367\n",
            "  batch 4 loss: 0.39733778363017996\n",
            "  batch 6 loss: 1.0613266854759913\n",
            "  batch 8 loss: 1.206480320400182\n",
            "LOSS train 1.206480320400182\n",
            "EPOCH 32:\n",
            "  batch 2 loss: 1.1589825344883957\n",
            "  batch 4 loss: 1.0703928493018058\n",
            "  batch 6 loss: 0.6261805179897607\n",
            "  batch 8 loss: 0.9309378169592697\n",
            "LOSS train 0.9309378169592697\n",
            "EPOCH 33:\n",
            "  batch 2 loss: 1.0766457382999688\n",
            "  batch 4 loss: 0.8037039649098727\n",
            "  batch 6 loss: 1.1149786223672906\n",
            "  batch 8 loss: 0.7660422671996504\n",
            "LOSS train 0.7660422671996504\n",
            "EPOCH 34:\n",
            "  batch 2 loss: 0.7447751634198922\n",
            "  batch 4 loss: 0.8761373039775051\n",
            "  batch 6 loss: 0.7583545596799399\n",
            "  batch 8 loss: 0.9009008483273004\n",
            "LOSS train 0.9009008483273004\n",
            "EPOCH 35:\n",
            "  batch 2 loss: 0.7767129118731875\n",
            "  batch 4 loss: 1.0846436935229304\n",
            "  batch 6 loss: 0.7728358274163927\n",
            "  batch 8 loss: 0.8148314795384414\n",
            "LOSS train 0.8148314795384414\n",
            "EPOCH 36:\n",
            "  batch 2 loss: 0.6648561883081134\n",
            "  batch 4 loss: 1.053376258625229\n",
            "  batch 6 loss: 0.9366954061884227\n",
            "  batch 8 loss: 0.9410061169702655\n",
            "LOSS train 0.9410061169702655\n",
            "EPOCH 37:\n",
            "  batch 2 loss: 8.146172177418899\n",
            "  batch 4 loss: 2.2979090275923078\n",
            "  batch 6 loss: 0.6662441174967026\n",
            "  batch 8 loss: 0.925268280072965\n",
            "LOSS train 0.925268280072965\n",
            "EPOCH 38:\n",
            "  batch 2 loss: 2.843328174245589\n",
            "  batch 4 loss: 0.4798925109433281\n",
            "  batch 6 loss: 1.1368624482626062\n",
            "  batch 8 loss: 1.3196778395253006\n",
            "LOSS train 1.3196778395253006\n",
            "EPOCH 39:\n",
            "  batch 2 loss: 0.9711377153331322\n",
            "  batch 4 loss: 1.4566800802263455\n",
            "  batch 6 loss: 2.0251627246533057\n",
            "  batch 8 loss: 1.8460174459803014\n",
            "LOSS train 1.8460174459803014\n",
            "EPOCH 40:\n",
            "  batch 2 loss: 1.0733882831793629\n",
            "  batch 4 loss: 0.9978970864335526\n",
            "  batch 6 loss: 1.0305438622130048\n",
            "  batch 8 loss: 2.5886597991808764\n",
            "LOSS train 2.5886597991808764\n",
            "EPOCH 41:\n",
            "  batch 2 loss: 1.1234708726284652\n",
            "  batch 4 loss: 1.1252353211456225\n",
            "  batch 6 loss: 0.9041384435074351\n",
            "  batch 8 loss: 0.9461602147777624\n",
            "LOSS train 0.9461602147777624\n",
            "EPOCH 42:\n",
            "  batch 2 loss: 0.9217790695468921\n",
            "  batch 4 loss: 0.8681482395175104\n",
            "  batch 6 loss: 0.9138496498228836\n",
            "  batch 8 loss: 0.9725782666135296\n",
            "LOSS train 0.9725782666135296\n",
            "EPOCH 43:\n",
            "  batch 2 loss: 0.9625362140182968\n",
            "  batch 4 loss: 1.2660047728863484\n",
            "  batch 6 loss: 1.067331153319235\n",
            "  batch 8 loss: 0.9296769373766407\n",
            "LOSS train 0.9296769373766407\n",
            "EPOCH 44:\n",
            "  batch 2 loss: 0.8370719850002085\n",
            "  batch 4 loss: 1.1010716535208107\n",
            "  batch 6 loss: 0.9946901519583765\n",
            "  batch 8 loss: 1.254039218034154\n",
            "LOSS train 1.254039218034154\n",
            "EPOCH 45:\n",
            "  batch 2 loss: 1.0808198563243185\n",
            "  batch 4 loss: 0.977397784519412\n",
            "  batch 6 loss: 1.0239352964404225\n",
            "  batch 8 loss: 1.4552081165162238\n",
            "LOSS train 1.4552081165162238\n",
            "EPOCH 46:\n",
            "  batch 2 loss: 0.9305050369481562\n",
            "  batch 4 loss: 0.9264989438809745\n",
            "  batch 6 loss: 0.9610882750973293\n",
            "  batch 8 loss: 0.8422674359418826\n",
            "LOSS train 0.8422674359418826\n",
            "EPOCH 47:\n",
            "  batch 2 loss: 0.8116476058707323\n",
            "  batch 4 loss: 0.9654943305449091\n",
            "  batch 6 loss: 0.9903141065999177\n",
            "  batch 8 loss: 0.907978162029061\n",
            "LOSS train 0.907978162029061\n",
            "EPOCH 48:\n",
            "  batch 2 loss: 0.9866262589880221\n",
            "  batch 4 loss: 0.9916031241478622\n",
            "  batch 6 loss: 0.964223806129658\n",
            "  batch 8 loss: 1.0112394929734108\n",
            "LOSS train 1.0112394929734108\n",
            "EPOCH 49:\n",
            "  batch 2 loss: 0.8915309870990413\n",
            "  batch 4 loss: 0.9235717086928641\n",
            "  batch 6 loss: 1.1239097380551173\n",
            "  batch 8 loss: 1.3537307938840735\n",
            "LOSS train 1.3537307938840735\n",
            "EPOCH 50:\n",
            "  batch 2 loss: 1.0097841937527674\n",
            "  batch 4 loss: 0.8482616799956639\n",
            "  batch 6 loss: 0.9048175230841813\n",
            "  batch 8 loss: 0.9127392130868202\n",
            "LOSS train 0.9127392130868202\n",
            "EPOCH 51:\n",
            "  batch 2 loss: 0.9805771615667975\n",
            "  batch 4 loss: 0.8967583369023626\n",
            "  batch 6 loss: 0.9592260493966482\n",
            "  batch 8 loss: 0.7588112269384906\n",
            "LOSS train 0.7588112269384906\n",
            "EPOCH 52:\n",
            "  batch 2 loss: 0.8474462003398676\n",
            "  batch 4 loss: 0.8756881108175911\n",
            "  batch 6 loss: 0.9251316440275166\n",
            "  batch 8 loss: 0.8542811936161683\n",
            "LOSS train 0.8542811936161683\n",
            "EPOCH 53:\n",
            "  batch 2 loss: 0.9727933384775151\n",
            "  batch 4 loss: 0.9756077351746975\n",
            "  batch 6 loss: 0.9241496632317963\n",
            "  batch 8 loss: 1.0235782863759004\n",
            "LOSS train 1.0235782863759004\n",
            "EPOCH 54:\n",
            "  batch 2 loss: 0.9183653028489138\n",
            "  batch 4 loss: 0.9464843257322753\n",
            "  batch 6 loss: 0.9581096662449531\n",
            "  batch 8 loss: 0.9285303818937227\n",
            "LOSS train 0.9285303818937227\n",
            "EPOCH 55:\n",
            "  batch 2 loss: 0.753216469506188\n",
            "  batch 4 loss: 0.7166411435050868\n",
            "  batch 6 loss: 0.8983727861415574\n",
            "  batch 8 loss: 1.0319688597202863\n",
            "LOSS train 1.0319688597202863\n",
            "EPOCH 56:\n",
            "  batch 2 loss: 0.9449883266777468\n",
            "  batch 4 loss: 1.0447772447031871\n",
            "  batch 6 loss: 0.964046728794345\n",
            "  batch 8 loss: 0.9936984314701203\n",
            "LOSS train 0.9936984314701203\n",
            "EPOCH 57:\n",
            "  batch 2 loss: 0.8889599311626786\n",
            "  batch 4 loss: 1.4929191855599973\n",
            "  batch 6 loss: 1.023692953799228\n",
            "  batch 8 loss: 1.0450047578743953\n",
            "LOSS train 1.0450047578743953\n",
            "EPOCH 58:\n",
            "  batch 2 loss: 1.04181016756116\n",
            "  batch 4 loss: 0.9675774159403597\n",
            "  batch 6 loss: 1.0455286799707653\n",
            "  batch 8 loss: 0.57455120023045\n",
            "LOSS train 0.57455120023045\n",
            "EPOCH 59:\n",
            "  batch 2 loss: 0.7752503345867952\n",
            "  batch 4 loss: 0.7694545376731874\n",
            "  batch 6 loss: 0.922505921510159\n",
            "  batch 8 loss: 1.0543731372689766\n",
            "LOSS train 1.0543731372689766\n",
            "EPOCH 60:\n",
            "  batch 2 loss: 1.0432458741887856\n",
            "  batch 4 loss: 0.8945885872099208\n",
            "  batch 6 loss: 0.6863709005891734\n",
            "  batch 8 loss: 1.1554658272240101\n",
            "LOSS train 1.1554658272240101\n",
            "EPOCH 61:\n",
            "  batch 2 loss: 1.1420992031090504\n",
            "  batch 4 loss: 2.208568852485249\n",
            "  batch 6 loss: 0.981499576868264\n",
            "  batch 8 loss: 0.8006156255760528\n",
            "LOSS train 0.8006156255760528\n",
            "EPOCH 62:\n",
            "  batch 2 loss: 0.8175986999054474\n",
            "  batch 4 loss: 0.5606435402223066\n",
            "  batch 6 loss: 0.7444208577735056\n",
            "  batch 8 loss: 0.9921047492592184\n",
            "LOSS train 0.9921047492592184\n",
            "EPOCH 63:\n",
            "  batch 2 loss: 0.7838924008162951\n",
            "  batch 4 loss: 1.0249121893009376\n",
            "  batch 6 loss: 1.7969734181393258\n",
            "  batch 8 loss: 1.014963338602445\n",
            "LOSS train 1.014963338602445\n",
            "EPOCH 64:\n",
            "  batch 2 loss: 0.9856303268111025\n",
            "  batch 4 loss: 1.0371103800686172\n",
            "  batch 6 loss: 1.016856773458989\n",
            "  batch 8 loss: 0.8963173636843758\n",
            "LOSS train 0.8963173636843758\n",
            "EPOCH 65:\n",
            "  batch 2 loss: 0.9267395092136081\n",
            "  batch 4 loss: 1.0677805038859294\n",
            "  batch 6 loss: 1.057255582311957\n",
            "  batch 8 loss: 0.9323770548659851\n",
            "LOSS train 0.9323770548659851\n",
            "EPOCH 66:\n",
            "  batch 2 loss: 0.9595727724243229\n",
            "  batch 4 loss: 0.8239331085948045\n",
            "  batch 6 loss: 1.086278060504502\n",
            "  batch 8 loss: 0.775903912278302\n",
            "LOSS train 0.775903912278302\n",
            "EPOCH 67:\n",
            "  batch 2 loss: 0.8952047290982252\n",
            "  batch 4 loss: 0.9963408579388902\n",
            "  batch 6 loss: 0.8080729600814364\n",
            "  batch 8 loss: 0.9912581492039629\n",
            "LOSS train 0.9912581492039629\n",
            "EPOCH 68:\n",
            "  batch 2 loss: 1.0127617054900337\n",
            "  batch 4 loss: 0.8177638240109445\n",
            "  batch 6 loss: 1.0108710456470988\n",
            "  batch 8 loss: 0.8796206654367679\n",
            "LOSS train 0.8796206654367679\n",
            "EPOCH 69:\n",
            "  batch 2 loss: 0.8146657170672491\n",
            "  batch 4 loss: 0.9768944984029887\n",
            "  batch 6 loss: 1.0426475455677522\n",
            "  batch 8 loss: 1.013134477209985\n",
            "LOSS train 1.013134477209985\n",
            "EPOCH 70:\n",
            "  batch 2 loss: 0.7790565665526223\n",
            "  batch 4 loss: 1.0319536497012005\n",
            "  batch 6 loss: 0.7728460103356434\n",
            "  batch 8 loss: 0.9889840229993409\n",
            "LOSS train 0.9889840229993409\n",
            "EPOCH 71:\n",
            "  batch 2 loss: 0.7775715148630988\n",
            "  batch 4 loss: 1.0232532418604654\n",
            "  batch 6 loss: 1.0118059493860725\n",
            "  batch 8 loss: 8.138376750935361\n",
            "LOSS train 8.138376750935361\n",
            "EPOCH 72:\n",
            "  batch 2 loss: 0.8746207393709015\n",
            "  batch 4 loss: 1.1347785751541855\n",
            "  batch 6 loss: 1.4631903370661403\n",
            "  batch 8 loss: 1.6572674220894703\n",
            "LOSS train 1.6572674220894703\n",
            "EPOCH 73:\n",
            "  batch 2 loss: 1.0818776218048585\n",
            "  batch 4 loss: 2.7573898765070743\n",
            "  batch 6 loss: 0.9763055960727849\n",
            "  batch 8 loss: 1.286407651996226\n",
            "LOSS train 1.286407651996226\n",
            "EPOCH 74:\n",
            "  batch 2 loss: 1.3503768496714263\n",
            "  batch 4 loss: 0.30648263808120413\n",
            "  batch 6 loss: 0.35711136983111014\n",
            "  batch 8 loss: 1.2227089467166103\n",
            "LOSS train 1.2227089467166103\n",
            "EPOCH 75:\n",
            "  batch 2 loss: 1.6588539970786051\n",
            "  batch 4 loss: 1.355551291476327\n",
            "  batch 6 loss: 0.6664624339232179\n",
            "  batch 8 loss: 1.031269331705819\n",
            "LOSS train 1.031269331705819\n",
            "EPOCH 76:\n",
            "  batch 2 loss: 1.085614559721854\n",
            "  batch 4 loss: 0.7968382128029434\n",
            "  batch 6 loss: 73.3547218060903\n",
            "  batch 8 loss: 1.4768585231493383\n",
            "LOSS train 1.4768585231493383\n",
            "EPOCH 77:\n",
            "  batch 2 loss: 6.603683549862815\n",
            "  batch 4 loss: 583.793799235299\n",
            "  batch 6 loss: 1.530551129601904\n",
            "  batch 8 loss: 0.9908232357344909\n",
            "LOSS train 0.9908232357344909\n",
            "EPOCH 78:\n",
            "  batch 2 loss: 1.1152506331597762\n",
            "  batch 4 loss: 1.0267102834844386\n",
            "  batch 6 loss: 5.734319849776802\n",
            "  batch 8 loss: 1.0167500896099972\n",
            "LOSS train 1.0167500896099972\n",
            "EPOCH 79:\n",
            "  batch 2 loss: 4.515350908958478\n",
            "  batch 4 loss: 5.5420751669419515\n",
            "  batch 6 loss: 2.0630902796506314\n",
            "  batch 8 loss: 1.66753684038969\n",
            "LOSS train 1.66753684038969\n",
            "EPOCH 80:\n",
            "  batch 2 loss: 7.533804002726916\n",
            "  batch 4 loss: 2.4269685243294186\n",
            "  batch 6 loss: 7.606390605299154\n",
            "  batch 8 loss: 1.6759483335340355\n",
            "LOSS train 1.6759483335340355\n",
            "EPOCH 81:\n",
            "  batch 2 loss: 1.0753204102131968\n",
            "  batch 4 loss: 1.6803586283155076\n",
            "  batch 6 loss: 1.3142144702282132\n",
            "  batch 8 loss: 2.450864945919786\n",
            "LOSS train 2.450864945919786\n",
            "EPOCH 82:\n",
            "  batch 2 loss: 1.4840488607237172\n",
            "  batch 4 loss: 0.93236235015974\n",
            "  batch 6 loss: 2.916617326057922\n",
            "  batch 8 loss: 1.051343962308115\n",
            "LOSS train 1.051343962308115\n",
            "EPOCH 83:\n",
            "  batch 2 loss: 1.331004784302709\n",
            "  batch 4 loss: 0.8039275108760707\n",
            "  batch 6 loss: 1.2493862426721496\n",
            "  batch 8 loss: 0.6970807684913992\n",
            "LOSS train 0.6970807684913992\n",
            "EPOCH 84:\n",
            "  batch 2 loss: 1.0323947095408197\n",
            "  batch 4 loss: 1.0055256226077698\n",
            "  batch 6 loss: 1.1115339195429064\n",
            "  batch 8 loss: 0.8471011062909111\n",
            "LOSS train 0.8471011062909111\n",
            "EPOCH 85:\n",
            "  batch 2 loss: 0.897488850216801\n",
            "  batch 4 loss: 0.8207756045055883\n",
            "  batch 6 loss: 0.7443201827381083\n",
            "  batch 8 loss: 0.9005681536742813\n",
            "LOSS train 0.9005681536742813\n",
            "EPOCH 86:\n",
            "  batch 2 loss: 0.9174097817494227\n",
            "  batch 4 loss: 1.1464869508015862\n",
            "  batch 6 loss: 0.7539848842260022\n",
            "  batch 8 loss: 0.9240803116238516\n",
            "LOSS train 0.9240803116238516\n",
            "EPOCH 87:\n",
            "  batch 2 loss: 0.9029533789462775\n",
            "  batch 4 loss: 1.2376195296521857\n",
            "  batch 6 loss: 0.8479794086317538\n",
            "  batch 8 loss: 1.3835027687819228\n",
            "LOSS train 1.3835027687819228\n",
            "EPOCH 88:\n",
            "  batch 2 loss: 0.6472667757644714\n",
            "  batch 4 loss: 1.37218094462968\n",
            "  batch 6 loss: 1.0524867273598217\n",
            "  batch 8 loss: 0.511324291923092\n",
            "LOSS train 0.511324291923092\n",
            "EPOCH 89:\n",
            "  batch 2 loss: 0.5706462702153586\n",
            "  batch 4 loss: 0.7170074468970165\n",
            "  batch 6 loss: 1.024855531888861\n",
            "  batch 8 loss: 1.094795268910353\n",
            "LOSS train 1.094795268910353\n",
            "EPOCH 90:\n",
            "  batch 2 loss: 1.354124704297059\n",
            "  batch 4 loss: 2.0190953311312936\n",
            "  batch 6 loss: 0.47314119340548066\n",
            "  batch 8 loss: 1.1619021763406474\n",
            "LOSS train 1.1619021763406474\n",
            "EPOCH 91:\n",
            "  batch 2 loss: 1.7518046330401078\n",
            "  batch 4 loss: 0.6262273265245004\n",
            "  batch 6 loss: 2.406084411151321\n",
            "  batch 8 loss: 1.0036511073180998\n",
            "LOSS train 1.0036511073180998\n",
            "EPOCH 92:\n",
            "  batch 2 loss: 1.0907788161568306\n",
            "  batch 4 loss: 1.501998921320236\n",
            "  batch 6 loss: 0.5270577382219868\n",
            "  batch 8 loss: 0.36008649383485036\n",
            "LOSS train 0.36008649383485036\n",
            "EPOCH 93:\n",
            "  batch 2 loss: 0.7805620325559584\n",
            "  batch 4 loss: 0.7229195533148196\n",
            "  batch 6 loss: 0.5210009029631125\n",
            "  batch 8 loss: 1.110336717986315\n",
            "LOSS train 1.110336717986315\n",
            "EPOCH 94:\n",
            "  batch 2 loss: 0.7488017449357033\n",
            "  batch 4 loss: 0.9267911381044008\n",
            "  batch 6 loss: 0.7937991873335306\n",
            "  batch 8 loss: 1.1277648110022387\n",
            "LOSS train 1.1277648110022387\n",
            "EPOCH 95:\n",
            "  batch 2 loss: 1.0326314945790667\n",
            "  batch 4 loss: 1.3365989402613714\n",
            "  batch 6 loss: 1.1037694719356321\n",
            "  batch 8 loss: 0.670572540967626\n",
            "LOSS train 0.670572540967626\n",
            "EPOCH 96:\n",
            "  batch 2 loss: 1.5952486319513421\n",
            "  batch 4 loss: 0.7257903743945839\n",
            "  batch 6 loss: 0.8564354364734554\n",
            "  batch 8 loss: 0.8377826971752653\n",
            "LOSS train 0.8377826971752653\n",
            "EPOCH 97:\n",
            "  batch 2 loss: 1.0281490252611465\n",
            "  batch 4 loss: 0.7602666232862295\n",
            "  batch 6 loss: 0.6702925705117098\n",
            "  batch 8 loss: 0.4398733911575915\n",
            "LOSS train 0.4398733911575915\n",
            "EPOCH 98:\n",
            "  batch 2 loss: 0.899405600167027\n",
            "  batch 4 loss: 0.7835096235409531\n",
            "  batch 6 loss: 0.9912099798531796\n",
            "  batch 8 loss: 1.046286393800564\n",
            "LOSS train 1.046286393800564\n",
            "EPOCH 99:\n",
            "  batch 2 loss: 0.1278226552208563\n",
            "  batch 4 loss: 0.7533155942255687\n",
            "  batch 6 loss: 4.7597588421607435\n",
            "  batch 8 loss: 1.1392729573704303\n",
            "LOSS train 1.1392729573704303\n",
            "EPOCH 100:\n",
            "  batch 2 loss: 1.0668043553706927\n",
            "  batch 4 loss: 0.9275956513213016\n",
            "  batch 6 loss: 1.306698552271062\n",
            "  batch 8 loss: 0.7818873200143278\n",
            "LOSS train 0.7818873200143278\n",
            "EPOCH 101:\n",
            "  batch 2 loss: 0.7551824278080599\n",
            "  batch 4 loss: 0.7046779654409132\n",
            "  batch 6 loss: 0.9330051321635529\n",
            "  batch 8 loss: 0.3727553250120751\n",
            "LOSS train 0.3727553250120751\n",
            "EPOCH 102:\n",
            "  batch 2 loss: 1.2645828340528809\n",
            "  batch 4 loss: 0.9866295161487866\n",
            "  batch 6 loss: 1.515693401452511\n",
            "  batch 8 loss: 1.1306060649361684\n",
            "LOSS train 1.1306060649361684\n",
            "EPOCH 103:\n",
            "  batch 2 loss: 13.542692864967234\n",
            "  batch 4 loss: 1.0195548326784072\n",
            "  batch 6 loss: 1.302441616339151\n",
            "  batch 8 loss: 3.9603171811482794\n",
            "LOSS train 3.9603171811482794\n",
            "EPOCH 104:\n",
            "  batch 2 loss: 0.7913638548335127\n",
            "  batch 4 loss: 1.3719501440770085\n",
            "  batch 6 loss: 1.2189162176902553\n",
            "  batch 8 loss: 28.563723780045883\n",
            "LOSS train 28.563723780045883\n",
            "EPOCH 105:\n",
            "  batch 2 loss: 1.3476334802747074\n",
            "  batch 4 loss: 1.2341414061268576\n",
            "  batch 6 loss: 0.8805494992517902\n",
            "  batch 8 loss: 0.9951302337504688\n",
            "LOSS train 0.9951302337504688\n",
            "EPOCH 106:\n",
            "  batch 2 loss: 0.9850808392505794\n",
            "  batch 4 loss: 0.9474350689163341\n",
            "  batch 6 loss: 1.0314401072665267\n",
            "  batch 8 loss: 1.143582955377873\n",
            "LOSS train 1.143582955377873\n",
            "EPOCH 107:\n",
            "  batch 2 loss: 1.267379638918999\n",
            "  batch 4 loss: 0.9814474687829446\n",
            "  batch 6 loss: 0.7280805165371902\n",
            "  batch 8 loss: 0.6720537881317592\n",
            "LOSS train 0.6720537881317592\n",
            "EPOCH 108:\n",
            "  batch 2 loss: 0.8245079390341172\n",
            "  batch 4 loss: 1.1744896310787278\n",
            "  batch 6 loss: 0.791164476088321\n",
            "  batch 8 loss: 1.192485071276469\n",
            "LOSS train 1.192485071276469\n",
            "EPOCH 109:\n",
            "  batch 2 loss: 1.0745271291457978\n",
            "  batch 4 loss: 0.7667620522681741\n",
            "  batch 6 loss: 1.052570111547082\n",
            "  batch 8 loss: 0.9320450058915473\n",
            "LOSS train 0.9320450058915473\n",
            "EPOCH 110:\n",
            "  batch 2 loss: 0.6689332049304736\n",
            "  batch 4 loss: 0.9195829499179333\n",
            "  batch 6 loss: 0.9494288035651343\n",
            "  batch 8 loss: 0.9941436498680092\n",
            "LOSS train 0.9941436498680092\n",
            "EPOCH 111:\n",
            "  batch 2 loss: 0.9044035980166165\n",
            "  batch 4 loss: 0.631387584465793\n",
            "  batch 6 loss: 1.0020625585791882\n",
            "  batch 8 loss: 0.7192790161003448\n",
            "LOSS train 0.7192790161003448\n",
            "EPOCH 112:\n",
            "  batch 2 loss: 0.5694012855577688\n",
            "  batch 4 loss: 0.7335312547309597\n",
            "  batch 6 loss: 1.1235779112938746\n",
            "  batch 8 loss: 1.0584928548284052\n",
            "LOSS train 1.0584928548284052\n",
            "EPOCH 113:\n",
            "  batch 2 loss: 0.9516398231157357\n",
            "  batch 4 loss: 0.7922228315139799\n",
            "  batch 6 loss: 1.0062036296589916\n",
            "  batch 8 loss: 1.0073090379689833\n",
            "LOSS train 1.0073090379689833\n",
            "EPOCH 114:\n",
            "  batch 2 loss: 0.6125793803215142\n",
            "  batch 4 loss: 0.8256071637270719\n",
            "  batch 6 loss: 0.8661751957180097\n",
            "  batch 8 loss: 1.105093382369576\n",
            "LOSS train 1.105093382369576\n",
            "EPOCH 115:\n",
            "  batch 2 loss: 0.9887462883373389\n",
            "  batch 4 loss: 1.132363247160575\n",
            "  batch 6 loss: 0.878363726461272\n",
            "  batch 8 loss: 0.9757363004866834\n",
            "LOSS train 0.9757363004866834\n",
            "EPOCH 116:\n",
            "  batch 2 loss: 1.3253392669166502\n",
            "  batch 4 loss: 1.2662179877292887\n",
            "  batch 6 loss: 0.9360137108576194\n",
            "  batch 8 loss: 1.006686091043561\n",
            "LOSS train 1.006686091043561\n",
            "EPOCH 117:\n",
            "  batch 2 loss: 0.952349313712152\n",
            "  batch 4 loss: 0.9571445857392801\n",
            "  batch 6 loss: 1.0308409066644155\n",
            "  batch 8 loss: 0.8411262626377377\n",
            "LOSS train 0.8411262626377377\n",
            "EPOCH 118:\n",
            "  batch 2 loss: 1.031938268409038\n",
            "  batch 4 loss: 0.9370059027185681\n",
            "  batch 6 loss: 1.3084059706768354\n",
            "  batch 8 loss: 0.9141161325045624\n",
            "LOSS train 0.9141161325045624\n",
            "EPOCH 119:\n",
            "  batch 2 loss: 1.0461459394323138\n",
            "  batch 4 loss: 0.9215736062255392\n",
            "  batch 6 loss: 1.0273327414477766\n",
            "  batch 8 loss: 0.8959859872672803\n",
            "LOSS train 0.8959859872672803\n",
            "EPOCH 120:\n",
            "  batch 2 loss: 1.0353081632519845\n",
            "  batch 4 loss: 1.146557482815492\n",
            "  batch 6 loss: 0.7755498361942892\n",
            "  batch 8 loss: 1.0024872381361727\n",
            "LOSS train 1.0024872381361727\n",
            "EPOCH 121:\n",
            "  batch 2 loss: 0.8880922265280555\n",
            "  batch 4 loss: 1.0195342458112036\n",
            "  batch 6 loss: 0.9966841182962092\n",
            "  batch 8 loss: 0.8771451285170586\n",
            "LOSS train 0.8771451285170586\n",
            "EPOCH 122:\n",
            "  batch 2 loss: 0.9915444676107933\n",
            "  batch 4 loss: 1.0335495128797378\n",
            "  batch 6 loss: 0.9395459329711369\n",
            "  batch 8 loss: 1.0602659053723635\n",
            "LOSS train 1.0602659053723635\n",
            "EPOCH 123:\n",
            "  batch 2 loss: 1.0489782230145253\n",
            "  batch 4 loss: 10.24822574942951\n",
            "  batch 6 loss: 1.092759356728703\n",
            "  batch 8 loss: 1.026562001557258\n",
            "LOSS train 1.026562001557258\n",
            "EPOCH 124:\n",
            "  batch 2 loss: 1.2883112959776384\n",
            "  batch 4 loss: 1.1481494857932832\n",
            "  batch 6 loss: 1.1678962633796561\n",
            "  batch 8 loss: 1.5011104877987014\n",
            "LOSS train 1.5011104877987014\n",
            "EPOCH 125:\n",
            "  batch 2 loss: 1.4536580391106448\n",
            "  batch 4 loss: 0.8535698369181972\n",
            "  batch 6 loss: 1.975138155194351\n",
            "  batch 8 loss: 2.055803936231275\n",
            "LOSS train 2.055803936231275\n",
            "EPOCH 126:\n",
            "  batch 2 loss: 0.7626183716493365\n",
            "  batch 4 loss: 1.0665368549693703\n",
            "  batch 6 loss: 1.6852910564999355\n",
            "  batch 8 loss: 1.2667035838727592\n",
            "LOSS train 1.2667035838727592\n",
            "EPOCH 127:\n",
            "  batch 2 loss: 0.914234695867591\n",
            "  batch 4 loss: 0.9504670540880068\n",
            "  batch 6 loss: 1.798058125892265\n",
            "  batch 8 loss: 1.1207046553534687\n",
            "LOSS train 1.1207046553534687\n",
            "EPOCH 128:\n",
            "  batch 2 loss: 1.2294359955203116\n",
            "  batch 4 loss: 1.229181123653642\n",
            "  batch 6 loss: 1.104774362117082\n",
            "  batch 8 loss: 1.0757433979156024\n",
            "LOSS train 1.0757433979156024\n",
            "EPOCH 129:\n",
            "  batch 2 loss: 1.0852585605328304\n",
            "  batch 4 loss: 0.9865943727758075\n",
            "  batch 6 loss: 1.0822020717071636\n",
            "  batch 8 loss: 0.944446070775526\n",
            "LOSS train 0.944446070775526\n",
            "EPOCH 130:\n",
            "  batch 2 loss: 1.0065403734404834\n",
            "  batch 4 loss: 0.9747589335008446\n",
            "  batch 6 loss: 0.9663590133047777\n",
            "  batch 8 loss: 1.0121017195477564\n",
            "LOSS train 1.0121017195477564\n",
            "EPOCH 131:\n",
            "  batch 2 loss: 0.9490729602846093\n",
            "  batch 4 loss: 0.9914594462277078\n",
            "  batch 6 loss: 1.0163991601416695\n",
            "  batch 8 loss: 0.9452164174770084\n",
            "LOSS train 0.9452164174770084\n",
            "EPOCH 132:\n",
            "  batch 2 loss: 1.008992197171607\n",
            "  batch 4 loss: 1.0052732364789947\n",
            "  batch 6 loss: 0.9424616554274872\n",
            "  batch 8 loss: 1.0192156156483838\n",
            "LOSS train 1.0192156156483838\n",
            "EPOCH 133:\n",
            "  batch 2 loss: 0.936213207838074\n",
            "  batch 4 loss: 0.9942693862176504\n",
            "  batch 6 loss: 0.9918002881772126\n",
            "  batch 8 loss: 0.9959359894112737\n",
            "LOSS train 0.9959359894112737\n",
            "EPOCH 134:\n",
            "  batch 2 loss: 0.9886632469696235\n",
            "  batch 4 loss: 1.0005569936199092\n",
            "  batch 6 loss: 1.0078260267445383\n",
            "  batch 8 loss: 0.9954502418669042\n",
            "LOSS train 0.9954502418669042\n",
            "EPOCH 135:\n",
            "  batch 2 loss: 1.0001062769134776\n",
            "  batch 4 loss: 1.0125624310234929\n",
            "  batch 6 loss: 0.9915671023423391\n",
            "  batch 8 loss: 0.9537577894693917\n",
            "LOSS train 0.9537577894693917\n",
            "EPOCH 136:\n",
            "  batch 2 loss: 0.9997230768739508\n",
            "  batch 4 loss: 1.0002390812057702\n",
            "  batch 6 loss: 0.9222933680471368\n",
            "  batch 8 loss: 1.0147716862267215\n",
            "LOSS train 1.0147716862267215\n",
            "EPOCH 137:\n",
            "  batch 2 loss: 0.9979243977675086\n",
            "  batch 4 loss: 0.994114770672185\n",
            "  batch 6 loss: 1.0023695638991617\n",
            "  batch 8 loss: 0.9663850146239952\n",
            "LOSS train 0.9663850146239952\n",
            "EPOCH 138:\n",
            "  batch 2 loss: 0.9728829503918391\n",
            "  batch 4 loss: 0.9902124558598674\n",
            "  batch 6 loss: 0.9901961029420829\n",
            "  batch 8 loss: 0.9899985865129317\n",
            "LOSS train 0.9899985865129317\n",
            "EPOCH 139:\n",
            "  batch 2 loss: 1.0069461284061418\n",
            "  batch 4 loss: 1.015226803222151\n",
            "  batch 6 loss: 0.9938327867093854\n",
            "  batch 8 loss: 1.00187711505868\n",
            "LOSS train 1.00187711505868\n",
            "EPOCH 140:\n",
            "  batch 2 loss: 1.0218905000364162\n",
            "  batch 4 loss: 0.9528693922870263\n",
            "  batch 6 loss: 0.9871586187432367\n",
            "  batch 8 loss: 1.0064702906565293\n",
            "LOSS train 1.0064702906565293\n",
            "EPOCH 141:\n",
            "  batch 2 loss: 0.9791831530211594\n",
            "  batch 4 loss: 1.0379600863472498\n",
            "  batch 6 loss: 0.976920167182328\n",
            "  batch 8 loss: 0.9852119559974364\n",
            "LOSS train 0.9852119559974364\n",
            "EPOCH 142:\n",
            "  batch 2 loss: 0.9991229641108615\n",
            "  batch 4 loss: 0.9438947521464472\n",
            "  batch 6 loss: 0.9869382328610063\n",
            "  batch 8 loss: 1.005179779822219\n",
            "LOSS train 1.005179779822219\n",
            "EPOCH 143:\n",
            "  batch 2 loss: 0.9999455743279686\n",
            "  batch 4 loss: 1.0099948011470636\n",
            "  batch 6 loss: 0.9335174325162585\n",
            "  batch 8 loss: 0.9960420098306634\n",
            "LOSS train 0.9960420098306634\n",
            "EPOCH 144:\n",
            "  batch 2 loss: 0.985689305794932\n",
            "  batch 4 loss: 0.9614118384375048\n",
            "  batch 6 loss: 0.9922566145164664\n",
            "  batch 8 loss: 0.9995275851321346\n",
            "LOSS train 0.9995275851321346\n",
            "EPOCH 145:\n",
            "  batch 2 loss: 0.9737665795245052\n",
            "  batch 4 loss: 1.0493120982476087\n",
            "  batch 6 loss: 0.9035398009458069\n",
            "  batch 8 loss: 0.9762499638010121\n",
            "LOSS train 0.9762499638010121\n",
            "EPOCH 146:\n",
            "  batch 2 loss: 0.9869931001552157\n",
            "  batch 4 loss: 1.0063342434806208\n",
            "  batch 6 loss: 0.9951522891800904\n",
            "  batch 8 loss: 1.0049981690067473\n",
            "LOSS train 1.0049981690067473\n",
            "EPOCH 147:\n",
            "  batch 2 loss: 1.0018902220942558\n",
            "  batch 4 loss: 1.0103966825315283\n",
            "  batch 6 loss: 0.9823069104548356\n",
            "  batch 8 loss: 0.9931785252699924\n",
            "LOSS train 0.9931785252699924\n",
            "EPOCH 148:\n",
            "  batch 2 loss: 0.9299610648898758\n",
            "  batch 4 loss: 0.9726902317201738\n",
            "  batch 6 loss: 1.0302719359455792\n",
            "  batch 8 loss: 0.9838757952208484\n",
            "LOSS train 0.9838757952208484\n",
            "EPOCH 149:\n",
            "  batch 2 loss: 0.9934330945224498\n",
            "  batch 4 loss: 0.98503548225515\n",
            "  batch 6 loss: 1.011879289532789\n",
            "  batch 8 loss: 0.9553937807861348\n",
            "LOSS train 0.9553937807861348\n",
            "EPOCH 150:\n",
            "  batch 2 loss: 0.9367277036863101\n",
            "  batch 4 loss: 0.9967458613720679\n",
            "  batch 6 loss: 0.9539926272174929\n",
            "  batch 8 loss: 0.9966598334173776\n",
            "LOSS train 0.9966598334173776\n",
            "EPOCH 151:\n",
            "  batch 2 loss: 0.9148272580920613\n",
            "  batch 4 loss: 1.0137799589414151\n",
            "  batch 6 loss: 1.0056823764225122\n",
            "  batch 8 loss: 0.991563351890006\n",
            "LOSS train 0.991563351890006\n",
            "EPOCH 152:\n",
            "  batch 2 loss: 0.9543633010076238\n",
            "  batch 4 loss: 0.9125991704049721\n",
            "  batch 6 loss: 0.9306752542331407\n",
            "  batch 8 loss: 0.9905295461528822\n",
            "LOSS train 0.9905295461528822\n",
            "EPOCH 153:\n",
            "  batch 2 loss: 0.9866931833108978\n",
            "  batch 4 loss: 1.0026074422399178\n",
            "  batch 6 loss: 0.9832887118917831\n",
            "  batch 8 loss: 0.963715128925267\n",
            "LOSS train 0.963715128925267\n",
            "EPOCH 154:\n",
            "  batch 2 loss: 1.1665770199214744\n",
            "  batch 4 loss: 0.9983884865343648\n",
            "  batch 6 loss: 0.9302179705999626\n",
            "  batch 8 loss: 0.9475071539196269\n",
            "LOSS train 0.9475071539196269\n",
            "EPOCH 155:\n",
            "  batch 2 loss: 1.0331502737277631\n",
            "  batch 4 loss: 0.9948316033942308\n",
            "  batch 6 loss: 1.008563553754422\n",
            "  batch 8 loss: 1.0156993854150858\n",
            "LOSS train 1.0156993854150858\n",
            "EPOCH 156:\n",
            "  batch 2 loss: 1.0106745725706672\n",
            "  batch 4 loss: 0.9729150511841101\n",
            "  batch 6 loss: 0.9942606778267729\n",
            "  batch 8 loss: 0.9834995394854935\n",
            "LOSS train 0.9834995394854935\n",
            "EPOCH 157:\n",
            "  batch 2 loss: 1.010625437400202\n",
            "  batch 4 loss: 1.0228889827818408\n",
            "  batch 6 loss: 0.977207740941518\n",
            "  batch 8 loss: 0.998516718723471\n",
            "LOSS train 0.998516718723471\n",
            "EPOCH 158:\n",
            "  batch 2 loss: 0.9514420864014743\n",
            "  batch 4 loss: 0.9662281926758679\n",
            "  batch 6 loss: 0.8857878323906605\n",
            "  batch 8 loss: 1.0058622168391134\n",
            "LOSS train 1.0058622168391134\n",
            "EPOCH 159:\n",
            "  batch 2 loss: 1.024262293645719\n",
            "  batch 4 loss: 0.9988164361797301\n",
            "  batch 6 loss: 0.967135037998238\n",
            "  batch 8 loss: 0.9902722348652044\n",
            "LOSS train 0.9902722348652044\n",
            "EPOCH 160:\n",
            "  batch 2 loss: 1.0559524585596713\n",
            "  batch 4 loss: 0.9965795414686962\n",
            "  batch 6 loss: 1.0016944285948788\n",
            "  batch 8 loss: 0.9588309285146673\n",
            "LOSS train 0.9588309285146673\n",
            "EPOCH 161:\n",
            "  batch 2 loss: 0.9696957480651588\n",
            "  batch 4 loss: 0.9786054611283745\n",
            "  batch 6 loss: 0.9625231842703867\n",
            "  batch 8 loss: 0.9318490898650156\n",
            "LOSS train 0.9318490898650156\n",
            "EPOCH 162:\n",
            "  batch 2 loss: 0.956793457459235\n",
            "  batch 4 loss: 0.9816388424784457\n",
            "  batch 6 loss: 0.928815508724074\n",
            "  batch 8 loss: 0.9992493715393533\n",
            "LOSS train 0.9992493715393533\n",
            "EPOCH 163:\n",
            "  batch 2 loss: 0.9877246714415504\n",
            "  batch 4 loss: 1.0016240234642517\n",
            "  batch 6 loss: 1.0072527306248789\n",
            "  batch 8 loss: 0.9774725291440114\n",
            "LOSS train 0.9774725291440114\n",
            "EPOCH 164:\n",
            "  batch 2 loss: 0.9985967202617386\n",
            "  batch 4 loss: 0.9894875287956997\n",
            "  batch 6 loss: 1.0227860168839147\n",
            "  batch 8 loss: 0.9241274742142563\n",
            "LOSS train 0.9241274742142563\n",
            "EPOCH 165:\n",
            "  batch 2 loss: 0.9468085651750313\n",
            "  batch 4 loss: 0.8506717363590174\n",
            "  batch 6 loss: 0.9774536856074942\n",
            "  batch 8 loss: 1.0100830970229846\n",
            "LOSS train 1.0100830970229846\n",
            "EPOCH 166:\n",
            "  batch 2 loss: 1.0054917039903786\n",
            "  batch 4 loss: 0.9388940836646975\n",
            "  batch 6 loss: 0.9439165792970766\n",
            "  batch 8 loss: 1.0269004617450297\n",
            "LOSS train 1.0269004617450297\n",
            "EPOCH 167:\n",
            "  batch 2 loss: 0.9557276064685094\n",
            "  batch 4 loss: 0.9480481116326125\n",
            "  batch 6 loss: 0.9730673748343479\n",
            "  batch 8 loss: 0.994116854898912\n",
            "LOSS train 0.994116854898912\n",
            "EPOCH 168:\n",
            "  batch 2 loss: 1.0480962435721874\n",
            "  batch 4 loss: 0.9958187306793508\n",
            "  batch 6 loss: 0.9962918675902015\n",
            "  batch 8 loss: 1.0026260140068306\n",
            "LOSS train 1.0026260140068306\n",
            "EPOCH 169:\n",
            "  batch 2 loss: 1.0070086253089268\n",
            "  batch 4 loss: 0.9432269056992013\n",
            "  batch 6 loss: 0.9513770707038097\n",
            "  batch 8 loss: 1.0350701318864097\n",
            "LOSS train 1.0350701318864097\n",
            "EPOCH 170:\n",
            "  batch 2 loss: 0.9832135635846313\n",
            "  batch 4 loss: 1.0086733007050466\n",
            "  batch 6 loss: 0.9460705515701834\n",
            "  batch 8 loss: 0.9572748504150332\n",
            "LOSS train 0.9572748504150332\n",
            "EPOCH 171:\n",
            "  batch 2 loss: 0.9046835423412648\n",
            "  batch 4 loss: 0.9998715740751729\n",
            "  batch 6 loss: 1.0020185543050188\n",
            "  batch 8 loss: 0.9877097996434407\n",
            "LOSS train 0.9877097996434407\n",
            "EPOCH 172:\n",
            "  batch 2 loss: 1.0235606978684735\n",
            "  batch 4 loss: 0.9994259591553458\n",
            "  batch 6 loss: 0.9432359252633462\n",
            "  batch 8 loss: 1.0007203632253368\n",
            "LOSS train 1.0007203632253368\n",
            "EPOCH 173:\n",
            "  batch 2 loss: 0.9369371400328514\n",
            "  batch 4 loss: 0.9973109579435664\n",
            "  batch 6 loss: 0.9292292397092046\n",
            "  batch 8 loss: 1.016963458105077\n",
            "LOSS train 1.016963458105077\n",
            "EPOCH 174:\n",
            "  batch 2 loss: 0.9312673572726784\n",
            "  batch 4 loss: 0.9823729144939746\n",
            "  batch 6 loss: 1.010859574860986\n",
            "  batch 8 loss: 0.988978516628397\n",
            "LOSS train 0.988978516628397\n",
            "EPOCH 175:\n",
            "  batch 2 loss: 1.0534697019479469\n",
            "  batch 4 loss: 0.9958474840536398\n",
            "  batch 6 loss: 0.962412137727654\n",
            "  batch 8 loss: 0.9762675550740336\n",
            "LOSS train 0.9762675550740336\n",
            "EPOCH 176:\n",
            "  batch 2 loss: 0.9755303735289449\n",
            "  batch 4 loss: 0.9106339476027495\n",
            "  batch 6 loss: 1.0296138906097032\n",
            "  batch 8 loss: 0.9055878359152607\n",
            "LOSS train 0.9055878359152607\n",
            "EPOCH 177:\n",
            "  batch 2 loss: 0.999400195952842\n",
            "  batch 4 loss: 0.9906255136781347\n",
            "  batch 6 loss: 1.0121718819983463\n",
            "  batch 8 loss: 1.0376759310679344\n",
            "LOSS train 1.0376759310679344\n",
            "EPOCH 178:\n",
            "  batch 2 loss: 1.0035689933031304\n",
            "  batch 4 loss: 0.8998039819151527\n",
            "  batch 6 loss: 0.9666631603294371\n",
            "  batch 8 loss: 0.9580309786923495\n",
            "LOSS train 0.9580309786923495\n",
            "EPOCH 179:\n",
            "  batch 2 loss: 1.0220178236926463\n",
            "  batch 4 loss: 0.9832763606895577\n",
            "  batch 6 loss: 0.9953788251858955\n",
            "  batch 8 loss: 0.9314429915846896\n",
            "LOSS train 0.9314429915846896\n",
            "EPOCH 180:\n",
            "  batch 2 loss: 0.97757839551396\n",
            "  batch 4 loss: 0.8722546772154376\n",
            "  batch 6 loss: 1.0118057477162707\n",
            "  batch 8 loss: 0.8715887854238525\n",
            "LOSS train 0.8715887854238525\n",
            "EPOCH 181:\n",
            "  batch 2 loss: 0.9936178305717622\n",
            "  batch 4 loss: 1.0293086464373304\n",
            "  batch 6 loss: 0.9190383627399625\n",
            "  batch 8 loss: 0.9720225175603311\n",
            "LOSS train 0.9720225175603311\n",
            "EPOCH 182:\n",
            "  batch 2 loss: 1.0591918116326298\n",
            "  batch 4 loss: 0.9686338129910423\n",
            "  batch 6 loss: 0.881740152313053\n",
            "  batch 8 loss: 1.0302202144621353\n",
            "LOSS train 1.0302202144621353\n",
            "EPOCH 183:\n",
            "  batch 2 loss: 1.0119201346849702\n",
            "  batch 4 loss: 1.0181571947688417\n",
            "  batch 6 loss: 1.0258617981680174\n",
            "  batch 8 loss: 0.9889490197325371\n",
            "LOSS train 0.9889490197325371\n",
            "EPOCH 184:\n",
            "  batch 2 loss: 0.8709197287710606\n",
            "  batch 4 loss: 0.998435761159457\n",
            "  batch 6 loss: 1.0488504087712571\n",
            "  batch 8 loss: 1.0085093244347345\n",
            "LOSS train 1.0085093244347345\n",
            "EPOCH 185:\n",
            "  batch 2 loss: 0.9526874399835279\n",
            "  batch 4 loss: 1.0317815881761505\n",
            "  batch 6 loss: 0.8491783917154219\n",
            "  batch 8 loss: 1.003463073418568\n",
            "LOSS train 1.003463073418568\n",
            "EPOCH 186:\n",
            "  batch 2 loss: 0.9737135178817132\n",
            "  batch 4 loss: 0.9882323125863791\n",
            "  batch 6 loss: 0.9470698799989344\n",
            "  batch 8 loss: 0.9774592410562573\n",
            "LOSS train 0.9774592410562573\n",
            "EPOCH 187:\n",
            "  batch 2 loss: 1.0704529438152286\n",
            "  batch 4 loss: 0.8231298472159028\n",
            "  batch 6 loss: 0.9405622144241939\n",
            "  batch 8 loss: 0.9058337776968065\n",
            "LOSS train 0.9058337776968065\n",
            "EPOCH 188:\n",
            "  batch 2 loss: 0.9490363899514509\n",
            "  batch 4 loss: 0.9602550958584302\n",
            "  batch 6 loss: 0.9409688738578859\n",
            "  batch 8 loss: 1.0211614909443758\n",
            "LOSS train 1.0211614909443758\n",
            "EPOCH 189:\n",
            "  batch 2 loss: 0.8205517713062246\n",
            "  batch 4 loss: 0.9431425132562605\n",
            "  batch 6 loss: 0.8680967718526689\n",
            "  batch 8 loss: 1.0067054259671075\n",
            "LOSS train 1.0067054259671075\n",
            "EPOCH 190:\n",
            "  batch 2 loss: 0.8692118879207497\n",
            "  batch 4 loss: 1.0056205338062005\n",
            "  batch 6 loss: 0.9815572659109052\n",
            "  batch 8 loss: 1.01574315847689\n",
            "LOSS train 1.01574315847689\n",
            "EPOCH 191:\n",
            "  batch 2 loss: 0.9563455527127848\n",
            "  batch 4 loss: 0.9536525408011794\n",
            "  batch 6 loss: 0.8519759276286196\n",
            "  batch 8 loss: 0.9284138388713958\n",
            "LOSS train 0.9284138388713958\n",
            "EPOCH 192:\n",
            "  batch 2 loss: 0.9907933650623362\n",
            "  batch 4 loss: 0.9920608145018435\n",
            "  batch 6 loss: 0.9552940667611735\n",
            "  batch 8 loss: 0.962660600103709\n",
            "LOSS train 0.962660600103709\n",
            "EPOCH 193:\n",
            "  batch 2 loss: 1.0044761664887945\n",
            "  batch 4 loss: 0.9720879577350028\n",
            "  batch 6 loss: 1.00651682601243\n",
            "  batch 8 loss: 0.9155109395122668\n",
            "LOSS train 0.9155109395122668\n",
            "EPOCH 194:\n",
            "  batch 2 loss: 0.9064239378500949\n",
            "  batch 4 loss: 0.93356245945501\n",
            "  batch 6 loss: 1.0089437818906157\n",
            "  batch 8 loss: 1.100677293828106\n",
            "LOSS train 1.100677293828106\n",
            "EPOCH 195:\n",
            "  batch 2 loss: 1.0122610525935263\n",
            "  batch 4 loss: 0.8993571966500093\n",
            "  batch 6 loss: 0.9098898299840033\n",
            "  batch 8 loss: 0.7903498584362987\n",
            "LOSS train 0.7903498584362987\n",
            "EPOCH 196:\n",
            "  batch 2 loss: 1.0149786053049448\n",
            "  batch 4 loss: 1.051961875764999\n",
            "  batch 6 loss: 0.9287812375232246\n",
            "  batch 8 loss: 0.9343935962903784\n",
            "LOSS train 0.9343935962903784\n",
            "EPOCH 197:\n",
            "  batch 2 loss: 1.2018229466562884\n",
            "  batch 4 loss: 1.0700485493141887\n",
            "  batch 6 loss: 0.744432373605427\n",
            "  batch 8 loss: 0.8197334465077017\n",
            "LOSS train 0.8197334465077017\n",
            "EPOCH 198:\n",
            "  batch 2 loss: 0.5891821621429092\n",
            "  batch 4 loss: 1.0404677278868468\n",
            "  batch 6 loss: 1.051247165097644\n",
            "  batch 8 loss: 0.9863222716401137\n",
            "LOSS train 0.9863222716401137\n",
            "EPOCH 199:\n",
            "  batch 2 loss: 0.977471980203124\n",
            "  batch 4 loss: 0.8744684504695608\n",
            "  batch 6 loss: 1.0107164590995346\n",
            "  batch 8 loss: 0.8444346254711086\n",
            "LOSS train 0.8444346254711086\n",
            "EPOCH 200:\n",
            "  batch 2 loss: 0.9754745322976526\n",
            "  batch 4 loss: 0.9983699386982845\n",
            "  batch 6 loss: 1.0370173166390866\n",
            "  batch 8 loss: 1.041662003907539\n",
            "LOSS train 1.041662003907539\n",
            "EPOCH 201:\n",
            "  batch 2 loss: 1.1538573201001596\n",
            "  batch 4 loss: 0.8871713257836135\n",
            "  batch 6 loss: 1.0537223369830215\n",
            "  batch 8 loss: 0.8344061331162003\n",
            "LOSS train 0.8344061331162003\n",
            "EPOCH 202:\n",
            "  batch 2 loss: 1.012125801151912\n",
            "  batch 4 loss: 1.014711365398968\n",
            "  batch 6 loss: 0.9005817235816129\n",
            "  batch 8 loss: 1.0054223405584233\n",
            "LOSS train 1.0054223405584233\n",
            "EPOCH 203:\n",
            "  batch 2 loss: 1.0653337404362437\n",
            "  batch 4 loss: 0.9835735563905077\n",
            "  batch 6 loss: 1.0073189675336442\n",
            "  batch 8 loss: 0.9038697498481274\n",
            "LOSS train 0.9038697498481274\n",
            "EPOCH 204:\n",
            "  batch 2 loss: 1.0127061691107175\n",
            "  batch 4 loss: 0.9129404410723987\n",
            "  batch 6 loss: 1.117563301653655\n",
            "  batch 8 loss: 1.0220300584140258\n",
            "LOSS train 1.0220300584140258\n",
            "EPOCH 205:\n",
            "  batch 2 loss: 0.9713855825377852\n",
            "  batch 4 loss: 0.9440356060308754\n",
            "  batch 6 loss: 1.0338656547335792\n",
            "  batch 8 loss: 0.900764978420413\n",
            "LOSS train 0.900764978420413\n",
            "EPOCH 206:\n",
            "  batch 2 loss: 0.9644732394314337\n",
            "  batch 4 loss: 0.9092303750352362\n",
            "  batch 6 loss: 0.9681803391360445\n",
            "  batch 8 loss: 0.9715071859979625\n",
            "LOSS train 0.9715071859979625\n",
            "EPOCH 207:\n",
            "  batch 2 loss: 1.0236495667318968\n",
            "  batch 4 loss: 0.8933866250331586\n",
            "  batch 6 loss: 0.9134000873330161\n",
            "  batch 8 loss: 0.9787189386099784\n",
            "LOSS train 0.9787189386099784\n",
            "EPOCH 208:\n",
            "  batch 2 loss: 0.9058658964335142\n",
            "  batch 4 loss: 1.0432068648247688\n",
            "  batch 6 loss: 0.9688833989641967\n",
            "  batch 8 loss: 1.0167498985985666\n",
            "LOSS train 1.0167498985985666\n",
            "EPOCH 209:\n",
            "  batch 2 loss: 0.814649635848924\n",
            "  batch 4 loss: 1.0183229672218843\n",
            "  batch 6 loss: 1.0306480061765302\n",
            "  batch 8 loss: 1.1132040791860847\n",
            "LOSS train 1.1132040791860847\n",
            "EPOCH 210:\n",
            "  batch 2 loss: 0.9784535033216359\n",
            "  batch 4 loss: 0.9191341535406521\n",
            "  batch 6 loss: 0.7924321602433214\n",
            "  batch 8 loss: 0.9864618676075241\n",
            "LOSS train 0.9864618676075241\n",
            "EPOCH 211:\n",
            "  batch 2 loss: 1.1862343357556768\n",
            "  batch 4 loss: 0.8069480288253132\n",
            "  batch 6 loss: 0.7786432589350125\n",
            "  batch 8 loss: 0.9605021693059567\n",
            "LOSS train 0.9605021693059567\n",
            "EPOCH 212:\n",
            "  batch 2 loss: 1.106791812606244\n",
            "  batch 4 loss: 1.0671943739803473\n",
            "  batch 6 loss: 0.9945097892223402\n",
            "  batch 8 loss: 0.9943334087760411\n",
            "LOSS train 0.9943334087760411\n",
            "EPOCH 213:\n",
            "  batch 2 loss: 1.0590629412178016\n",
            "  batch 4 loss: 1.0622098186426858\n",
            "  batch 6 loss: 0.8018737568324645\n",
            "  batch 8 loss: 0.9874333201229495\n",
            "LOSS train 0.9874333201229495\n",
            "EPOCH 214:\n",
            "  batch 2 loss: 1.0457432029648406\n",
            "  batch 4 loss: 1.0906660407292548\n",
            "  batch 6 loss: 0.8127884897141229\n",
            "  batch 8 loss: 0.9396890847881745\n",
            "LOSS train 0.9396890847881745\n",
            "EPOCH 215:\n",
            "  batch 2 loss: 1.0550073839939147\n",
            "  batch 4 loss: 0.952977920542278\n",
            "  batch 6 loss: 0.9298211307602229\n",
            "  batch 8 loss: 1.0105726471344114\n",
            "LOSS train 1.0105726471344114\n",
            "EPOCH 216:\n",
            "  batch 2 loss: 0.7914536330711768\n",
            "  batch 4 loss: 0.9949016344305261\n",
            "  batch 6 loss: 1.1050440813984879\n",
            "  batch 8 loss: 1.0519267935276493\n",
            "LOSS train 1.0519267935276493\n",
            "EPOCH 217:\n",
            "  batch 2 loss: 0.6726236045463617\n",
            "  batch 4 loss: 1.0056831455930662\n",
            "  batch 6 loss: 0.8185346728570322\n",
            "  batch 8 loss: 0.9386702213200055\n",
            "LOSS train 0.9386702213200055\n",
            "EPOCH 218:\n",
            "  batch 2 loss: 1.0726950269227693\n",
            "  batch 4 loss: 1.0411689269980309\n",
            "  batch 6 loss: 1.0354561351708074\n",
            "  batch 8 loss: 1.0045288210117818\n",
            "LOSS train 1.0045288210117818\n",
            "EPOCH 219:\n",
            "  batch 2 loss: 0.6298061649538014\n",
            "  batch 4 loss: 0.7847211630348971\n",
            "  batch 6 loss: 0.9944871449995416\n",
            "  batch 8 loss: 1.1187830657234272\n",
            "LOSS train 1.1187830657234272\n",
            "EPOCH 220:\n",
            "  batch 2 loss: 0.7842509493691658\n",
            "  batch 4 loss: 1.1425134394632557\n",
            "  batch 6 loss: 1.0314961414344885\n",
            "  batch 8 loss: 0.9991718065535536\n",
            "LOSS train 0.9991718065535536\n",
            "EPOCH 221:\n",
            "  batch 2 loss: 1.0159886455710816\n",
            "  batch 4 loss: 1.026482909001212\n",
            "  batch 6 loss: 1.0311125170585669\n",
            "  batch 8 loss: 0.7922019328900811\n",
            "LOSS train 0.7922019328900811\n",
            "EPOCH 222:\n",
            "  batch 2 loss: 1.1652171065204548\n",
            "  batch 4 loss: 0.7904972211955494\n",
            "  batch 6 loss: 0.9603059612533948\n",
            "  batch 8 loss: 1.2677988290528943\n",
            "LOSS train 1.2677988290528943\n",
            "EPOCH 223:\n",
            "  batch 2 loss: 0.8857498768064396\n",
            "  batch 4 loss: 0.9688359380202892\n",
            "  batch 6 loss: 0.9043399615434313\n",
            "  batch 8 loss: 1.0931349325722723\n",
            "LOSS train 1.0931349325722723\n",
            "EPOCH 224:\n",
            "  batch 2 loss: 0.6372404787964556\n",
            "  batch 4 loss: 1.1047656098587488\n",
            "  batch 6 loss: 1.187020094283528\n",
            "  batch 8 loss: 0.9648482280145623\n",
            "LOSS train 0.9648482280145623\n",
            "EPOCH 225:\n",
            "  batch 2 loss: 0.9640318999803766\n",
            "  batch 4 loss: 0.8828390287182519\n",
            "  batch 6 loss: 0.905001797951511\n",
            "  batch 8 loss: 0.9263314868650379\n",
            "LOSS train 0.9263314868650379\n",
            "EPOCH 226:\n",
            "  batch 2 loss: 1.06065862728033\n",
            "  batch 4 loss: 0.9698085911677495\n",
            "  batch 6 loss: 0.9028655599017988\n",
            "  batch 8 loss: 0.8961126506068278\n",
            "LOSS train 0.8961126506068278\n",
            "EPOCH 227:\n",
            "  batch 2 loss: 1.0246848493985348\n",
            "  batch 4 loss: 0.9330526384921696\n",
            "  batch 6 loss: 1.025407836383674\n",
            "  batch 8 loss: 1.017910428766613\n",
            "LOSS train 1.017910428766613\n",
            "EPOCH 228:\n",
            "  batch 2 loss: 0.7289430896080771\n",
            "  batch 4 loss: 0.7341753284081731\n",
            "  batch 6 loss: 1.0768350338914094\n",
            "  batch 8 loss: 0.7834428957103605\n",
            "LOSS train 0.7834428957103605\n",
            "EPOCH 229:\n",
            "  batch 2 loss: 1.1732893732685392\n",
            "  batch 4 loss: 0.9021489265008442\n",
            "  batch 6 loss: 1.0403495460616188\n",
            "  batch 8 loss: 1.0223760892336247\n",
            "LOSS train 1.0223760892336247\n",
            "EPOCH 230:\n",
            "  batch 2 loss: 0.9532533706247968\n",
            "  batch 4 loss: 0.891633776507752\n",
            "  batch 6 loss: 1.0167554995135761\n",
            "  batch 8 loss: 0.8000777584798463\n",
            "LOSS train 0.8000777584798463\n",
            "EPOCH 231:\n",
            "  batch 2 loss: 1.1092714058034079\n",
            "  batch 4 loss: 1.0158501954725248\n",
            "  batch 6 loss: 1.0894243207304766\n",
            "  batch 8 loss: 1.1037294297570561\n",
            "LOSS train 1.1037294297570561\n",
            "EPOCH 232:\n",
            "  batch 2 loss: 0.7909473655655828\n",
            "  batch 4 loss: 0.8110048091543948\n",
            "  batch 6 loss: 0.8813629297818947\n",
            "  batch 8 loss: 0.9672513726728518\n",
            "LOSS train 0.9672513726728518\n",
            "EPOCH 233:\n",
            "  batch 2 loss: 1.0738582589597918\n",
            "  batch 4 loss: 0.8090908750064225\n",
            "  batch 6 loss: 0.9331904247836307\n",
            "  batch 8 loss: 0.8712789839576676\n",
            "LOSS train 0.8712789839576676\n",
            "EPOCH 234:\n",
            "  batch 2 loss: 0.7560981211935092\n",
            "  batch 4 loss: 0.6479776632374328\n",
            "  batch 6 loss: 1.067969119981369\n",
            "  batch 8 loss: 0.7823695482152436\n",
            "LOSS train 0.7823695482152436\n",
            "EPOCH 235:\n",
            "  batch 2 loss: 0.9149973940853946\n",
            "  batch 4 loss: 3.5426425726864244\n",
            "  batch 6 loss: 0.7217474462199205\n",
            "  batch 8 loss: 1.0494610496761674\n",
            "LOSS train 1.0494610496761674\n",
            "EPOCH 236:\n",
            "  batch 2 loss: 1.096000584697608\n",
            "  batch 4 loss: 0.9622313334070156\n",
            "  batch 6 loss: 0.82410188907476\n",
            "  batch 8 loss: 1.142955569557896\n",
            "LOSS train 1.142955569557896\n",
            "EPOCH 237:\n",
            "  batch 2 loss: 1.1455043234403963\n",
            "  batch 4 loss: 0.7136434432003849\n",
            "  batch 6 loss: 1.5516308235633396\n",
            "  batch 8 loss: 0.6764961678123078\n",
            "LOSS train 0.6764961678123078\n",
            "EPOCH 238:\n",
            "  batch 2 loss: 1.1464580266783646\n",
            "  batch 4 loss: 0.991686140517264\n",
            "  batch 6 loss: 1.0970965387737377\n",
            "  batch 8 loss: 1.3084962521319583\n",
            "LOSS train 1.3084962521319583\n",
            "EPOCH 239:\n",
            "  batch 2 loss: 0.9299067464076229\n",
            "  batch 4 loss: 1.036981526430524\n",
            "  batch 6 loss: 1.0477377259809795\n",
            "  batch 8 loss: 0.8139721441838481\n",
            "LOSS train 0.8139721441838481\n",
            "EPOCH 240:\n",
            "  batch 2 loss: 0.9633002694443675\n",
            "  batch 4 loss: 1.0670620305322496\n",
            "  batch 6 loss: 0.7763293126157346\n",
            "  batch 8 loss: 1.0069298608457131\n",
            "LOSS train 1.0069298608457131\n",
            "EPOCH 241:\n",
            "  batch 2 loss: 0.9899373378367283\n",
            "  batch 4 loss: 0.9037970962238219\n",
            "  batch 6 loss: 1.0136076502089972\n",
            "  batch 8 loss: 0.886935007547159\n",
            "LOSS train 0.886935007547159\n",
            "EPOCH 242:\n",
            "  batch 2 loss: 1.041256214134183\n",
            "  batch 4 loss: 0.9276549584245459\n",
            "  batch 6 loss: 1.2749686782859364\n",
            "  batch 8 loss: 0.9528782440018524\n",
            "LOSS train 0.9528782440018524\n",
            "EPOCH 243:\n",
            "  batch 2 loss: 0.9755732597524923\n",
            "  batch 4 loss: 0.8096730008529084\n",
            "  batch 6 loss: 1.0602200173203298\n",
            "  batch 8 loss: 0.99789109746022\n",
            "LOSS train 0.99789109746022\n",
            "EPOCH 244:\n",
            "  batch 2 loss: 0.833196474684845\n",
            "  batch 4 loss: 1.0670197664380627\n",
            "  batch 6 loss: 0.7860478023464208\n",
            "  batch 8 loss: 1.0230590932528805\n",
            "LOSS train 1.0230590932528805\n",
            "EPOCH 245:\n",
            "  batch 2 loss: 0.9635214557669428\n",
            "  batch 4 loss: 1.0774069563970954\n",
            "  batch 6 loss: 1.0535648636042008\n",
            "  batch 8 loss: 0.9893259693139025\n",
            "LOSS train 0.9893259693139025\n",
            "EPOCH 246:\n",
            "  batch 2 loss: 1.0249351092921994\n",
            "  batch 4 loss: 0.9775486979776051\n",
            "  batch 6 loss: 0.955296042410178\n",
            "  batch 8 loss: 1.0615249542751433\n",
            "LOSS train 1.0615249542751433\n",
            "EPOCH 247:\n",
            "  batch 2 loss: 0.7079058346206542\n",
            "  batch 4 loss: 1.0863827445230179\n",
            "  batch 6 loss: 0.8061928052644901\n",
            "  batch 8 loss: 1.0547890190390405\n",
            "LOSS train 1.0547890190390405\n",
            "EPOCH 248:\n",
            "  batch 2 loss: 1.1553063748786623\n",
            "  batch 4 loss: 0.8272013147540961\n",
            "  batch 6 loss: 0.9286371065421695\n",
            "  batch 8 loss: 1.0482631738421015\n",
            "LOSS train 1.0482631738421015\n",
            "EPOCH 249:\n",
            "  batch 2 loss: 0.9828272710010418\n",
            "  batch 4 loss: 1.0774974956244439\n",
            "  batch 6 loss: 0.9910651599611455\n",
            "  batch 8 loss: 0.8650197800185511\n",
            "LOSS train 0.8650197800185511\n",
            "EPOCH 250:\n",
            "  batch 2 loss: 0.96252726847536\n",
            "  batch 4 loss: 0.9507739319324725\n",
            "  batch 6 loss: 1.0387283942428154\n",
            "  batch 8 loss: 0.7592952962360833\n",
            "LOSS train 0.7592952962360833\n",
            "EPOCH 251:\n",
            "  batch 2 loss: 0.9633189292116937\n",
            "  batch 4 loss: 0.678548144471015\n",
            "  batch 6 loss: 0.8367885696771089\n",
            "  batch 8 loss: 0.9670458837745259\n",
            "LOSS train 0.9670458837745259\n",
            "EPOCH 252:\n",
            "  batch 2 loss: 0.9807593033645544\n",
            "  batch 4 loss: 1.0121156778512033\n",
            "  batch 6 loss: 0.9578112758694783\n",
            "  batch 8 loss: 0.808787790371692\n",
            "LOSS train 0.808787790371692\n",
            "EPOCH 253:\n",
            "  batch 2 loss: 0.9861654074026454\n",
            "  batch 4 loss: 0.9839894021186357\n",
            "  batch 6 loss: 1.007089468303307\n",
            "  batch 8 loss: 1.1222702869411934\n",
            "LOSS train 1.1222702869411934\n",
            "EPOCH 254:\n",
            "  batch 2 loss: 0.9503818630651995\n",
            "  batch 4 loss: 1.03347137783426\n",
            "  batch 6 loss: 0.8516716187469081\n",
            "  batch 8 loss: 0.9000820335614924\n",
            "LOSS train 0.9000820335614924\n",
            "EPOCH 255:\n",
            "  batch 2 loss: 0.7679005817578781\n",
            "  batch 4 loss: 1.0051577877072697\n",
            "  batch 6 loss: 0.9613716000070093\n",
            "  batch 8 loss: 0.9653298637916721\n",
            "LOSS train 0.9653298637916721\n",
            "EPOCH 256:\n",
            "  batch 2 loss: 0.7905539381708484\n",
            "  batch 4 loss: 1.0068976861569747\n",
            "  batch 6 loss: 1.0208585488128938\n",
            "  batch 8 loss: 1.1025163434822933\n",
            "LOSS train 1.1025163434822933\n",
            "EPOCH 257:\n",
            "  batch 2 loss: 0.9581520997385635\n",
            "  batch 4 loss: 0.8659488985102544\n",
            "  batch 6 loss: 0.8298191381466313\n",
            "  batch 8 loss: 1.0556049283126754\n",
            "LOSS train 1.0556049283126754\n",
            "EPOCH 258:\n",
            "  batch 2 loss: 0.9872919302934375\n",
            "  batch 4 loss: 1.0960612705401838\n",
            "  batch 6 loss: 0.8402823299261347\n",
            "  batch 8 loss: 1.028835420101255\n",
            "LOSS train 1.028835420101255\n",
            "EPOCH 259:\n",
            "  batch 2 loss: 1.0219792943114183\n",
            "  batch 4 loss: 0.9852100745036654\n",
            "  batch 6 loss: 0.7402448095353905\n",
            "  batch 8 loss: 0.9963781325728593\n",
            "LOSS train 0.9963781325728593\n",
            "EPOCH 260:\n",
            "  batch 2 loss: 1.0222114227936097\n",
            "  batch 4 loss: 1.0341101998489868\n",
            "  batch 6 loss: 0.8629263456125438\n",
            "  batch 8 loss: 1.007417040349942\n",
            "LOSS train 1.007417040349942\n",
            "EPOCH 261:\n",
            "  batch 2 loss: 1.1113252523042483\n",
            "  batch 4 loss: 0.8447166025984163\n",
            "  batch 6 loss: 0.984781046777257\n",
            "  batch 8 loss: 0.7998826508849584\n",
            "LOSS train 0.7998826508849584\n",
            "EPOCH 262:\n",
            "  batch 2 loss: 0.8371991448403672\n",
            "  batch 4 loss: 1.058009247035471\n",
            "  batch 6 loss: 1.0457155501969089\n",
            "  batch 8 loss: 0.8677823031680825\n",
            "LOSS train 0.8677823031680825\n",
            "EPOCH 263:\n",
            "  batch 2 loss: 0.9324360544227687\n",
            "  batch 4 loss: 0.8096333623613878\n",
            "  batch 6 loss: 0.9645930186114252\n",
            "  batch 8 loss: 0.9237787469088865\n",
            "LOSS train 0.9237787469088865\n",
            "EPOCH 264:\n",
            "  batch 2 loss: 1.0388648055148417\n",
            "  batch 4 loss: 1.1089640142213448\n",
            "  batch 6 loss: 0.9802525723292932\n",
            "  batch 8 loss: 1.0088953959934628\n",
            "LOSS train 1.0088953959934628\n",
            "EPOCH 265:\n",
            "  batch 2 loss: 0.8949501568317441\n",
            "  batch 4 loss: 0.926066245540192\n",
            "  batch 6 loss: 0.9145623779897019\n",
            "  batch 8 loss: 0.9002901888241576\n",
            "LOSS train 0.9002901888241576\n",
            "EPOCH 266:\n",
            "  batch 2 loss: 0.8521303121969654\n",
            "  batch 4 loss: 0.8172744179877558\n",
            "  batch 6 loss: 0.9531673223825701\n",
            "  batch 8 loss: 0.8373441978445109\n",
            "LOSS train 0.8373441978445109\n",
            "EPOCH 267:\n",
            "  batch 2 loss: 1.1809693036591669\n",
            "  batch 4 loss: 1.0454816063701928\n",
            "  batch 6 loss: 0.876152541341991\n",
            "  batch 8 loss: 0.949305419374649\n",
            "LOSS train 0.949305419374649\n",
            "EPOCH 268:\n",
            "  batch 2 loss: 0.7887682935769831\n",
            "  batch 4 loss: 0.8075571915869382\n",
            "  batch 6 loss: 0.9865555270445475\n",
            "  batch 8 loss: 0.8420895575304838\n",
            "LOSS train 0.8420895575304838\n",
            "EPOCH 269:\n",
            "  batch 2 loss: 0.6609907319103041\n",
            "  batch 4 loss: 0.6459742475000331\n",
            "  batch 6 loss: 1.0058195305103137\n",
            "  batch 8 loss: 0.8840557780531759\n",
            "LOSS train 0.8840557780531759\n",
            "EPOCH 270:\n",
            "  batch 2 loss: 0.8183656709669447\n",
            "  batch 4 loss: 0.7873885152660586\n",
            "  batch 6 loss: 0.750708420962447\n",
            "  batch 8 loss: 1.0326297715609345\n",
            "LOSS train 1.0326297715609345\n",
            "EPOCH 271:\n",
            "  batch 2 loss: 0.6859414629447668\n",
            "  batch 4 loss: 0.9654125345987573\n",
            "  batch 6 loss: 0.9537550867501019\n",
            "  batch 8 loss: 1.014048768902327\n",
            "LOSS train 1.014048768902327\n",
            "EPOCH 272:\n",
            "  batch 2 loss: 1.037209907292243\n",
            "  batch 4 loss: 0.8958048292276127\n",
            "  batch 6 loss: 0.894420773503477\n",
            "  batch 8 loss: 0.9672017005462988\n",
            "LOSS train 0.9672017005462988\n",
            "EPOCH 273:\n",
            "  batch 2 loss: 0.7593114395228544\n",
            "  batch 4 loss: 1.4184210982612973\n",
            "  batch 6 loss: 0.7733477974271137\n",
            "  batch 8 loss: 1.1824849631669472\n",
            "LOSS train 1.1824849631669472\n",
            "EPOCH 274:\n",
            "  batch 2 loss: 1.01163395245764\n",
            "  batch 4 loss: 1.1101013863857319\n",
            "  batch 6 loss: 0.9990091032509392\n",
            "  batch 8 loss: 0.7911756189860114\n",
            "LOSS train 0.7911756189860114\n",
            "EPOCH 275:\n",
            "  batch 2 loss: 1.021157045314113\n",
            "  batch 4 loss: 1.0089052222158084\n",
            "  batch 6 loss: 1.0349369663465875\n",
            "  batch 8 loss: 0.8291427182726456\n",
            "LOSS train 0.8291427182726456\n",
            "EPOCH 276:\n",
            "  batch 2 loss: 1.1387507022654653\n",
            "  batch 4 loss: 0.5288478569029056\n",
            "  batch 6 loss: 1.0942177072434982\n",
            "  batch 8 loss: 0.9530455777677643\n",
            "LOSS train 0.9530455777677643\n",
            "EPOCH 277:\n",
            "  batch 2 loss: 0.8469425227454264\n",
            "  batch 4 loss: 0.7518216730712703\n",
            "  batch 6 loss: 1.0526076841753433\n",
            "  batch 8 loss: 1.0963259815259998\n",
            "LOSS train 1.0963259815259998\n",
            "EPOCH 278:\n",
            "  batch 2 loss: 1.1522255550804712\n",
            "  batch 4 loss: 0.6202979090901077\n",
            "  batch 6 loss: 1.0699076765513889\n",
            "  batch 8 loss: 0.9617947210986653\n",
            "LOSS train 0.9617947210986653\n",
            "EPOCH 279:\n",
            "  batch 2 loss: 1.0117634617742874\n",
            "  batch 4 loss: 0.6992841739300261\n",
            "  batch 6 loss: 0.9262265482337954\n",
            "  batch 8 loss: 0.9110089807491781\n",
            "LOSS train 0.9110089807491781\n",
            "EPOCH 280:\n",
            "  batch 2 loss: 0.9262553119695667\n",
            "  batch 4 loss: 0.681384229787233\n",
            "  batch 6 loss: 0.8702462215392291\n",
            "  batch 8 loss: 0.5388799090927165\n",
            "LOSS train 0.5388799090927165\n",
            "EPOCH 281:\n",
            "  batch 2 loss: 0.8548961305319029\n",
            "  batch 4 loss: 1.0755879133192277\n",
            "  batch 6 loss: 1.045517604967984\n",
            "  batch 8 loss: 1.0415907377510427\n",
            "LOSS train 1.0415907377510427\n",
            "EPOCH 282:\n",
            "  batch 2 loss: 0.8268733754940824\n",
            "  batch 4 loss: 0.938446946789635\n",
            "  batch 6 loss: 1.0545022715155463\n",
            "  batch 8 loss: 1.2233575573624558\n",
            "LOSS train 1.2233575573624558\n",
            "EPOCH 283:\n",
            "  batch 2 loss: 1.1017187566950046\n",
            "  batch 4 loss: 0.5864874185581826\n",
            "  batch 6 loss: 0.7718227367785788\n",
            "  batch 8 loss: 1.1059390842386374\n",
            "LOSS train 1.1059390842386374\n",
            "EPOCH 284:\n",
            "  batch 2 loss: 1.0415900264578242\n",
            "  batch 4 loss: 0.8823433894263842\n",
            "  batch 6 loss: 0.8635498420177437\n",
            "  batch 8 loss: 0.9326736582791367\n",
            "LOSS train 0.9326736582791367\n",
            "EPOCH 285:\n",
            "  batch 2 loss: 1.0419779715439583\n",
            "  batch 4 loss: 1.207982998471428\n",
            "  batch 6 loss: 0.9323807126903476\n",
            "  batch 8 loss: 0.7626220476290226\n",
            "LOSS train 0.7626220476290226\n",
            "EPOCH 286:\n",
            "  batch 2 loss: 0.5819723347191224\n",
            "  batch 4 loss: 0.7738449471292825\n",
            "  batch 6 loss: 0.9143552642441211\n",
            "  batch 8 loss: 1.0015573589779363\n",
            "LOSS train 1.0015573589779363\n",
            "EPOCH 287:\n",
            "  batch 2 loss: 1.0232982844970542\n",
            "  batch 4 loss: 1.044855296842045\n",
            "  batch 6 loss: 0.9490659600058104\n",
            "  batch 8 loss: 1.0594379332371606\n",
            "LOSS train 1.0594379332371606\n",
            "EPOCH 288:\n",
            "  batch 2 loss: 1.0604954526871024\n",
            "  batch 4 loss: 1.1478090025632985\n",
            "  batch 6 loss: 0.8665334946770652\n",
            "  batch 8 loss: 0.7799757396536304\n",
            "LOSS train 0.7799757396536304\n",
            "EPOCH 289:\n",
            "  batch 2 loss: 0.7748178133060248\n",
            "  batch 4 loss: 1.0020718614383317\n",
            "  batch 6 loss: 1.0666868405960255\n",
            "  batch 8 loss: 1.1587296264722076\n",
            "LOSS train 1.1587296264722076\n",
            "EPOCH 290:\n",
            "  batch 2 loss: 0.8168535220237036\n",
            "  batch 4 loss: 1.0367023269196032\n",
            "  batch 6 loss: 1.2472684821723008\n",
            "  batch 8 loss: 0.9431404039352034\n",
            "LOSS train 0.9431404039352034\n",
            "EPOCH 291:\n",
            "  batch 2 loss: 1.1380711487658988\n",
            "  batch 4 loss: 1.014972238494933\n",
            "  batch 6 loss: 1.1121065156058672\n",
            "  batch 8 loss: 0.9972480065282838\n",
            "LOSS train 0.9972480065282838\n",
            "EPOCH 292:\n",
            "  batch 2 loss: 0.9648595169149039\n",
            "  batch 4 loss: 1.0052786381658763\n",
            "  batch 6 loss: 1.2391328412419957\n",
            "  batch 8 loss: 0.7155880219978121\n",
            "LOSS train 0.7155880219978121\n",
            "EPOCH 293:\n",
            "  batch 2 loss: 0.9194816370596047\n",
            "  batch 4 loss: 0.6299377087349833\n",
            "  batch 6 loss: 0.6398479328785702\n",
            "  batch 8 loss: 0.7903525083027444\n",
            "LOSS train 0.7903525083027444\n",
            "EPOCH 294:\n",
            "  batch 2 loss: 0.8800357361197035\n",
            "  batch 4 loss: 0.6986181532817377\n",
            "  batch 6 loss: 0.7108626527042541\n",
            "  batch 8 loss: 1.1964905925340124\n",
            "LOSS train 1.1964905925340124\n",
            "EPOCH 295:\n",
            "  batch 2 loss: 0.7545791799654483\n",
            "  batch 4 loss: 1.0024651555315773\n",
            "  batch 6 loss: 0.9737656171399202\n",
            "  batch 8 loss: 1.0785868379461796\n",
            "LOSS train 1.0785868379461796\n",
            "EPOCH 296:\n",
            "  batch 2 loss: 0.6428430297975929\n",
            "  batch 4 loss: 0.84858646013725\n",
            "  batch 6 loss: 0.9824460914124558\n",
            "  batch 8 loss: 0.9127261675507399\n",
            "LOSS train 0.9127261675507399\n",
            "EPOCH 297:\n",
            "  batch 2 loss: 0.9883002628199908\n",
            "  batch 4 loss: 1.113266703131339\n",
            "  batch 6 loss: 0.3726008297599309\n",
            "  batch 8 loss: 1.0311019188892998\n",
            "LOSS train 1.0311019188892998\n",
            "EPOCH 298:\n",
            "  batch 2 loss: 0.8212175807200383\n",
            "  batch 4 loss: 0.9538046454792124\n",
            "  batch 6 loss: 0.9474111418000256\n",
            "  batch 8 loss: 0.8067906890712028\n",
            "LOSS train 0.8067906890712028\n",
            "EPOCH 299:\n",
            "  batch 2 loss: 0.861098994504239\n",
            "  batch 4 loss: 0.7011618257145321\n",
            "  batch 6 loss: 1.079004192196129\n",
            "  batch 8 loss: 1.0322889714451071\n",
            "LOSS train 1.0322889714451071\n",
            "EPOCH 300:\n",
            "  batch 2 loss: 0.9762036877571167\n",
            "  batch 4 loss: 0.7009437510162111\n",
            "  batch 6 loss: 0.9823064494288601\n",
            "  batch 8 loss: 0.6509451146709435\n",
            "LOSS train 0.6509451146709435\n",
            "EPOCH 301:\n",
            "  batch 2 loss: 0.9249989149017259\n",
            "  batch 4 loss: 0.5061640858504666\n",
            "  batch 6 loss: 1.0670532455743298\n",
            "  batch 8 loss: 0.7106166046743863\n",
            "LOSS train 0.7106166046743863\n",
            "EPOCH 302:\n",
            "  batch 2 loss: 0.9161882451436465\n",
            "  batch 4 loss: 1.0095244832152188\n",
            "  batch 6 loss: 0.7693428816838085\n",
            "  batch 8 loss: 0.9730364554966577\n",
            "LOSS train 0.9730364554966577\n",
            "EPOCH 303:\n",
            "  batch 2 loss: 0.5960964774273081\n",
            "  batch 4 loss: 1.1153946779732862\n",
            "  batch 6 loss: 1.3594975366845905\n",
            "  batch 8 loss: 0.8305237498480571\n",
            "LOSS train 0.8305237498480571\n",
            "EPOCH 304:\n",
            "  batch 2 loss: 0.9338205280302063\n",
            "  batch 4 loss: 14.426420091477304\n",
            "  batch 6 loss: 1.074458503311757\n",
            "  batch 8 loss: 3.149920661481051\n",
            "LOSS train 3.149920661481051\n",
            "EPOCH 305:\n",
            "  batch 2 loss: 3.1924969396866576\n",
            "  batch 4 loss: 1.9022122063432962\n",
            "  batch 6 loss: 2.727705721289613\n",
            "  batch 8 loss: 1.5004086047979588\n",
            "LOSS train 1.5004086047979588\n",
            "EPOCH 306:\n",
            "  batch 2 loss: 1.0673714553984865\n",
            "  batch 4 loss: 4.945190629670963\n",
            "  batch 6 loss: 4.465150039928399\n",
            "  batch 8 loss: 1.1686385957386451\n",
            "LOSS train 1.1686385957386451\n",
            "EPOCH 307:\n",
            "  batch 2 loss: 1.1211254327597646\n",
            "  batch 4 loss: 0.7116189602582562\n",
            "  batch 6 loss: 1.4350569225523513\n",
            "  batch 8 loss: 0.9959906334553035\n",
            "LOSS train 0.9959906334553035\n",
            "EPOCH 308:\n",
            "  batch 2 loss: 0.797196109970085\n",
            "  batch 4 loss: 1.4943039410546866\n",
            "  batch 6 loss: 0.8206630119611298\n",
            "  batch 8 loss: 0.8347376169167295\n",
            "LOSS train 0.8347376169167295\n",
            "EPOCH 309:\n",
            "  batch 2 loss: 1.0907926431004844\n",
            "  batch 4 loss: 0.9248722118423007\n",
            "  batch 6 loss: 3.782448540730838\n",
            "  batch 8 loss: 1.092800635498102\n",
            "LOSS train 1.092800635498102\n",
            "EPOCH 310:\n",
            "  batch 2 loss: 0.88947209059082\n",
            "  batch 4 loss: 1.1912842630845828\n",
            "  batch 6 loss: 1.4193147793758432\n",
            "  batch 8 loss: 1.783477414033592\n",
            "LOSS train 1.783477414033592\n",
            "EPOCH 311:\n",
            "  batch 2 loss: 1.4418880586594383\n",
            "  batch 4 loss: 1.1425548259964653\n",
            "  batch 6 loss: 1.3799683645242968\n",
            "  batch 8 loss: 0.8847775110865395\n",
            "LOSS train 0.8847775110865395\n",
            "EPOCH 312:\n",
            "  batch 2 loss: 1.3830400581511593\n",
            "  batch 4 loss: 2.0202858368316177\n",
            "  batch 6 loss: 1.6239478071703424\n",
            "  batch 8 loss: 0.8920671929819195\n",
            "LOSS train 0.8920671929819195\n",
            "EPOCH 313:\n",
            "  batch 2 loss: 1.0890157420117506\n",
            "  batch 4 loss: 0.9574022602600748\n",
            "  batch 6 loss: 0.6934932467304352\n",
            "  batch 8 loss: 0.9381111007137966\n",
            "LOSS train 0.9381111007137966\n",
            "EPOCH 314:\n",
            "  batch 2 loss: 0.8691297411916992\n",
            "  batch 4 loss: 0.9108494606445179\n",
            "  batch 6 loss: 0.7125235635776469\n",
            "  batch 8 loss: 0.7549991870258528\n",
            "LOSS train 0.7549991870258528\n",
            "EPOCH 315:\n",
            "  batch 2 loss: 0.7763139719883194\n",
            "  batch 4 loss: 1.0144299045539318\n",
            "  batch 6 loss: 0.7272528451227777\n",
            "  batch 8 loss: 1.1115132170100308\n",
            "LOSS train 1.1115132170100308\n",
            "EPOCH 316:\n",
            "  batch 2 loss: 0.8683483259269345\n",
            "  batch 4 loss: 1.8148842389076525\n",
            "  batch 6 loss: 0.7166407215171356\n",
            "  batch 8 loss: 1.0669938579334755\n",
            "LOSS train 1.0669938579334755\n",
            "EPOCH 317:\n",
            "  batch 2 loss: 1.0052453809021706\n",
            "  batch 4 loss: 1.0126997214903188\n",
            "  batch 6 loss: 1.6590352486585065\n",
            "  batch 8 loss: 0.8705983081881734\n",
            "LOSS train 0.8705983081881734\n",
            "EPOCH 318:\n",
            "  batch 2 loss: 0.9794225653776569\n",
            "  batch 4 loss: 0.876710979268402\n",
            "  batch 6 loss: 0.8837397390687574\n",
            "  batch 8 loss: 1.0279944569497173\n",
            "LOSS train 1.0279944569497173\n",
            "EPOCH 319:\n",
            "  batch 2 loss: 0.9507696748221002\n",
            "  batch 4 loss: 0.9717212650544435\n",
            "  batch 6 loss: 1.0473753409915152\n",
            "  batch 8 loss: 0.7631200429267829\n",
            "LOSS train 0.7631200429267829\n",
            "EPOCH 320:\n",
            "  batch 2 loss: 0.9028061183742515\n",
            "  batch 4 loss: 0.8789061856818445\n",
            "  batch 6 loss: 0.8209794958908812\n",
            "  batch 8 loss: 0.810058018807635\n",
            "LOSS train 0.810058018807635\n",
            "EPOCH 321:\n",
            "  batch 2 loss: 0.8406636389516394\n",
            "  batch 4 loss: 0.9907159199243202\n",
            "  batch 6 loss: 1.0303886102232398\n",
            "  batch 8 loss: 1.1507070828834938\n",
            "LOSS train 1.1507070828834938\n",
            "EPOCH 322:\n",
            "  batch 2 loss: 1.0190818561964838\n",
            "  batch 4 loss: 1.0162031692746876\n",
            "  batch 6 loss: 1.0398754422819274\n",
            "  batch 8 loss: 1.0324875826123152\n",
            "LOSS train 1.0324875826123152\n",
            "EPOCH 323:\n",
            "  batch 2 loss: 0.9017491834189821\n",
            "  batch 4 loss: 0.8989531147843817\n",
            "  batch 6 loss: 1.939996335478338\n",
            "  batch 8 loss: 0.8290553558429505\n",
            "LOSS train 0.8290553558429505\n",
            "EPOCH 324:\n",
            "  batch 2 loss: 1.269328938917095\n",
            "  batch 4 loss: 0.9710219751664629\n",
            "  batch 6 loss: 1.0192308352680763\n",
            "  batch 8 loss: 0.8601296362032756\n",
            "LOSS train 0.8601296362032756\n",
            "EPOCH 325:\n",
            "  batch 2 loss: 1.120874668090598\n",
            "  batch 4 loss: 0.9705734325278792\n",
            "  batch 6 loss: 0.6686381527434161\n",
            "  batch 8 loss: 0.8836870824526095\n",
            "LOSS train 0.8836870824526095\n",
            "EPOCH 326:\n",
            "  batch 2 loss: 0.940283086090264\n",
            "  batch 4 loss: 1.0751171179047299\n",
            "  batch 6 loss: 0.9918455269529843\n",
            "  batch 8 loss: 0.8592443470302935\n",
            "LOSS train 0.8592443470302935\n",
            "EPOCH 327:\n",
            "  batch 2 loss: 0.98208649965486\n",
            "  batch 4 loss: 0.8789156829141915\n",
            "  batch 6 loss: 0.8788043769310973\n",
            "  batch 8 loss: 1.0543861976410422\n",
            "LOSS train 1.0543861976410422\n",
            "EPOCH 328:\n",
            "  batch 2 loss: 0.9735460463154922\n",
            "  batch 4 loss: 1.0373493217255594\n",
            "  batch 6 loss: 1.0321864104757563\n",
            "  batch 8 loss: 0.8526353832024864\n",
            "LOSS train 0.8526353832024864\n",
            "EPOCH 329:\n",
            "  batch 2 loss: 0.823778534856857\n",
            "  batch 4 loss: 0.812630215809429\n",
            "  batch 6 loss: 1.0294150759238292\n",
            "  batch 8 loss: 1.0674654005842696\n",
            "LOSS train 1.0674654005842696\n",
            "EPOCH 330:\n",
            "  batch 2 loss: 1.0534232377721584\n",
            "  batch 4 loss: 0.838581462818855\n",
            "  batch 6 loss: 0.9386344023197873\n",
            "  batch 8 loss: 0.9318896322348601\n",
            "LOSS train 0.9318896322348601\n",
            "EPOCH 331:\n",
            "  batch 2 loss: 0.7400363100126941\n",
            "  batch 4 loss: 0.8856146424198124\n",
            "  batch 6 loss: 1.0799216325643601\n",
            "  batch 8 loss: 1.034081685184581\n",
            "LOSS train 1.034081685184581\n",
            "EPOCH 332:\n",
            "  batch 2 loss: 1.4852574069574174\n",
            "  batch 4 loss: 0.973705999045748\n",
            "  batch 6 loss: 0.9244464181406482\n",
            "  batch 8 loss: 0.9864163976784289\n",
            "LOSS train 0.9864163976784289\n",
            "EPOCH 333:\n",
            "  batch 2 loss: 0.763251077063275\n",
            "  batch 4 loss: 0.966482257901836\n",
            "  batch 6 loss: 1.0408097965004663\n",
            "  batch 8 loss: 0.9069380191182435\n",
            "LOSS train 0.9069380191182435\n",
            "EPOCH 334:\n",
            "  batch 2 loss: 0.7928319293857391\n",
            "  batch 4 loss: 1.1255277726497956\n",
            "  batch 6 loss: 0.8934639699651483\n",
            "  batch 8 loss: 0.6922975370906146\n",
            "LOSS train 0.6922975370906146\n",
            "EPOCH 335:\n",
            "  batch 2 loss: 0.8668169026166208\n",
            "  batch 4 loss: 1.0415384777307035\n",
            "  batch 6 loss: 0.7210852025453545\n",
            "  batch 8 loss: 1.0035574200314508\n",
            "LOSS train 1.0035574200314508\n",
            "EPOCH 336:\n",
            "  batch 2 loss: 0.9131403037851691\n",
            "  batch 4 loss: 0.9372031705137212\n",
            "  batch 6 loss: 1.0253747235675665\n",
            "  batch 8 loss: 0.8252224650926071\n",
            "LOSS train 0.8252224650926071\n",
            "EPOCH 337:\n",
            "  batch 2 loss: 1.0973218583402127\n",
            "  batch 4 loss: 0.9257789524234958\n",
            "  batch 6 loss: 1.0890413424073848\n",
            "  batch 8 loss: 1.2811596190712282\n",
            "LOSS train 1.2811596190712282\n",
            "EPOCH 338:\n",
            "  batch 2 loss: 0.8836487963191249\n",
            "  batch 4 loss: 1.00483251664859\n",
            "  batch 6 loss: 1.1564506554848113\n",
            "  batch 8 loss: 0.7586838375835052\n",
            "LOSS train 0.7586838375835052\n",
            "EPOCH 339:\n",
            "  batch 2 loss: 1.0809828289404075\n",
            "  batch 4 loss: 0.7461030109339943\n",
            "  batch 6 loss: 0.9964146777858812\n",
            "  batch 8 loss: 0.7710407942303777\n",
            "LOSS train 0.7710407942303777\n",
            "EPOCH 340:\n",
            "  batch 2 loss: 0.9289963401466816\n",
            "  batch 4 loss: 0.7554136377564012\n",
            "  batch 6 loss: 0.8817466663336968\n",
            "  batch 8 loss: 0.8052278575383885\n",
            "LOSS train 0.8052278575383885\n",
            "EPOCH 341:\n",
            "  batch 2 loss: 0.9561551013845218\n",
            "  batch 4 loss: 0.8539003440069897\n",
            "  batch 6 loss: 5.9238949711721105\n",
            "  batch 8 loss: 1.0119822402700347\n",
            "LOSS train 1.0119822402700347\n",
            "EPOCH 342:\n",
            "  batch 2 loss: 0.8653848502860698\n",
            "  batch 4 loss: 0.8876566958814318\n",
            "  batch 6 loss: 0.9533367760741431\n",
            "  batch 8 loss: 1.0994685419994972\n",
            "LOSS train 1.0994685419994972\n",
            "EPOCH 343:\n",
            "  batch 2 loss: 1.248624164833601\n",
            "  batch 4 loss: 0.9440717043144695\n",
            "  batch 6 loss: 1.3792729701765658\n",
            "  batch 8 loss: 0.917595019041914\n",
            "LOSS train 0.917595019041914\n",
            "EPOCH 344:\n",
            "  batch 2 loss: 1.099312630671379\n",
            "  batch 4 loss: 1.5017874533022095\n",
            "  batch 6 loss: 1.241533299049633\n",
            "  batch 8 loss: 1.0291333888430838\n",
            "LOSS train 1.0291333888430838\n",
            "EPOCH 345:\n",
            "  batch 2 loss: 0.9655353852974409\n",
            "  batch 4 loss: 1.0347977333626859\n",
            "  batch 6 loss: 0.9922808992697115\n",
            "  batch 8 loss: 1.0172261949537256\n",
            "LOSS train 1.0172261949537256\n",
            "EPOCH 346:\n",
            "  batch 2 loss: 1.0185149316666746\n",
            "  batch 4 loss: 1.122918086073455\n",
            "  batch 6 loss: 0.982212520317111\n",
            "  batch 8 loss: 0.9412285727067946\n",
            "LOSS train 0.9412285727067946\n",
            "EPOCH 347:\n",
            "  batch 2 loss: 1.0527743582657494\n",
            "  batch 4 loss: 0.9714771216377454\n",
            "  batch 6 loss: 1.0970945812576516\n",
            "  batch 8 loss: 1.0407391053589792\n",
            "LOSS train 1.0407391053589792\n",
            "EPOCH 348:\n",
            "  batch 2 loss: 0.9827338887987297\n",
            "  batch 4 loss: 0.8625232856124639\n",
            "  batch 6 loss: 0.9881006880208192\n",
            "  batch 8 loss: 1.0002442113827796\n",
            "LOSS train 1.0002442113827796\n",
            "EPOCH 349:\n",
            "  batch 2 loss: 1.0287509487307012\n",
            "  batch 4 loss: 1.013119079829269\n",
            "  batch 6 loss: 1.0100245498915528\n",
            "  batch 8 loss: 0.9757373746346931\n",
            "LOSS train 0.9757373746346931\n",
            "EPOCH 350:\n",
            "  batch 2 loss: 0.9338119158043149\n",
            "  batch 4 loss: 0.9223948515401182\n",
            "  batch 6 loss: 1.0021985369958515\n",
            "  batch 8 loss: 0.9414939152192506\n",
            "LOSS train 0.9414939152192506\n",
            "EPOCH 351:\n",
            "  batch 2 loss: 0.9107463711940593\n",
            "  batch 4 loss: 1.036458460397012\n",
            "  batch 6 loss: 1.0447727616667928\n",
            "  batch 8 loss: 0.9840857814275462\n",
            "LOSS train 0.9840857814275462\n",
            "EPOCH 352:\n",
            "  batch 2 loss: 1.0171864409238736\n",
            "  batch 4 loss: 0.9447332171448268\n",
            "  batch 6 loss: 0.9446821495195471\n",
            "  batch 8 loss: 0.992436279408671\n",
            "LOSS train 0.992436279408671\n",
            "EPOCH 353:\n",
            "  batch 2 loss: 1.0079316865338024\n",
            "  batch 4 loss: 1.0457025823259745\n",
            "  batch 6 loss: 0.923224333134272\n",
            "  batch 8 loss: 0.9279999015930376\n",
            "LOSS train 0.9279999015930376\n",
            "EPOCH 354:\n",
            "  batch 2 loss: 0.785009961840432\n",
            "  batch 4 loss: 1.0762998458624446\n",
            "  batch 6 loss: 1.0021816319552779\n",
            "  batch 8 loss: 1.0067726646271895\n",
            "LOSS train 1.0067726646271895\n",
            "EPOCH 355:\n",
            "  batch 2 loss: 1.0221695723924111\n",
            "  batch 4 loss: 0.9446934155950857\n",
            "  batch 6 loss: 0.9586700374569226\n",
            "  batch 8 loss: 1.007652924427545\n",
            "LOSS train 1.007652924427545\n",
            "EPOCH 356:\n",
            "  batch 2 loss: 0.9544016979550701\n",
            "  batch 4 loss: 1.0070341674600276\n",
            "  batch 6 loss: 1.3679119906752002\n",
            "  batch 8 loss: 1.0022660338539249\n",
            "LOSS train 1.0022660338539249\n",
            "EPOCH 357:\n",
            "  batch 2 loss: 1.2202574844646346\n",
            "  batch 4 loss: 1.0156213767142912\n",
            "  batch 6 loss: 1.0273868628090308\n",
            "  batch 8 loss: 1.0707067906327992\n",
            "LOSS train 1.0707067906327992\n",
            "EPOCH 358:\n",
            "  batch 2 loss: 1.044076313589529\n",
            "  batch 4 loss: 1.0056018862198197\n",
            "  batch 6 loss: 0.9674372087929413\n",
            "  batch 8 loss: 0.9393967097285997\n",
            "LOSS train 0.9393967097285997\n",
            "EPOCH 359:\n",
            "  batch 2 loss: 0.9896410755410134\n",
            "  batch 4 loss: 1.012524477528893\n",
            "  batch 6 loss: 0.96832632715094\n",
            "  batch 8 loss: 0.9835060130834538\n",
            "LOSS train 0.9835060130834538\n",
            "EPOCH 360:\n",
            "  batch 2 loss: 0.907603676393149\n",
            "  batch 4 loss: 0.9145119145165043\n",
            "  batch 6 loss: 0.9841653196774156\n",
            "  batch 8 loss: 0.9674092687795044\n",
            "LOSS train 0.9674092687795044\n",
            "EPOCH 361:\n",
            "  batch 2 loss: 1.0010414805113257\n",
            "  batch 4 loss: 0.8903841617132235\n",
            "  batch 6 loss: 0.9973847805853168\n",
            "  batch 8 loss: 0.939935165013936\n",
            "LOSS train 0.939935165013936\n",
            "EPOCH 362:\n",
            "  batch 2 loss: 0.9631015324319587\n",
            "  batch 4 loss: 0.8643278350701951\n",
            "  batch 6 loss: 1.0146031661075352\n",
            "  batch 8 loss: 1.0334493724047193\n",
            "LOSS train 1.0334493724047193\n",
            "EPOCH 363:\n",
            "  batch 2 loss: 0.9884092228225553\n",
            "  batch 4 loss: 1.0099100654913222\n",
            "  batch 6 loss: 0.8524983289487953\n",
            "  batch 8 loss: 0.9242341123534273\n",
            "LOSS train 0.9242341123534273\n",
            "EPOCH 364:\n",
            "  batch 2 loss: 1.0451355101800996\n",
            "  batch 4 loss: 1.0092932637629306\n",
            "  batch 6 loss: 0.8276489827140211\n",
            "  batch 8 loss: 1.049658245094041\n",
            "LOSS train 1.049658245094041\n",
            "EPOCH 365:\n",
            "  batch 2 loss: 1.0005540506174524\n",
            "  batch 4 loss: 0.8812308423817158\n",
            "  batch 6 loss: 0.9156693697988064\n",
            "  batch 8 loss: 1.0026920209347088\n",
            "LOSS train 1.0026920209347088\n",
            "EPOCH 366:\n",
            "  batch 2 loss: 1.0241038443655515\n",
            "  batch 4 loss: 0.989722365271573\n",
            "  batch 6 loss: 1.0223855853359582\n",
            "  batch 8 loss: 1.0077302798715344\n",
            "LOSS train 1.0077302798715344\n",
            "EPOCH 367:\n",
            "  batch 2 loss: 1.0189780797556875\n",
            "  batch 4 loss: 1.0041307833596638\n",
            "  batch 6 loss: 0.9833645290451418\n",
            "  batch 8 loss: 1.0360980266407198\n",
            "LOSS train 1.0360980266407198\n",
            "EPOCH 368:\n",
            "  batch 2 loss: 0.8609487282665085\n",
            "  batch 4 loss: 0.9596885623304068\n",
            "  batch 6 loss: 1.0281949300593645\n",
            "  batch 8 loss: 0.9767446160318386\n",
            "LOSS train 0.9767446160318386\n",
            "EPOCH 369:\n",
            "  batch 2 loss: 0.8019141175566742\n",
            "  batch 4 loss: 1.013656868197519\n",
            "  batch 6 loss: 1.0030901148453069\n",
            "  batch 8 loss: 0.9671339099863645\n",
            "LOSS train 0.9671339099863645\n",
            "EPOCH 370:\n",
            "  batch 2 loss: 0.7942316412843216\n",
            "  batch 4 loss: 1.0191780561586838\n",
            "  batch 6 loss: 0.9952687653751822\n",
            "  batch 8 loss: 1.043584109393195\n",
            "LOSS train 1.043584109393195\n",
            "EPOCH 371:\n",
            "  batch 2 loss: 1.008079724166452\n",
            "  batch 4 loss: 0.9936804953427897\n",
            "  batch 6 loss: 1.0679822681726483\n",
            "  batch 8 loss: 1.3366848911901332\n",
            "LOSS train 1.3366848911901332\n",
            "EPOCH 372:\n",
            "  batch 2 loss: 0.9158757063634022\n",
            "  batch 4 loss: 0.9546250045571887\n",
            "  batch 6 loss: 1.0123845958426352\n",
            "  batch 8 loss: 0.7879615979314296\n",
            "LOSS train 0.7879615979314296\n",
            "EPOCH 373:\n",
            "  batch 2 loss: 0.8326630706808493\n",
            "  batch 4 loss: 1.024487692401525\n",
            "  batch 6 loss: 1.1597464849549164\n",
            "  batch 8 loss: 1.02428275895491\n",
            "LOSS train 1.02428275895491\n",
            "EPOCH 374:\n",
            "  batch 2 loss: 0.9584433244599071\n",
            "  batch 4 loss: 0.826633618224133\n",
            "  batch 6 loss: 1.0492485854988653\n",
            "  batch 8 loss: 0.9919918115312\n",
            "LOSS train 0.9919918115312\n",
            "EPOCH 375:\n",
            "  batch 2 loss: 0.9732783407709316\n",
            "  batch 4 loss: 1.0010036703542202\n",
            "  batch 6 loss: 0.9988701031524833\n",
            "  batch 8 loss: 0.899108255476609\n",
            "LOSS train 0.899108255476609\n",
            "EPOCH 376:\n",
            "  batch 2 loss: 0.948901723441313\n",
            "  batch 4 loss: 0.8947302679421851\n",
            "  batch 6 loss: 1.0668575834083476\n",
            "  batch 8 loss: 0.9230316568515439\n",
            "LOSS train 0.9230316568515439\n",
            "EPOCH 377:\n",
            "  batch 2 loss: 1.0411416140236804\n",
            "  batch 4 loss: 0.825985919515807\n",
            "  batch 6 loss: 1.028544347777265\n",
            "  batch 8 loss: 0.8536973510252017\n",
            "LOSS train 0.8536973510252017\n",
            "EPOCH 378:\n",
            "  batch 2 loss: 0.9520439405318984\n",
            "  batch 4 loss: 0.9962268684583745\n",
            "  batch 6 loss: 1.0136716082708834\n",
            "  batch 8 loss: 0.9384928689729234\n",
            "LOSS train 0.9384928689729234\n",
            "EPOCH 379:\n",
            "  batch 2 loss: 0.9346363397206867\n",
            "  batch 4 loss: 0.9899952714526156\n",
            "  batch 6 loss: 1.0141690145889832\n",
            "  batch 8 loss: 0.9570287400827229\n",
            "LOSS train 0.9570287400827229\n",
            "EPOCH 380:\n",
            "  batch 2 loss: 0.8462978401536203\n",
            "  batch 4 loss: 0.974739068992979\n",
            "  batch 6 loss: 0.8687596586968042\n",
            "  batch 8 loss: 1.025549654123129\n",
            "LOSS train 1.025549654123129\n",
            "EPOCH 381:\n",
            "  batch 2 loss: 1.0322237251963913\n",
            "  batch 4 loss: 0.9697356488394261\n",
            "  batch 6 loss: 1.0096554587075024\n",
            "  batch 8 loss: 1.011989440328609\n",
            "LOSS train 1.011989440328609\n",
            "EPOCH 382:\n",
            "  batch 2 loss: 0.9753699552566862\n",
            "  batch 4 loss: 0.94075946577226\n",
            "  batch 6 loss: 0.9896609163124253\n",
            "  batch 8 loss: 1.0496007793054734\n",
            "LOSS train 1.0496007793054734\n",
            "EPOCH 383:\n",
            "  batch 2 loss: 1.0117472931932068\n",
            "  batch 4 loss: 0.9375450574143073\n",
            "  batch 6 loss: 0.998551239533175\n",
            "  batch 8 loss: 1.036224120556316\n",
            "LOSS train 1.036224120556316\n",
            "EPOCH 384:\n",
            "  batch 2 loss: 0.9312140514249057\n",
            "  batch 4 loss: 1.0090252570020986\n",
            "  batch 6 loss: 1.0336453666518324\n",
            "  batch 8 loss: 1.087395510497431\n",
            "LOSS train 1.087395510497431\n",
            "EPOCH 385:\n",
            "  batch 2 loss: 0.867515715878455\n",
            "  batch 4 loss: 0.8721411142585527\n",
            "  batch 6 loss: 0.9036976427168092\n",
            "  batch 8 loss: 1.011647199691009\n",
            "LOSS train 1.011647199691009\n",
            "EPOCH 386:\n",
            "  batch 2 loss: 0.9366292190998866\n",
            "  batch 4 loss: 1.0149109108115626\n",
            "  batch 6 loss: 1.0337162349126188\n",
            "  batch 8 loss: 0.9377642466017745\n",
            "LOSS train 0.9377642466017745\n",
            "EPOCH 387:\n",
            "  batch 2 loss: 0.7003927722077445\n",
            "  batch 4 loss: 0.9055369452419199\n",
            "  batch 6 loss: 1.0341345023921416\n",
            "  batch 8 loss: 0.8781271088217997\n",
            "LOSS train 0.8781271088217997\n",
            "EPOCH 388:\n",
            "  batch 2 loss: 0.9421075089219102\n",
            "  batch 4 loss: 0.851644410163927\n",
            "  batch 6 loss: 0.9626466169004099\n",
            "  batch 8 loss: 0.9382838238729797\n",
            "LOSS train 0.9382838238729797\n",
            "EPOCH 389:\n",
            "  batch 2 loss: 1.0953495219614768\n",
            "  batch 4 loss: 0.9492734840666767\n",
            "  batch 6 loss: 0.9838624036393767\n",
            "  batch 8 loss: 0.8979275014972076\n",
            "LOSS train 0.8979275014972076\n",
            "EPOCH 390:\n",
            "  batch 2 loss: 1.0024319481162682\n",
            "  batch 4 loss: 1.0353642924219493\n",
            "  batch 6 loss: 0.8256123286892325\n",
            "  batch 8 loss: 0.8420447065867132\n",
            "LOSS train 0.8420447065867132\n",
            "EPOCH 391:\n",
            "  batch 2 loss: 1.025596437379395\n",
            "  batch 4 loss: 1.0224013892036243\n",
            "  batch 6 loss: 1.1157527234631226\n",
            "  batch 8 loss: 0.8169296535756654\n",
            "LOSS train 0.8169296535756654\n",
            "EPOCH 392:\n",
            "  batch 2 loss: 0.9690312695295895\n",
            "  batch 4 loss: 0.8601070040970817\n",
            "  batch 6 loss: 0.942484555173398\n",
            "  batch 8 loss: 0.8732218198967302\n",
            "LOSS train 0.8732218198967302\n",
            "EPOCH 393:\n",
            "  batch 2 loss: 1.0991239093998422\n",
            "  batch 4 loss: 0.9649778583983366\n",
            "  batch 6 loss: 0.735290467715855\n",
            "  batch 8 loss: 1.0193781919819846\n",
            "LOSS train 1.0193781919819846\n",
            "EPOCH 394:\n",
            "  batch 2 loss: 0.8690942221275988\n",
            "  batch 4 loss: 0.9340120835148996\n",
            "  batch 6 loss: 0.8556106441989302\n",
            "  batch 8 loss: 1.000449676037992\n",
            "LOSS train 1.000449676037992\n",
            "EPOCH 395:\n",
            "  batch 2 loss: 0.7579131932797737\n",
            "  batch 4 loss: 0.9953277777331047\n",
            "  batch 6 loss: 0.9487067930434765\n",
            "  batch 8 loss: 0.9358754106844163\n",
            "LOSS train 0.9358754106844163\n",
            "EPOCH 396:\n",
            "  batch 2 loss: 0.9759154047250979\n",
            "  batch 4 loss: 1.0521255570106465\n",
            "  batch 6 loss: 1.046886657778796\n",
            "  batch 8 loss: 0.7875471491599548\n",
            "LOSS train 0.7875471491599548\n",
            "EPOCH 397:\n",
            "  batch 2 loss: 1.0342969000745659\n",
            "  batch 4 loss: 0.9228026732971342\n",
            "  batch 6 loss: 1.0047208448360971\n",
            "  batch 8 loss: 0.9138751363106046\n",
            "LOSS train 0.9138751363106046\n",
            "EPOCH 398:\n",
            "  batch 2 loss: 0.9037177891115101\n",
            "  batch 4 loss: 0.9566696190783212\n",
            "  batch 6 loss: 0.8942022765346213\n",
            "  batch 8 loss: 0.7178329885378234\n",
            "LOSS train 0.7178329885378234\n",
            "EPOCH 399:\n",
            "  batch 2 loss: 0.834690490636729\n",
            "  batch 4 loss: 0.8822740864068075\n",
            "  batch 6 loss: 1.023688715858505\n",
            "  batch 8 loss: 0.886207171972986\n",
            "LOSS train 0.886207171972986\n",
            "EPOCH 400:\n",
            "  batch 2 loss: 0.9601773911637832\n",
            "  batch 4 loss: 0.9973562917286878\n",
            "  batch 6 loss: 1.094231658394982\n",
            "  batch 8 loss: 0.951248679134375\n",
            "LOSS train 0.951248679134375\n",
            "EPOCH 401:\n",
            "  batch 2 loss: 1.0080246304021079\n",
            "  batch 4 loss: 1.0329080224675116\n",
            "  batch 6 loss: 0.8729384732125858\n",
            "  batch 8 loss: 0.8082313775449614\n",
            "LOSS train 0.8082313775449614\n",
            "EPOCH 402:\n",
            "  batch 2 loss: 0.7869150142707796\n",
            "  batch 4 loss: 1.0364651613898355\n",
            "  batch 6 loss: 1.0162423182488391\n",
            "  batch 8 loss: 0.8007694027322256\n",
            "LOSS train 0.8007694027322256\n",
            "EPOCH 403:\n",
            "  batch 2 loss: 1.0239874259120492\n",
            "  batch 4 loss: 0.9342443790245418\n",
            "  batch 6 loss: 0.9976162660628611\n",
            "  batch 8 loss: 0.9419622793883896\n",
            "LOSS train 0.9419622793883896\n",
            "EPOCH 404:\n",
            "  batch 2 loss: 0.9134364441684538\n",
            "  batch 4 loss: 0.9232574562241083\n",
            "  batch 6 loss: 0.7611319664068732\n",
            "  batch 8 loss: 1.0187353559638517\n",
            "LOSS train 1.0187353559638517\n",
            "EPOCH 405:\n",
            "  batch 2 loss: 0.8984002974111165\n",
            "  batch 4 loss: 0.8296671635223483\n",
            "  batch 6 loss: 1.0045869233264788\n",
            "  batch 8 loss: 0.8007071528483082\n",
            "LOSS train 0.8007071528483082\n",
            "EPOCH 406:\n",
            "  batch 2 loss: 0.8421364221886605\n",
            "  batch 4 loss: 1.0157661275676606\n",
            "  batch 6 loss: 0.9472815573178078\n",
            "  batch 8 loss: 1.00264897801322\n",
            "LOSS train 1.00264897801322\n",
            "EPOCH 407:\n",
            "  batch 2 loss: 0.6977194202302344\n",
            "  batch 4 loss: 1.0312969673009489\n",
            "  batch 6 loss: 0.9983781514050007\n",
            "  batch 8 loss: 1.109854624297815\n",
            "LOSS train 1.109854624297815\n",
            "EPOCH 408:\n",
            "  batch 2 loss: 0.6407449558322964\n",
            "  batch 4 loss: 1.0320347164568442\n",
            "  batch 6 loss: 0.9944166243551822\n",
            "  batch 8 loss: 0.8344099210441054\n",
            "LOSS train 0.8344099210441054\n",
            "EPOCH 409:\n",
            "  batch 2 loss: 0.7126769524373524\n",
            "  batch 4 loss: 0.9597390411555765\n",
            "  batch 6 loss: 1.0938586613044543\n",
            "  batch 8 loss: 1.184561567441336\n",
            "LOSS train 1.184561567441336\n",
            "EPOCH 410:\n",
            "  batch 2 loss: 1.0548137608651138\n",
            "  batch 4 loss: 0.7072781113683404\n",
            "  batch 6 loss: 0.9262262684254028\n",
            "  batch 8 loss: 0.8237411610458939\n",
            "LOSS train 0.8237411610458939\n",
            "EPOCH 411:\n",
            "  batch 2 loss: 1.000839587135696\n",
            "  batch 4 loss: 0.7373857450488187\n",
            "  batch 6 loss: 0.8747485035001417\n",
            "  batch 8 loss: 0.5493394955707767\n",
            "LOSS train 0.5493394955707767\n",
            "EPOCH 412:\n",
            "  batch 2 loss: 1.05665738802444\n",
            "  batch 4 loss: 1.03386532253062\n",
            "  batch 6 loss: 0.8464062656718443\n",
            "  batch 8 loss: 0.9799422841433052\n",
            "LOSS train 0.9799422841433052\n",
            "EPOCH 413:\n",
            "  batch 2 loss: 0.6423388833444206\n",
            "  batch 4 loss: 0.9104290506502014\n",
            "  batch 6 loss: 0.965525763882765\n",
            "  batch 8 loss: 1.041922351835678\n",
            "LOSS train 1.041922351835678\n",
            "EPOCH 414:\n",
            "  batch 2 loss: 1.504296192380453\n",
            "  batch 4 loss: 1.0596946670448106\n",
            "  batch 6 loss: 0.892346421323543\n",
            "  batch 8 loss: 1.0704701603460212\n",
            "LOSS train 1.0704701603460212\n",
            "EPOCH 415:\n",
            "  batch 2 loss: 0.42458112574355483\n",
            "  batch 4 loss: 0.6550070675068885\n",
            "  batch 6 loss: 1.1969699840958978\n",
            "  batch 8 loss: 1.0192421431558643\n",
            "LOSS train 1.0192421431558643\n",
            "EPOCH 416:\n",
            "  batch 2 loss: 1.031126205363514\n",
            "  batch 4 loss: 0.7969431412228125\n",
            "  batch 6 loss: 0.8111718598924154\n",
            "  batch 8 loss: 0.9245582190319832\n",
            "LOSS train 0.9245582190319832\n",
            "EPOCH 417:\n",
            "  batch 2 loss: 0.9209630948659897\n",
            "  batch 4 loss: 0.902696837410885\n",
            "  batch 6 loss: 0.9275280163829563\n",
            "  batch 8 loss: 0.7521267424943616\n",
            "LOSS train 0.7521267424943616\n",
            "EPOCH 418:\n",
            "  batch 2 loss: 0.9166762617838491\n",
            "  batch 4 loss: 1.2291309363596357\n",
            "  batch 6 loss: 1.0796064064734658\n",
            "  batch 8 loss: 1.077996945450416\n",
            "LOSS train 1.077996945450416\n",
            "EPOCH 419:\n",
            "  batch 2 loss: 0.9068647272502788\n",
            "  batch 4 loss: 0.9059612007252802\n",
            "  batch 6 loss: 0.7375083096135497\n",
            "  batch 8 loss: 1.1046970364805144\n",
            "LOSS train 1.1046970364805144\n",
            "EPOCH 420:\n",
            "  batch 2 loss: 0.9363262662792655\n",
            "  batch 4 loss: 1.1031693757224352\n",
            "  batch 6 loss: 0.9579929162786052\n",
            "  batch 8 loss: 0.9075688298737464\n",
            "LOSS train 0.9075688298737464\n",
            "EPOCH 421:\n",
            "  batch 2 loss: 0.8724582274925468\n",
            "  batch 4 loss: 0.9816295448026572\n",
            "  batch 6 loss: 0.7873110123445497\n",
            "  batch 8 loss: 1.07836941278474\n",
            "LOSS train 1.07836941278474\n",
            "EPOCH 422:\n",
            "  batch 2 loss: 0.8537460134437767\n",
            "  batch 4 loss: 0.8123391866304926\n",
            "  batch 6 loss: 1.09816436582964\n",
            "  batch 8 loss: 1.0926162498708698\n",
            "LOSS train 1.0926162498708698\n",
            "EPOCH 423:\n",
            "  batch 2 loss: 0.81740074039519\n",
            "  batch 4 loss: 1.025647916995254\n",
            "  batch 6 loss: 1.1106478469215184\n",
            "  batch 8 loss: 1.0692808813293608\n",
            "LOSS train 1.0692808813293608\n",
            "EPOCH 424:\n",
            "  batch 2 loss: 1.1185610801220203\n",
            "  batch 4 loss: 0.6304674707947636\n",
            "  batch 6 loss: 0.7103035880196271\n",
            "  batch 8 loss: 1.1183480789873739\n",
            "LOSS train 1.1183480789873739\n",
            "EPOCH 425:\n",
            "  batch 2 loss: 1.1273008178171153\n",
            "  batch 4 loss: 0.9958770136161315\n",
            "  batch 6 loss: 0.9021836827594796\n",
            "  batch 8 loss: 0.9393613009058297\n",
            "LOSS train 0.9393613009058297\n",
            "EPOCH 426:\n",
            "  batch 2 loss: 0.9507253323247806\n",
            "  batch 4 loss: 1.0362738981211255\n",
            "  batch 6 loss: 0.9685710329895354\n",
            "  batch 8 loss: 1.515121770663899\n",
            "LOSS train 1.515121770663899\n",
            "EPOCH 427:\n",
            "  batch 2 loss: 1.0545761659929185\n",
            "  batch 4 loss: 0.7774129390489284\n",
            "  batch 6 loss: 0.9172125289878137\n",
            "  batch 8 loss: 0.8967307953235711\n",
            "LOSS train 0.8967307953235711\n",
            "EPOCH 428:\n",
            "  batch 2 loss: 0.8701129702225785\n",
            "  batch 4 loss: 0.9207463299769543\n",
            "  batch 6 loss: 0.7295575299161644\n",
            "  batch 8 loss: 0.5915030677673917\n",
            "LOSS train 0.5915030677673917\n",
            "EPOCH 429:\n",
            "  batch 2 loss: 0.9751083625295192\n",
            "  batch 4 loss: 0.8965137571572989\n",
            "  batch 6 loss: 0.6555546255168287\n",
            "  batch 8 loss: 1.0147582245857123\n",
            "LOSS train 1.0147582245857123\n",
            "EPOCH 430:\n",
            "  batch 2 loss: 0.8656750462658716\n",
            "  batch 4 loss: 1.0241242856289219\n",
            "  batch 6 loss: 0.8909035944686388\n",
            "  batch 8 loss: 0.8919905610016964\n",
            "LOSS train 0.8919905610016964\n",
            "EPOCH 431:\n",
            "  batch 2 loss: 0.8834396884240768\n",
            "  batch 4 loss: 0.7986017354206278\n",
            "  batch 6 loss: 1.1070345468838627\n",
            "  batch 8 loss: 0.9128383143799244\n",
            "LOSS train 0.9128383143799244\n",
            "EPOCH 432:\n",
            "  batch 2 loss: 1.0070635618943304\n",
            "  batch 4 loss: 0.7310284828538729\n",
            "  batch 6 loss: 0.7397775383682961\n",
            "  batch 8 loss: 0.8435483140731983\n",
            "LOSS train 0.8435483140731983\n",
            "EPOCH 433:\n",
            "  batch 2 loss: 1.0339129623283605\n",
            "  batch 4 loss: 0.7880709065569818\n",
            "  batch 6 loss: 0.995119090895644\n",
            "  batch 8 loss: 0.9031621029974555\n",
            "LOSS train 0.9031621029974555\n",
            "EPOCH 434:\n",
            "  batch 2 loss: 1.1252703116079217\n",
            "  batch 4 loss: 1.0669840067784053\n",
            "  batch 6 loss: 1.0021168717061664\n",
            "  batch 8 loss: 1.0130366720699415\n",
            "LOSS train 1.0130366720699415\n",
            "EPOCH 435:\n",
            "  batch 2 loss: 0.8719577059381851\n",
            "  batch 4 loss: 0.9475816564166681\n",
            "  batch 6 loss: 0.9684402767745335\n",
            "  batch 8 loss: 0.8338690181597993\n",
            "LOSS train 0.8338690181597993\n",
            "EPOCH 436:\n",
            "  batch 2 loss: 0.7610780403309051\n",
            "  batch 4 loss: 0.9794136580229373\n",
            "  batch 6 loss: 1.021378966853423\n",
            "  batch 8 loss: 0.8927233323998167\n",
            "LOSS train 0.8927233323998167\n",
            "EPOCH 437:\n",
            "  batch 2 loss: 1.1090163323949682\n",
            "  batch 4 loss: 0.8805163006162446\n",
            "  batch 6 loss: 1.0314068557232166\n",
            "  batch 8 loss: 0.9620027762590759\n",
            "LOSS train 0.9620027762590759\n",
            "EPOCH 438:\n",
            "  batch 2 loss: 0.5250843940537326\n",
            "  batch 4 loss: 0.7739294068298006\n",
            "  batch 6 loss: 1.0474727441815928\n",
            "  batch 8 loss: 0.8204262531478546\n",
            "LOSS train 0.8204262531478546\n",
            "EPOCH 439:\n",
            "  batch 2 loss: 0.8659594104012567\n",
            "  batch 4 loss: 0.40581118749293676\n",
            "  batch 6 loss: 0.787697289767071\n",
            "  batch 8 loss: 0.8608204232059714\n",
            "LOSS train 0.8608204232059714\n",
            "EPOCH 440:\n",
            "  batch 2 loss: 0.9906160168422258\n",
            "  batch 4 loss: 0.9933238813714373\n",
            "  batch 6 loss: 0.8050489977865471\n",
            "  batch 8 loss: 0.8995506820537194\n",
            "LOSS train 0.8995506820537194\n",
            "EPOCH 441:\n",
            "  batch 2 loss: 0.998016846205762\n",
            "  batch 4 loss: 0.8699941245818991\n",
            "  batch 6 loss: 0.8670976604080898\n",
            "  batch 8 loss: 0.9954702726144142\n",
            "LOSS train 0.9954702726144142\n",
            "EPOCH 442:\n",
            "  batch 2 loss: 0.9121965497531532\n",
            "  batch 4 loss: 0.822062147185112\n",
            "  batch 6 loss: 1.068531346146028\n",
            "  batch 8 loss: 1.0475706846012223\n",
            "LOSS train 1.0475706846012223\n",
            "EPOCH 443:\n",
            "  batch 2 loss: 0.9052371454373636\n",
            "  batch 4 loss: 0.8926302447085439\n",
            "  batch 6 loss: 0.9326915248563299\n",
            "  batch 8 loss: 1.0895196254505444\n",
            "LOSS train 1.0895196254505444\n",
            "EPOCH 444:\n",
            "  batch 2 loss: 1.0758680121248023\n",
            "  batch 4 loss: 0.9090109973326334\n",
            "  batch 6 loss: 1.0992655623308085\n",
            "  batch 8 loss: 1.0591988312787435\n",
            "LOSS train 1.0591988312787435\n",
            "EPOCH 445:\n",
            "  batch 2 loss: 1.0969217277929744\n",
            "  batch 4 loss: 1.2682359638132703\n",
            "  batch 6 loss: 0.9616083675909073\n",
            "  batch 8 loss: 0.7695200822507793\n",
            "LOSS train 0.7695200822507793\n",
            "EPOCH 446:\n",
            "  batch 2 loss: 0.6832185970463854\n",
            "  batch 4 loss: 1.0127724512730918\n",
            "  batch 6 loss: 0.8993425836990827\n",
            "  batch 8 loss: 0.8183241415515154\n",
            "LOSS train 0.8183241415515154\n",
            "EPOCH 447:\n",
            "  batch 2 loss: 0.9620882533361994\n",
            "  batch 4 loss: 0.8405786683916094\n",
            "  batch 6 loss: 1.0514421394176017\n",
            "  batch 8 loss: 0.8910796531681187\n",
            "LOSS train 0.8910796531681187\n",
            "EPOCH 448:\n",
            "  batch 2 loss: 0.6443112549074546\n",
            "  batch 4 loss: 0.6805502667179628\n",
            "  batch 6 loss: 0.9071449219106241\n",
            "  batch 8 loss: 0.9336949479117231\n",
            "LOSS train 0.9336949479117231\n",
            "EPOCH 449:\n",
            "  batch 2 loss: 0.9176346177811971\n",
            "  batch 4 loss: 1.1358260918859644\n",
            "  batch 6 loss: 0.9824466350541528\n",
            "  batch 8 loss: 1.0557842428599997\n",
            "LOSS train 1.0557842428599997\n",
            "EPOCH 450:\n",
            "  batch 2 loss: 0.8599596335772024\n",
            "  batch 4 loss: 0.9898645773425075\n",
            "  batch 6 loss: 1.1387104178788443\n",
            "  batch 8 loss: 0.8143153890253549\n",
            "LOSS train 0.8143153890253549\n",
            "EPOCH 451:\n",
            "  batch 2 loss: 1.0241422038113686\n",
            "  batch 4 loss: 1.07984148725455\n",
            "  batch 6 loss: 1.1075192789645352\n",
            "  batch 8 loss: 0.766806547193128\n",
            "LOSS train 0.766806547193128\n",
            "EPOCH 452:\n",
            "  batch 2 loss: 0.8241890205944569\n",
            "  batch 4 loss: 0.826112225100477\n",
            "  batch 6 loss: 0.9292117377042562\n",
            "  batch 8 loss: 1.1329281305085406\n",
            "LOSS train 1.1329281305085406\n",
            "EPOCH 453:\n",
            "  batch 2 loss: 1.0399531847052996\n",
            "  batch 4 loss: 1.0930742799759887\n",
            "  batch 6 loss: 0.8027946126459965\n",
            "  batch 8 loss: 0.9279133953944964\n",
            "LOSS train 0.9279133953944964\n",
            "EPOCH 454:\n",
            "  batch 2 loss: 1.0861311170204184\n",
            "  batch 4 loss: 0.8213015821367061\n",
            "  batch 6 loss: 0.8884372315994803\n",
            "  batch 8 loss: 0.9606924623039006\n",
            "LOSS train 0.9606924623039006\n",
            "EPOCH 455:\n",
            "  batch 2 loss: 1.051179933340586\n",
            "  batch 4 loss: 0.9280847180064783\n",
            "  batch 6 loss: 0.9347072335586599\n",
            "  batch 8 loss: 1.0179860173475062\n",
            "LOSS train 1.0179860173475062\n",
            "EPOCH 456:\n",
            "  batch 2 loss: 1.06121444094375\n",
            "  batch 4 loss: 0.8876068916000643\n",
            "  batch 6 loss: 1.069367397784839\n",
            "  batch 8 loss: 0.8218386102372859\n",
            "LOSS train 0.8218386102372859\n",
            "EPOCH 457:\n",
            "  batch 2 loss: 0.7425259194753551\n",
            "  batch 4 loss: 0.828040314206905\n",
            "  batch 6 loss: 1.0327312027852507\n",
            "  batch 8 loss: 0.5770050881846874\n",
            "LOSS train 0.5770050881846874\n",
            "EPOCH 458:\n",
            "  batch 2 loss: 1.0113866984286992\n",
            "  batch 4 loss: 0.7553726446632143\n",
            "  batch 6 loss: 0.5850753153577684\n",
            "  batch 8 loss: 0.8719430907171519\n",
            "LOSS train 0.8719430907171519\n",
            "EPOCH 459:\n",
            "  batch 2 loss: 0.7726314706805666\n",
            "  batch 4 loss: 0.9883972811064068\n",
            "  batch 6 loss: 0.9306948497325584\n",
            "  batch 8 loss: 0.9865602145183883\n",
            "LOSS train 0.9865602145183883\n",
            "EPOCH 460:\n",
            "  batch 2 loss: 0.8759178952277358\n",
            "  batch 4 loss: 0.9851144595995676\n",
            "  batch 6 loss: 0.7714407437680824\n",
            "  batch 8 loss: 1.0106623410357902\n",
            "LOSS train 1.0106623410357902\n",
            "EPOCH 461:\n",
            "  batch 2 loss: 0.8357583738315948\n",
            "  batch 4 loss: 1.0651047171280092\n",
            "  batch 6 loss: 0.9581062514701214\n",
            "  batch 8 loss: 0.6230631423382461\n",
            "LOSS train 0.6230631423382461\n",
            "EPOCH 462:\n",
            "  batch 2 loss: 0.8281292131769386\n",
            "  batch 4 loss: 0.7699924519314016\n",
            "  batch 6 loss: 0.8473906883808857\n",
            "  batch 8 loss: 1.037147366013587\n",
            "LOSS train 1.037147366013587\n",
            "EPOCH 463:\n",
            "  batch 2 loss: 0.5482008758884556\n",
            "  batch 4 loss: 0.9026739469127405\n",
            "  batch 6 loss: 0.9732211576202797\n",
            "  batch 8 loss: 1.1930317145117904\n",
            "LOSS train 1.1930317145117904\n",
            "EPOCH 464:\n",
            "  batch 2 loss: 0.832590137871468\n",
            "  batch 4 loss: 0.8253336236212937\n",
            "  batch 6 loss: 0.8858991977673163\n",
            "  batch 8 loss: 0.912614366904017\n",
            "LOSS train 0.912614366904017\n",
            "EPOCH 465:\n",
            "  batch 2 loss: 0.7750747015781666\n",
            "  batch 4 loss: 1.1634917768547066\n",
            "  batch 6 loss: 1.3559298544508331\n",
            "  batch 8 loss: 0.7933017120237218\n",
            "LOSS train 0.7933017120237218\n",
            "EPOCH 466:\n",
            "  batch 2 loss: 1.0640430340317517\n",
            "  batch 4 loss: 1.0433991678811259\n",
            "  batch 6 loss: 0.8778814734916973\n",
            "  batch 8 loss: 0.7478427062225195\n",
            "LOSS train 0.7478427062225195\n",
            "EPOCH 467:\n",
            "  batch 2 loss: 0.8634831781595247\n",
            "  batch 4 loss: 0.7914320282320291\n",
            "  batch 6 loss: 0.8028205616300711\n",
            "  batch 8 loss: 1.0815358677435198\n",
            "LOSS train 1.0815358677435198\n",
            "EPOCH 468:\n",
            "  batch 2 loss: 0.7910216832560959\n",
            "  batch 4 loss: 0.7942226034051627\n",
            "  batch 6 loss: 1.4331766129124244\n",
            "  batch 8 loss: 1.012336908446419\n",
            "LOSS train 1.012336908446419\n",
            "EPOCH 469:\n",
            "  batch 2 loss: 1.0858642976964195\n",
            "  batch 4 loss: 1.0525275680379114\n",
            "  batch 6 loss: 0.7983560862309016\n",
            "  batch 8 loss: 0.8797814728995419\n",
            "LOSS train 0.8797814728995419\n",
            "EPOCH 470:\n",
            "  batch 2 loss: 0.6877325129683401\n",
            "  batch 4 loss: 0.9032737179674446\n",
            "  batch 6 loss: 1.1979381376703329\n",
            "  batch 8 loss: 0.9508655716259256\n",
            "LOSS train 0.9508655716259256\n",
            "EPOCH 471:\n",
            "  batch 2 loss: 0.9497292122156273\n",
            "  batch 4 loss: 0.792115286667013\n",
            "  batch 6 loss: 0.771001424685991\n",
            "  batch 8 loss: 0.9068397232710644\n",
            "LOSS train 0.9068397232710644\n",
            "EPOCH 472:\n",
            "  batch 2 loss: 1.0274753344984635\n",
            "  batch 4 loss: 0.6428825317974171\n",
            "  batch 6 loss: 1.0504147121728584\n",
            "  batch 8 loss: 0.9249113356429868\n",
            "LOSS train 0.9249113356429868\n",
            "EPOCH 473:\n",
            "  batch 2 loss: 0.767059313638838\n",
            "  batch 4 loss: 0.6970207966713058\n",
            "  batch 6 loss: 0.855335135441635\n",
            "  batch 8 loss: 0.9092445800792576\n",
            "LOSS train 0.9092445800792576\n",
            "EPOCH 474:\n",
            "  batch 2 loss: 0.975460288005298\n",
            "  batch 4 loss: 0.9288307140636443\n",
            "  batch 6 loss: 2.166877073439383\n",
            "  batch 8 loss: 1.0068459187032066\n",
            "LOSS train 1.0068459187032066\n",
            "EPOCH 475:\n",
            "  batch 2 loss: 1.2134857985465146\n",
            "  batch 4 loss: 0.8529751912838014\n",
            "  batch 6 loss: 0.6517455350631449\n",
            "  batch 8 loss: 0.6262502129657134\n",
            "LOSS train 0.6262502129657134\n",
            "EPOCH 476:\n",
            "  batch 2 loss: 0.9042163540305569\n",
            "  batch 4 loss: 0.6908275768188019\n",
            "  batch 6 loss: 0.954847727743112\n",
            "  batch 8 loss: 1.1950953530042905\n",
            "LOSS train 1.1950953530042905\n",
            "EPOCH 477:\n",
            "  batch 2 loss: 1.0499851999211254\n",
            "  batch 4 loss: 1.0532528938395336\n",
            "  batch 6 loss: 0.9864483147155887\n",
            "  batch 8 loss: 0.7623653929283516\n",
            "LOSS train 0.7623653929283516\n",
            "EPOCH 478:\n",
            "  batch 2 loss: 0.9338088641887847\n",
            "  batch 4 loss: 1.0663691672356423\n",
            "  batch 6 loss: 0.6680988448131994\n",
            "  batch 8 loss: 0.8427118209382338\n",
            "LOSS train 0.8427118209382338\n",
            "EPOCH 479:\n",
            "  batch 2 loss: 0.7260179309565055\n",
            "  batch 4 loss: 0.8349914028005407\n",
            "  batch 6 loss: 0.8908960934158381\n",
            "  batch 8 loss: 0.9814795515782444\n",
            "LOSS train 0.9814795515782444\n",
            "EPOCH 480:\n",
            "  batch 2 loss: 0.6646801662660295\n",
            "  batch 4 loss: 0.9083683529772002\n",
            "  batch 6 loss: 1.149934981359782\n",
            "  batch 8 loss: 0.6293321846282074\n",
            "LOSS train 0.6293321846282074\n",
            "EPOCH 481:\n",
            "  batch 2 loss: 0.9031546626498832\n",
            "  batch 4 loss: 0.9760066583424727\n",
            "  batch 6 loss: 0.8435216232195375\n",
            "  batch 8 loss: 1.0753508657552078\n",
            "LOSS train 1.0753508657552078\n",
            "EPOCH 482:\n",
            "  batch 2 loss: 0.8257806345767056\n",
            "  batch 4 loss: 0.9799293064662209\n",
            "  batch 6 loss: 0.9887423139752657\n",
            "  batch 8 loss: 0.8307518239818963\n",
            "LOSS train 0.8307518239818963\n",
            "EPOCH 483:\n",
            "  batch 2 loss: 0.9465507112361711\n",
            "  batch 4 loss: 1.0172294775293338\n",
            "  batch 6 loss: 0.8643057041166604\n",
            "  batch 8 loss: 1.0865620456612792\n",
            "LOSS train 1.0865620456612792\n",
            "EPOCH 484:\n",
            "  batch 2 loss: 0.9885908029462847\n",
            "  batch 4 loss: 0.8194948731820708\n",
            "  batch 6 loss: 0.8070643904872723\n",
            "  batch 8 loss: 0.8946713518789357\n",
            "LOSS train 0.8946713518789357\n",
            "EPOCH 485:\n",
            "  batch 2 loss: 0.931457622760473\n",
            "  batch 4 loss: 0.7386176182566577\n",
            "  batch 6 loss: 0.9177780441889826\n",
            "  batch 8 loss: 0.8157302611851549\n",
            "LOSS train 0.8157302611851549\n",
            "EPOCH 486:\n",
            "  batch 2 loss: 0.8060768335168744\n",
            "  batch 4 loss: 1.1268625523568292\n",
            "  batch 6 loss: 0.7662253232666256\n",
            "  batch 8 loss: 0.9754662796639395\n",
            "LOSS train 0.9754662796639395\n",
            "EPOCH 487:\n",
            "  batch 2 loss: 0.757381429303701\n",
            "  batch 4 loss: 0.8344946820668997\n",
            "  batch 6 loss: 0.6429894874141299\n",
            "  batch 8 loss: 0.9360280445302409\n",
            "LOSS train 0.9360280445302409\n",
            "EPOCH 488:\n",
            "  batch 2 loss: 0.6949325004473464\n",
            "  batch 4 loss: 1.1272981584375206\n",
            "  batch 6 loss: 1.1019882216510586\n",
            "  batch 8 loss: 0.9945654494464604\n",
            "LOSS train 0.9945654494464604\n",
            "EPOCH 489:\n",
            "  batch 2 loss: 0.6236733341723366\n",
            "  batch 4 loss: 0.8186027947653522\n",
            "  batch 6 loss: 0.8664037143702988\n",
            "  batch 8 loss: 0.9013019560728979\n",
            "LOSS train 0.9013019560728979\n",
            "EPOCH 490:\n",
            "  batch 2 loss: 0.6830730988931664\n",
            "  batch 4 loss: 0.8431982104291148\n",
            "  batch 6 loss: 0.9901116077211967\n",
            "  batch 8 loss: 0.7544622343768459\n",
            "LOSS train 0.7544622343768459\n",
            "EPOCH 491:\n",
            "  batch 2 loss: 0.9035755278576351\n",
            "  batch 4 loss: 0.9106000912004715\n",
            "  batch 6 loss: 0.7641484564933003\n",
            "  batch 8 loss: 0.8751719556904276\n",
            "LOSS train 0.8751719556904276\n",
            "EPOCH 492:\n",
            "  batch 2 loss: 1.065788689166589\n",
            "  batch 4 loss: 0.5747140748697337\n",
            "  batch 6 loss: 1.0099308774443738\n",
            "  batch 8 loss: 0.5493372554726269\n",
            "LOSS train 0.5493372554726269\n",
            "EPOCH 493:\n",
            "  batch 2 loss: 0.843049840264688\n",
            "  batch 4 loss: 1.1122886681293334\n",
            "  batch 6 loss: 0.8715601311997941\n",
            "  batch 8 loss: 0.7948374846019499\n",
            "LOSS train 0.7948374846019499\n",
            "EPOCH 494:\n",
            "  batch 2 loss: 0.6704623181804561\n",
            "  batch 4 loss: 0.6915819676809349\n",
            "  batch 6 loss: 0.6448153469317645\n",
            "  batch 8 loss: 1.1200545210323685\n",
            "LOSS train 1.1200545210323685\n",
            "EPOCH 495:\n",
            "  batch 2 loss: 0.7937749238232306\n",
            "  batch 4 loss: 0.3587752683429679\n",
            "  batch 6 loss: 0.7496238276981526\n",
            "  batch 8 loss: 1.196953281150503\n",
            "LOSS train 1.196953281150503\n",
            "EPOCH 496:\n",
            "  batch 2 loss: 0.8132617971826115\n",
            "  batch 4 loss: 0.894758291471755\n",
            "  batch 6 loss: 0.8339867607090146\n",
            "  batch 8 loss: 1.0980291829507838\n",
            "LOSS train 1.0980291829507838\n",
            "EPOCH 497:\n",
            "  batch 2 loss: 0.954278343373755\n",
            "  batch 4 loss: 0.8755657013846071\n",
            "  batch 6 loss: 0.8618660237208557\n",
            "  batch 8 loss: 0.8743926324741815\n",
            "LOSS train 0.8743926324741815\n",
            "EPOCH 498:\n",
            "  batch 2 loss: 0.8301269600837984\n",
            "  batch 4 loss: 1.0549502555419692\n",
            "  batch 6 loss: 0.5716518459097376\n",
            "  batch 8 loss: 0.6822815956145449\n",
            "LOSS train 0.6822815956145449\n",
            "EPOCH 499:\n",
            "  batch 2 loss: 0.7248698381596814\n",
            "  batch 4 loss: 0.728956096263671\n",
            "  batch 6 loss: 0.8616453842994837\n",
            "  batch 8 loss: 0.7854206708450774\n",
            "LOSS train 0.7854206708450774\n",
            "EPOCH 500:\n",
            "  batch 2 loss: 0.7071285763561183\n",
            "  batch 4 loss: 1.066576183385512\n",
            "  batch 6 loss: 0.6882410149314401\n",
            "  batch 8 loss: 1.1024580586892898\n",
            "LOSS train 1.1024580586892898\n",
            "EPOCH 501:\n",
            "  batch 2 loss: 0.9202945785049872\n",
            "  batch 4 loss: 1.0326522181193751\n",
            "  batch 6 loss: 1.06503799416595\n",
            "  batch 8 loss: 0.6024406894677682\n",
            "LOSS train 0.6024406894677682\n",
            "EPOCH 502:\n",
            "  batch 2 loss: 0.9539971644614352\n",
            "  batch 4 loss: 0.6827292073714495\n",
            "  batch 6 loss: 1.016325631731641\n",
            "  batch 8 loss: 1.1583309502467247\n",
            "LOSS train 1.1583309502467247\n",
            "EPOCH 503:\n",
            "  batch 2 loss: 1.1792087381912504\n",
            "  batch 4 loss: 1.087743437966829\n",
            "  batch 6 loss: 0.8302610596907583\n",
            "  batch 8 loss: 1.2549228050201071\n",
            "LOSS train 1.2549228050201071\n",
            "EPOCH 504:\n",
            "  batch 2 loss: 1.0660053033997683\n",
            "  batch 4 loss: 0.9050978704062469\n",
            "  batch 6 loss: 0.8744246296899102\n",
            "  batch 8 loss: 0.8567792256710531\n",
            "LOSS train 0.8567792256710531\n",
            "EPOCH 505:\n",
            "  batch 2 loss: 0.7818525364903286\n",
            "  batch 4 loss: 0.6275065883895108\n",
            "  batch 6 loss: 0.864330151944136\n",
            "  batch 8 loss: 0.9642078847710065\n",
            "LOSS train 0.9642078847710065\n",
            "EPOCH 506:\n",
            "  batch 2 loss: 0.5729681264247461\n",
            "  batch 4 loss: 1.0512354829403918\n",
            "  batch 6 loss: 0.7420806169168883\n",
            "  batch 8 loss: 0.9615473770978241\n",
            "LOSS train 0.9615473770978241\n",
            "EPOCH 507:\n",
            "  batch 2 loss: 0.802276719840955\n",
            "  batch 4 loss: 1.0320455363616348\n",
            "  batch 6 loss: 1.141188317839688\n",
            "  batch 8 loss: 0.7455675539750497\n",
            "LOSS train 0.7455675539750497\n",
            "EPOCH 508:\n",
            "  batch 2 loss: 0.8507220176190995\n",
            "  batch 4 loss: 0.9743840252985403\n",
            "  batch 6 loss: 0.7892655216477592\n",
            "  batch 8 loss: 0.8453271432556481\n",
            "LOSS train 0.8453271432556481\n",
            "EPOCH 509:\n",
            "  batch 2 loss: 1.053695747688589\n",
            "  batch 4 loss: 0.5734811340559716\n",
            "  batch 6 loss: 1.1262365731143742\n",
            "  batch 8 loss: 0.8034582573523972\n",
            "LOSS train 0.8034582573523972\n",
            "EPOCH 510:\n",
            "  batch 2 loss: 0.7497494693657694\n",
            "  batch 4 loss: 0.6942445930796564\n",
            "  batch 6 loss: 0.3684429981007371\n",
            "  batch 8 loss: 0.699539068520475\n",
            "LOSS train 0.699539068520475\n",
            "EPOCH 511:\n",
            "  batch 2 loss: 0.46997361637813945\n",
            "  batch 4 loss: 0.7833397101896697\n",
            "  batch 6 loss: 0.7189074889952037\n",
            "  batch 8 loss: 0.9238398007392061\n",
            "LOSS train 0.9238398007392061\n",
            "EPOCH 512:\n",
            "  batch 2 loss: 0.6393863243445153\n",
            "  batch 4 loss: 0.6626635273177381\n",
            "  batch 6 loss: 0.7667205680775458\n",
            "  batch 8 loss: 0.9945465300252533\n",
            "LOSS train 0.9945465300252533\n",
            "EPOCH 513:\n",
            "  batch 2 loss: 0.9161638135653406\n",
            "  batch 4 loss: 0.9555550225005538\n",
            "  batch 6 loss: 1.1390890028828546\n",
            "  batch 8 loss: 1.0707517151642931\n",
            "LOSS train 1.0707517151642931\n",
            "EPOCH 514:\n",
            "  batch 2 loss: 0.8298540265356253\n",
            "  batch 4 loss: 1.2148962301299062\n",
            "  batch 6 loss: 0.8490918514058814\n",
            "  batch 8 loss: 1.1643791130930516\n",
            "LOSS train 1.1643791130930516\n",
            "EPOCH 515:\n",
            "  batch 2 loss: 0.9836925154292089\n",
            "  batch 4 loss: 0.4535884340783175\n",
            "  batch 6 loss: 1.003373483115448\n",
            "  batch 8 loss: 0.9539373602848664\n",
            "LOSS train 0.9539373602848664\n",
            "EPOCH 516:\n",
            "  batch 2 loss: 0.8711843242828197\n",
            "  batch 4 loss: 1.149183241601572\n",
            "  batch 6 loss: 0.8224086315765342\n",
            "  batch 8 loss: 1.0503489284339038\n",
            "LOSS train 1.0503489284339038\n",
            "EPOCH 517:\n",
            "  batch 2 loss: 0.7222137883715632\n",
            "  batch 4 loss: 0.7879375506782497\n",
            "  batch 6 loss: 0.665049116763264\n",
            "  batch 8 loss: 1.0262270157304596\n",
            "LOSS train 1.0262270157304596\n",
            "EPOCH 518:\n",
            "  batch 2 loss: 1.1460677739426077\n",
            "  batch 4 loss: 0.6799348309128546\n",
            "  batch 6 loss: 1.2856404035039273\n",
            "  batch 8 loss: 1.0067015202956469\n",
            "LOSS train 1.0067015202956469\n",
            "EPOCH 519:\n",
            "  batch 2 loss: 0.8066389720494618\n",
            "  batch 4 loss: 0.8726928083025031\n",
            "  batch 6 loss: 0.7493108932285226\n",
            "  batch 8 loss: 0.9462675668984007\n",
            "LOSS train 0.9462675668984007\n",
            "EPOCH 520:\n",
            "  batch 2 loss: 0.8027444898505443\n",
            "  batch 4 loss: 0.9538092202114875\n",
            "  batch 6 loss: 1.0014276804589866\n",
            "  batch 8 loss: 0.8188384968644421\n",
            "LOSS train 0.8188384968644421\n",
            "EPOCH 521:\n",
            "  batch 2 loss: 0.561573391648485\n",
            "  batch 4 loss: 0.7678343928822318\n",
            "  batch 6 loss: 0.959273364612046\n",
            "  batch 8 loss: 0.8388096236136764\n",
            "LOSS train 0.8388096236136764\n",
            "EPOCH 522:\n",
            "  batch 2 loss: 0.9796987596600744\n",
            "  batch 4 loss: 1.0713622961922138\n",
            "  batch 6 loss: 0.6667239109182093\n",
            "  batch 8 loss: 0.7226171639541084\n",
            "LOSS train 0.7226171639541084\n",
            "EPOCH 523:\n",
            "  batch 2 loss: 0.9094439732497921\n",
            "  batch 4 loss: 0.991980702459517\n",
            "  batch 6 loss: 0.9099710493993226\n",
            "  batch 8 loss: 1.0803189317060935\n",
            "LOSS train 1.0803189317060935\n",
            "EPOCH 524:\n",
            "  batch 2 loss: 0.9917185715037364\n",
            "  batch 4 loss: 1.0892169293742298\n",
            "  batch 6 loss: 0.7487272534538456\n",
            "  batch 8 loss: 0.9937270846645345\n",
            "LOSS train 0.9937270846645345\n",
            "EPOCH 525:\n",
            "  batch 2 loss: 1.1537087467750078\n",
            "  batch 4 loss: 0.6197621581886752\n",
            "  batch 6 loss: 0.9873361168129338\n",
            "  batch 8 loss: 1.090152363837439\n",
            "LOSS train 1.090152363837439\n",
            "EPOCH 526:\n",
            "  batch 2 loss: 1.1708463407224285\n",
            "  batch 4 loss: 1.221796255816225\n",
            "  batch 6 loss: 1.0510343060388656\n",
            "  batch 8 loss: 1.1392102150531662\n",
            "LOSS train 1.1392102150531662\n",
            "EPOCH 527:\n",
            "  batch 2 loss: 1.0094343817862907\n",
            "  batch 4 loss: 0.5150387504001808\n",
            "  batch 6 loss: 0.8745255904075286\n",
            "  batch 8 loss: 0.8544913600080147\n",
            "LOSS train 0.8544913600080147\n",
            "EPOCH 528:\n",
            "  batch 2 loss: 0.9403351327969467\n",
            "  batch 4 loss: 1.086471747867575\n",
            "  batch 6 loss: 1.1782729760443122\n",
            "  batch 8 loss: 0.9332818509580886\n",
            "LOSS train 0.9332818509580886\n",
            "EPOCH 529:\n",
            "  batch 2 loss: 1.0899858975059302\n",
            "  batch 4 loss: 1.0814228589495078\n",
            "  batch 6 loss: 0.7709158634455208\n",
            "  batch 8 loss: 0.8155207631590069\n",
            "LOSS train 0.8155207631590069\n",
            "EPOCH 530:\n",
            "  batch 2 loss: 0.6290545558811667\n",
            "  batch 4 loss: 0.7211911874062228\n",
            "  batch 6 loss: 0.8202642287475683\n",
            "  batch 8 loss: 1.013476520056824\n",
            "LOSS train 1.013476520056824\n",
            "EPOCH 531:\n",
            "  batch 2 loss: 1.0030029097273307\n",
            "  batch 4 loss: 0.7773779520152404\n",
            "  batch 6 loss: 0.8225444469442613\n",
            "  batch 8 loss: 0.7981862950194479\n",
            "LOSS train 0.7981862950194479\n",
            "EPOCH 532:\n",
            "  batch 2 loss: 0.6179225663900036\n",
            "  batch 4 loss: 0.6330842810481214\n",
            "  batch 6 loss: 0.708262474800007\n",
            "  batch 8 loss: 0.6464132708968214\n",
            "LOSS train 0.6464132708968214\n",
            "EPOCH 533:\n",
            "  batch 2 loss: 0.805811283123795\n",
            "  batch 4 loss: 0.8076518996326795\n",
            "  batch 6 loss: 0.5404342235938472\n",
            "  batch 8 loss: 0.7729484522259149\n",
            "LOSS train 0.7729484522259149\n",
            "EPOCH 534:\n",
            "  batch 2 loss: 0.8200139346163708\n",
            "  batch 4 loss: 1.0303144840519414\n",
            "  batch 6 loss: 0.6476969022426596\n",
            "  batch 8 loss: 0.9311010963525428\n",
            "LOSS train 0.9311010963525428\n",
            "EPOCH 535:\n",
            "  batch 2 loss: 1.0016247367203779\n",
            "  batch 4 loss: 0.9708198945668398\n",
            "  batch 6 loss: 0.6154508499666561\n",
            "  batch 8 loss: 0.8445001768737537\n",
            "LOSS train 0.8445001768737537\n",
            "EPOCH 536:\n",
            "  batch 2 loss: 0.5084293581801904\n",
            "  batch 4 loss: 0.7326055248135306\n",
            "  batch 6 loss: 0.7037288705418927\n",
            "  batch 8 loss: 0.8919814741680386\n",
            "LOSS train 0.8919814741680386\n",
            "EPOCH 537:\n",
            "  batch 2 loss: 1.1549100165206125\n",
            "  batch 4 loss: 1.2242303314952043\n",
            "  batch 6 loss: 1.3267498231222081\n",
            "  batch 8 loss: 1.069612181908799\n",
            "LOSS train 1.069612181908799\n",
            "EPOCH 538:\n",
            "  batch 2 loss: 1.0953502475434567\n",
            "  batch 4 loss: 0.8099184495048343\n",
            "  batch 6 loss: 0.7402471152068357\n",
            "  batch 8 loss: 1.119250884359415\n",
            "LOSS train 1.119250884359415\n",
            "EPOCH 539:\n",
            "  batch 2 loss: 0.6297347473483031\n",
            "  batch 4 loss: 0.6037229837110258\n",
            "  batch 6 loss: 0.8852094495805625\n",
            "  batch 8 loss: 0.9191108081274434\n",
            "LOSS train 0.9191108081274434\n",
            "EPOCH 540:\n",
            "  batch 2 loss: 0.8268048873162807\n",
            "  batch 4 loss: 0.798268378311731\n",
            "  batch 6 loss: 0.5474922421833688\n",
            "  batch 8 loss: 0.32490805027168923\n",
            "LOSS train 0.32490805027168923\n",
            "EPOCH 541:\n",
            "  batch 2 loss: 0.918874348522214\n",
            "  batch 4 loss: 0.9832542165060584\n",
            "  batch 6 loss: 1.4628693277611176\n",
            "  batch 8 loss: 0.7485071531732885\n",
            "LOSS train 0.7485071531732885\n",
            "EPOCH 542:\n",
            "  batch 2 loss: 0.6463044986693309\n",
            "  batch 4 loss: 0.5874553359691754\n",
            "  batch 6 loss: 0.972695294692462\n",
            "  batch 8 loss: 1.05170145495627\n",
            "LOSS train 1.05170145495627\n",
            "EPOCH 543:\n",
            "  batch 2 loss: 0.6081358208794059\n",
            "  batch 4 loss: 1.0541237638992977\n",
            "  batch 6 loss: 0.8095223467970455\n",
            "  batch 8 loss: 0.6390529095048102\n",
            "LOSS train 0.6390529095048102\n",
            "EPOCH 544:\n",
            "  batch 2 loss: 0.6531708012397104\n",
            "  batch 4 loss: 0.860806838320687\n",
            "  batch 6 loss: 0.5493414025604049\n",
            "  batch 8 loss: 0.8749736380765036\n",
            "LOSS train 0.8749736380765036\n",
            "EPOCH 545:\n",
            "  batch 2 loss: 1.1295655089958871\n",
            "  batch 4 loss: 0.9864679282800769\n",
            "  batch 6 loss: 1.2069154641392719\n",
            "  batch 8 loss: 0.631732783033164\n",
            "LOSS train 0.631732783033164\n",
            "EPOCH 546:\n",
            "  batch 2 loss: 0.7821793499597182\n",
            "  batch 4 loss: 0.9049108220268746\n",
            "  batch 6 loss: 1.0130562630212487\n",
            "  batch 8 loss: 0.9054170533464139\n",
            "LOSS train 0.9054170533464139\n",
            "EPOCH 547:\n",
            "  batch 2 loss: 0.8501213819428666\n",
            "  batch 4 loss: 0.7934495954317983\n",
            "  batch 6 loss: 1.116553795035716\n",
            "  batch 8 loss: 0.7416038962177418\n",
            "LOSS train 0.7416038962177418\n",
            "EPOCH 548:\n",
            "  batch 2 loss: 1.1529320741741385\n",
            "  batch 4 loss: 0.9352463614011174\n",
            "  batch 6 loss: 0.8867459376614899\n",
            "  batch 8 loss: 1.0290269047314564\n",
            "LOSS train 1.0290269047314564\n",
            "EPOCH 549:\n",
            "  batch 2 loss: 1.0288025524323303\n",
            "  batch 4 loss: 1.1088394597647002\n",
            "  batch 6 loss: 0.5852201067772145\n",
            "  batch 8 loss: 0.7621219135540399\n",
            "LOSS train 0.7621219135540399\n",
            "EPOCH 550:\n",
            "  batch 2 loss: 0.9743440123961759\n",
            "  batch 4 loss: 0.9954493326558393\n",
            "  batch 6 loss: 0.6053230162784108\n",
            "  batch 8 loss: 0.8621532346483762\n",
            "LOSS train 0.8621532346483762\n",
            "EPOCH 551:\n",
            "  batch 2 loss: 0.7311832148698484\n",
            "  batch 4 loss: 0.8257601219792328\n",
            "  batch 6 loss: 0.8823987183102926\n",
            "  batch 8 loss: 0.801504352774308\n",
            "LOSS train 0.801504352774308\n",
            "EPOCH 552:\n",
            "  batch 2 loss: 0.9455893780718264\n",
            "  batch 4 loss: 0.9172906995120285\n",
            "  batch 6 loss: 0.877599723808161\n",
            "  batch 8 loss: 0.9403316480072432\n",
            "LOSS train 0.9403316480072432\n",
            "EPOCH 553:\n",
            "  batch 2 loss: 0.9615861068784708\n",
            "  batch 4 loss: 0.665980491928928\n",
            "  batch 6 loss: 0.49811063355653756\n",
            "  batch 8 loss: 0.9706634047238691\n",
            "LOSS train 0.9706634047238691\n",
            "EPOCH 554:\n",
            "  batch 2 loss: 0.9858726201502848\n",
            "  batch 4 loss: 0.7669282540132275\n",
            "  batch 6 loss: 0.5957547325599264\n",
            "  batch 8 loss: 0.7458618822572987\n",
            "LOSS train 0.7458618822572987\n",
            "EPOCH 555:\n",
            "  batch 2 loss: 1.1997992826935697\n",
            "  batch 4 loss: 1.0449451053352345\n",
            "  batch 6 loss: 1.144643388102157\n",
            "  batch 8 loss: 1.0356839046482416\n",
            "LOSS train 1.0356839046482416\n",
            "EPOCH 556:\n",
            "  batch 2 loss: 0.9023858561053006\n",
            "  batch 4 loss: 1.0353694281814105\n",
            "  batch 6 loss: 0.7683529056999533\n",
            "  batch 8 loss: 0.9191115374026606\n",
            "LOSS train 0.9191115374026606\n",
            "EPOCH 557:\n",
            "  batch 2 loss: 0.8584867273028047\n",
            "  batch 4 loss: 0.9067662030802672\n",
            "  batch 6 loss: 0.8235098572203667\n",
            "  batch 8 loss: 1.007416918403323\n",
            "LOSS train 1.007416918403323\n",
            "EPOCH 558:\n",
            "  batch 2 loss: 0.6859071366583664\n",
            "  batch 4 loss: 1.1441227165224128\n",
            "  batch 6 loss: 1.3001063690615196\n",
            "  batch 8 loss: 0.7465991986123872\n",
            "LOSS train 0.7465991986123872\n",
            "EPOCH 559:\n",
            "  batch 2 loss: 1.1052235769437528\n",
            "  batch 4 loss: 0.9898255113699135\n",
            "  batch 6 loss: 0.5345145987043658\n",
            "  batch 8 loss: 0.9037016293840272\n",
            "LOSS train 0.9037016293840272\n",
            "EPOCH 560:\n",
            "  batch 2 loss: 2.5336038678132415\n",
            "  batch 4 loss: 0.7749682662337839\n",
            "  batch 6 loss: 0.8172871805697884\n",
            "  batch 8 loss: 1.0464725888918458\n",
            "LOSS train 1.0464725888918458\n",
            "EPOCH 561:\n",
            "  batch 2 loss: 0.889126228572587\n",
            "  batch 4 loss: 1.117018620588131\n",
            "  batch 6 loss: 1.154442937013537\n",
            "  batch 8 loss: 1.1257575123697414\n",
            "LOSS train 1.1257575123697414\n",
            "EPOCH 562:\n",
            "  batch 2 loss: 1.0261112995271273\n",
            "  batch 4 loss: 1.3320781191401716\n",
            "  batch 6 loss: 1.2494722547868045\n",
            "  batch 8 loss: 0.290291211527741\n",
            "LOSS train 0.290291211527741\n",
            "EPOCH 563:\n",
            "  batch 2 loss: 0.8434531253094735\n",
            "  batch 4 loss: 0.9439685955931647\n",
            "  batch 6 loss: 0.6423384239304395\n",
            "  batch 8 loss: 0.8865680003784403\n",
            "LOSS train 0.8865680003784403\n",
            "EPOCH 564:\n",
            "  batch 2 loss: 1.089504298119218\n",
            "  batch 4 loss: 0.7455768964214188\n",
            "  batch 6 loss: 2.0213733509775413\n",
            "  batch 8 loss: 1.0283413198192746\n",
            "LOSS train 1.0283413198192746\n",
            "EPOCH 565:\n",
            "  batch 2 loss: 0.9420227970826117\n",
            "  batch 4 loss: 0.6708273673043157\n",
            "  batch 6 loss: 0.7319264950213061\n",
            "  batch 8 loss: 1.1809811663555325\n",
            "LOSS train 1.1809811663555325\n",
            "EPOCH 566:\n",
            "  batch 2 loss: 0.8994765740778045\n",
            "  batch 4 loss: 0.874937687145563\n",
            "  batch 6 loss: 0.971008367860972\n",
            "  batch 8 loss: 1.096570512018141\n",
            "LOSS train 1.096570512018141\n",
            "EPOCH 567:\n",
            "  batch 2 loss: 0.5875093208417428\n",
            "  batch 4 loss: 1.0901040368622024\n",
            "  batch 6 loss: 0.8292775992886616\n",
            "  batch 8 loss: 0.7025397764868647\n",
            "LOSS train 0.7025397764868647\n",
            "EPOCH 568:\n",
            "  batch 2 loss: 0.9645616168951742\n",
            "  batch 4 loss: 1.0994982513411333\n",
            "  batch 6 loss: 1.0476114879654221\n",
            "  batch 8 loss: 0.9307032348827873\n",
            "LOSS train 0.9307032348827873\n",
            "EPOCH 569:\n",
            "  batch 2 loss: 0.958482646088918\n",
            "  batch 4 loss: 0.7596380322755462\n",
            "  batch 6 loss: 1.1626060620354983\n",
            "  batch 8 loss: 0.6434049018688779\n",
            "LOSS train 0.6434049018688779\n",
            "EPOCH 570:\n",
            "  batch 2 loss: 0.7489238479671261\n",
            "  batch 4 loss: 0.8685721488711691\n",
            "  batch 6 loss: 1.2091008543576571\n",
            "  batch 8 loss: 0.8882733571919988\n",
            "LOSS train 0.8882733571919988\n",
            "EPOCH 571:\n",
            "  batch 2 loss: 0.9556246871289967\n",
            "  batch 4 loss: 1.0400199787864688\n",
            "  batch 6 loss: 0.920738525562049\n",
            "  batch 8 loss: 0.729555372414914\n",
            "LOSS train 0.729555372414914\n",
            "EPOCH 572:\n",
            "  batch 2 loss: 1.0495221285526473\n",
            "  batch 4 loss: 0.9898398552386805\n",
            "  batch 6 loss: 1.1082385014542098\n",
            "  batch 8 loss: 0.7141261584165081\n",
            "LOSS train 0.7141261584165081\n",
            "EPOCH 573:\n",
            "  batch 2 loss: 0.6819853661434421\n",
            "  batch 4 loss: 0.7498205196320014\n",
            "  batch 6 loss: 1.0524244331263986\n",
            "  batch 8 loss: 0.8122868823632341\n",
            "LOSS train 0.8122868823632341\n",
            "EPOCH 574:\n",
            "  batch 2 loss: 1.3067467660126102\n",
            "  batch 4 loss: 0.44136442419493194\n",
            "  batch 6 loss: 0.6659585468214837\n",
            "  batch 8 loss: 0.7797926836958702\n",
            "LOSS train 0.7797926836958702\n",
            "EPOCH 575:\n",
            "  batch 2 loss: 1.054683197929808\n",
            "  batch 4 loss: 0.7527786822305257\n",
            "  batch 6 loss: 0.6323405661074116\n",
            "  batch 8 loss: 0.8456541983134901\n",
            "LOSS train 0.8456541983134901\n",
            "EPOCH 576:\n",
            "  batch 2 loss: 1.1510381771554232\n",
            "  batch 4 loss: 0.8959668614293976\n",
            "  batch 6 loss: 0.9204677742710843\n",
            "  batch 8 loss: 0.923476772164842\n",
            "LOSS train 0.923476772164842\n",
            "EPOCH 577:\n",
            "  batch 2 loss: 0.8321141199463129\n",
            "  batch 4 loss: 0.7040358412536842\n",
            "  batch 6 loss: 0.6404099630449256\n",
            "  batch 8 loss: 0.990910858777872\n",
            "LOSS train 0.990910858777872\n",
            "EPOCH 578:\n",
            "  batch 2 loss: 0.9052461902933888\n",
            "  batch 4 loss: 1.062936912777141\n",
            "  batch 6 loss: 0.9769181852717941\n",
            "  batch 8 loss: 0.8621915872147126\n",
            "LOSS train 0.8621915872147126\n",
            "EPOCH 579:\n",
            "  batch 2 loss: 0.9041631254129436\n",
            "  batch 4 loss: 0.9991397872793373\n",
            "  batch 6 loss: 0.9838884223304731\n",
            "  batch 8 loss: 1.1686652613075048\n",
            "LOSS train 1.1686652613075048\n",
            "EPOCH 580:\n",
            "  batch 2 loss: 0.7531051430900844\n",
            "  batch 4 loss: 0.7467391615935774\n",
            "  batch 6 loss: 0.7324216436822555\n",
            "  batch 8 loss: 0.8039786196386646\n",
            "LOSS train 0.8039786196386646\n",
            "EPOCH 581:\n",
            "  batch 2 loss: 0.9675919897082335\n",
            "  batch 4 loss: 1.0266293299088067\n",
            "  batch 6 loss: 0.687448795292255\n",
            "  batch 8 loss: 1.0860398942038394\n",
            "LOSS train 1.0860398942038394\n",
            "EPOCH 582:\n",
            "  batch 2 loss: 0.7616343096516858\n",
            "  batch 4 loss: 0.7486164331866693\n",
            "  batch 6 loss: 0.982589152939848\n",
            "  batch 8 loss: 0.5511794707152236\n",
            "LOSS train 0.5511794707152236\n",
            "EPOCH 583:\n",
            "  batch 2 loss: 0.9348050323628923\n",
            "  batch 4 loss: 1.074250275929033\n",
            "  batch 6 loss: 0.7328130910194612\n",
            "  batch 8 loss: 1.0195011692497948\n",
            "LOSS train 1.0195011692497948\n",
            "EPOCH 584:\n",
            "  batch 2 loss: 1.0575034354217425\n",
            "  batch 4 loss: 0.9761650159565018\n",
            "  batch 6 loss: 1.0524741908903716\n",
            "  batch 8 loss: 0.6964962618430656\n",
            "LOSS train 0.6964962618430656\n",
            "EPOCH 585:\n",
            "  batch 2 loss: 0.8231069081886717\n",
            "  batch 4 loss: 1.1238946981565296\n",
            "  batch 6 loss: 0.8794692974046165\n",
            "  batch 8 loss: 0.727695879871835\n",
            "LOSS train 0.727695879871835\n",
            "EPOCH 586:\n",
            "  batch 2 loss: 0.9959493606829334\n",
            "  batch 4 loss: 0.9025135531492332\n",
            "  batch 6 loss: 1.0737600102700802\n",
            "  batch 8 loss: 0.8174766004459264\n",
            "LOSS train 0.8174766004459264\n",
            "EPOCH 587:\n",
            "  batch 2 loss: 0.7759245831745668\n",
            "  batch 4 loss: 0.9034161691147199\n",
            "  batch 6 loss: 0.8311719809458877\n",
            "  batch 8 loss: 1.1358696361649747\n",
            "LOSS train 1.1358696361649747\n",
            "EPOCH 588:\n",
            "  batch 2 loss: 0.8737393483158037\n",
            "  batch 4 loss: 1.0038684775232025\n",
            "  batch 6 loss: 1.2255423279988835\n",
            "  batch 8 loss: 0.8017222799433292\n",
            "LOSS train 0.8017222799433292\n",
            "EPOCH 589:\n",
            "  batch 2 loss: 0.8624768443772364\n",
            "  batch 4 loss: 0.9148202214205986\n",
            "  batch 6 loss: 0.910119536743293\n",
            "  batch 8 loss: 0.7611234634937717\n",
            "LOSS train 0.7611234634937717\n",
            "EPOCH 590:\n",
            "  batch 2 loss: 1.046656926508752\n",
            "  batch 4 loss: 1.1057836309969664\n",
            "  batch 6 loss: 1.0651338896837155\n",
            "  batch 8 loss: 0.8522448578054574\n",
            "LOSS train 0.8522448578054574\n",
            "EPOCH 591:\n",
            "  batch 2 loss: 0.9407480804687073\n",
            "  batch 4 loss: 0.8081660659134644\n",
            "  batch 6 loss: 1.0323246731102502\n",
            "  batch 8 loss: 0.9356450548670454\n",
            "LOSS train 0.9356450548670454\n",
            "EPOCH 592:\n",
            "  batch 2 loss: 1.0641724611095151\n",
            "  batch 4 loss: 1.1677175746407165\n",
            "  batch 6 loss: 1.10858669430328\n",
            "  batch 8 loss: 1.0459334716394972\n",
            "LOSS train 1.0459334716394972\n",
            "EPOCH 593:\n",
            "  batch 2 loss: 0.846629499754467\n",
            "  batch 4 loss: 0.7841604134071708\n",
            "  batch 6 loss: 0.9581146816499858\n",
            "  batch 8 loss: 1.1492344788280744\n",
            "LOSS train 1.1492344788280744\n",
            "EPOCH 594:\n",
            "  batch 2 loss: 0.7078882997045002\n",
            "  batch 4 loss: 0.7207096046715245\n",
            "  batch 6 loss: 1.012533517410202\n",
            "  batch 8 loss: 1.0955463541963881\n",
            "LOSS train 1.0955463541963881\n",
            "EPOCH 595:\n",
            "  batch 2 loss: 0.8138783066862864\n",
            "  batch 4 loss: 1.1178846273682341\n",
            "  batch 6 loss: 0.8889837437699921\n",
            "  batch 8 loss: 0.9986213157716824\n",
            "LOSS train 0.9986213157716824\n",
            "EPOCH 596:\n",
            "  batch 2 loss: 0.9673752442690622\n",
            "  batch 4 loss: 0.922193889006523\n",
            "  batch 6 loss: 0.8872614667577908\n",
            "  batch 8 loss: 0.669429989188325\n",
            "LOSS train 0.669429989188325\n",
            "EPOCH 597:\n",
            "  batch 2 loss: 1.1393887604330735\n",
            "  batch 4 loss: 0.8611243407874629\n",
            "  batch 6 loss: 0.8660101491167491\n",
            "  batch 8 loss: 1.038239719733829\n",
            "LOSS train 1.038239719733829\n",
            "EPOCH 598:\n",
            "  batch 2 loss: 0.9098550782400493\n",
            "  batch 4 loss: 0.9973300210306433\n",
            "  batch 6 loss: 0.7422937216028287\n",
            "  batch 8 loss: 0.9254534514116445\n",
            "LOSS train 0.9254534514116445\n",
            "EPOCH 599:\n",
            "  batch 2 loss: 0.746116399498671\n",
            "  batch 4 loss: 1.1205735913277721\n",
            "  batch 6 loss: 0.7256842009979956\n",
            "  batch 8 loss: 0.868891035466256\n",
            "LOSS train 0.868891035466256\n",
            "EPOCH 600:\n",
            "  batch 2 loss: 1.0032359817245426\n",
            "  batch 4 loss: 0.7995832316466523\n",
            "  batch 6 loss: 0.8880597221452065\n",
            "  batch 8 loss: 1.0757174300941332\n",
            "LOSS train 1.0757174300941332\n",
            "EPOCH 601:\n",
            "  batch 2 loss: 1.0813832473408533\n",
            "  batch 4 loss: 0.9672213219003751\n",
            "  batch 6 loss: 0.7061202407403693\n",
            "  batch 8 loss: 0.6343637275609442\n",
            "LOSS train 0.6343637275609442\n",
            "EPOCH 602:\n",
            "  batch 2 loss: 1.1260334035137336\n",
            "  batch 4 loss: 0.7848851300278437\n",
            "  batch 6 loss: 0.7287943830697649\n",
            "  batch 8 loss: 0.9010829400367668\n",
            "LOSS train 0.9010829400367668\n",
            "EPOCH 603:\n",
            "  batch 2 loss: 0.5102369240323982\n",
            "  batch 4 loss: 0.7058153533760848\n",
            "  batch 6 loss: 1.0951923838354047\n",
            "  batch 8 loss: 0.4929570633323559\n",
            "LOSS train 0.4929570633323559\n",
            "EPOCH 604:\n",
            "  batch 2 loss: 1.1124473914578479\n",
            "  batch 4 loss: 1.3054584327442036\n",
            "  batch 6 loss: 0.9807377469997741\n",
            "  batch 8 loss: 0.6507588592826866\n",
            "LOSS train 0.6507588592826866\n",
            "EPOCH 605:\n",
            "  batch 2 loss: 0.6711574788116752\n",
            "  batch 4 loss: 1.1944843107421501\n",
            "  batch 6 loss: 0.9176941906950835\n",
            "  batch 8 loss: 0.9677977372857282\n",
            "LOSS train 0.9677977372857282\n",
            "EPOCH 606:\n",
            "  batch 2 loss: 0.9161258864737887\n",
            "  batch 4 loss: 0.6423672556912445\n",
            "  batch 6 loss: 0.9612690436513623\n",
            "  batch 8 loss: 0.9498168785153439\n",
            "LOSS train 0.9498168785153439\n",
            "EPOCH 607:\n",
            "  batch 2 loss: 0.6105719126638316\n",
            "  batch 4 loss: 1.2292890162034915\n",
            "  batch 6 loss: 0.9652546700280407\n",
            "  batch 8 loss: 1.1230758902678315\n",
            "LOSS train 1.1230758902678315\n",
            "EPOCH 608:\n",
            "  batch 2 loss: 0.8206618210338879\n",
            "  batch 4 loss: 0.8278022193735168\n",
            "  batch 6 loss: 0.6535900530481495\n",
            "  batch 8 loss: 0.6574861790509806\n",
            "LOSS train 0.6574861790509806\n",
            "EPOCH 609:\n",
            "  batch 2 loss: 0.9325923732368544\n",
            "  batch 4 loss: 0.6326561898360428\n",
            "  batch 6 loss: 0.9188426975375399\n",
            "  batch 8 loss: 0.9676873292954508\n",
            "LOSS train 0.9676873292954508\n",
            "EPOCH 610:\n",
            "  batch 2 loss: 0.7305628069527651\n",
            "  batch 4 loss: 0.542896964841603\n",
            "  batch 6 loss: 0.534727705051831\n",
            "  batch 8 loss: 1.0757572606151133\n",
            "LOSS train 1.0757572606151133\n",
            "EPOCH 611:\n",
            "  batch 2 loss: 1.0506735262130547\n",
            "  batch 4 loss: 1.0933985294036446\n",
            "  batch 6 loss: 0.7528187018667378\n",
            "  batch 8 loss: 0.8647509817679255\n",
            "LOSS train 0.8647509817679255\n",
            "EPOCH 612:\n",
            "  batch 2 loss: 0.5176996920889725\n",
            "  batch 4 loss: 0.4530914479609666\n",
            "  batch 6 loss: 1.0822508735565737\n",
            "  batch 8 loss: 0.6326897245208186\n",
            "LOSS train 0.6326897245208186\n",
            "EPOCH 613:\n",
            "  batch 2 loss: 0.8908311392333318\n",
            "  batch 4 loss: 0.5405043367101771\n",
            "  batch 6 loss: 0.9547338297639151\n",
            "  batch 8 loss: 0.9885400464120853\n",
            "LOSS train 0.9885400464120853\n",
            "EPOCH 614:\n",
            "  batch 2 loss: 0.5195961075974301\n",
            "  batch 4 loss: 0.871638008317405\n",
            "  batch 6 loss: 1.064880871337983\n",
            "  batch 8 loss: 0.8872226958173202\n",
            "LOSS train 0.8872226958173202\n",
            "EPOCH 615:\n",
            "  batch 2 loss: 0.916442157525641\n",
            "  batch 4 loss: 0.8056948705055837\n",
            "  batch 6 loss: 0.6416990446048536\n",
            "  batch 8 loss: 0.9299329044037071\n",
            "LOSS train 0.9299329044037071\n",
            "EPOCH 616:\n",
            "  batch 2 loss: 1.1312810460864704\n",
            "  batch 4 loss: 0.438498383717554\n",
            "  batch 6 loss: 0.9011031872378218\n",
            "  batch 8 loss: 0.909634073639904\n",
            "LOSS train 0.909634073639904\n",
            "EPOCH 617:\n",
            "  batch 2 loss: 1.0434952875363246\n",
            "  batch 4 loss: 1.0847751420029348\n",
            "  batch 6 loss: 0.858574403384565\n",
            "  batch 8 loss: 1.1876519145480802\n",
            "LOSS train 1.1876519145480802\n",
            "EPOCH 618:\n",
            "  batch 2 loss: 0.9144010287881098\n",
            "  batch 4 loss: 0.5232320856841172\n",
            "  batch 6 loss: 0.8793342958722686\n",
            "  batch 8 loss: 1.0676463071617697\n",
            "LOSS train 1.0676463071617697\n",
            "EPOCH 619:\n",
            "  batch 2 loss: 0.8024709038065265\n",
            "  batch 4 loss: 0.7746073732913343\n",
            "  batch 6 loss: 0.9421723752287238\n",
            "  batch 8 loss: 1.2415160451017202\n",
            "LOSS train 1.2415160451017202\n",
            "EPOCH 620:\n",
            "  batch 2 loss: 0.5988577320155417\n",
            "  batch 4 loss: 0.7058870855425178\n",
            "  batch 6 loss: 0.6441577273779023\n",
            "  batch 8 loss: 0.9992614823461605\n",
            "LOSS train 0.9992614823461605\n",
            "EPOCH 621:\n",
            "  batch 2 loss: 0.7791841023508594\n",
            "  batch 4 loss: 0.8094858381525999\n",
            "  batch 6 loss: 0.42934089419955823\n",
            "  batch 8 loss: 0.1676589358566026\n",
            "LOSS train 0.1676589358566026\n",
            "EPOCH 622:\n",
            "  batch 2 loss: 0.4917370253792764\n",
            "  batch 4 loss: 0.9951568579303824\n",
            "  batch 6 loss: 0.7675764659914615\n",
            "  batch 8 loss: 1.0022191481569975\n",
            "LOSS train 1.0022191481569975\n",
            "EPOCH 623:\n",
            "  batch 2 loss: 0.867670937236535\n",
            "  batch 4 loss: 1.0204417869696596\n",
            "  batch 6 loss: 0.945207762842923\n",
            "  batch 8 loss: 0.6466389119969538\n",
            "LOSS train 0.6466389119969538\n",
            "EPOCH 624:\n",
            "  batch 2 loss: 1.0721805493582584\n",
            "  batch 4 loss: 0.8266397893777299\n",
            "  batch 6 loss: 0.6389827230791257\n",
            "  batch 8 loss: 1.0535818759607825\n",
            "LOSS train 1.0535818759607825\n",
            "EPOCH 625:\n",
            "  batch 2 loss: 1.134178376856023\n",
            "  batch 4 loss: 0.8377482108113918\n",
            "  batch 6 loss: 1.131507783304609\n",
            "  batch 8 loss: 0.4785527128662473\n",
            "LOSS train 0.4785527128662473\n",
            "EPOCH 626:\n",
            "  batch 2 loss: 1.2055487098550395\n",
            "  batch 4 loss: 1.1016053473184186\n",
            "  batch 6 loss: 0.5503661405045099\n",
            "  batch 8 loss: 0.6919080895338572\n",
            "LOSS train 0.6919080895338572\n",
            "EPOCH 627:\n",
            "  batch 2 loss: 0.9088676116839353\n",
            "  batch 4 loss: 1.068011926196677\n",
            "  batch 6 loss: 0.9776061715783391\n",
            "  batch 8 loss: 0.9695023320257823\n",
            "LOSS train 0.9695023320257823\n",
            "EPOCH 628:\n",
            "  batch 2 loss: 0.6378607838382577\n",
            "  batch 4 loss: 0.8776273904468649\n",
            "  batch 6 loss: 0.9139829263868962\n",
            "  batch 8 loss: 0.8295668659856064\n",
            "LOSS train 0.8295668659856064\n",
            "EPOCH 629:\n",
            "  batch 2 loss: 0.701970749842123\n",
            "  batch 4 loss: 0.6959978912807379\n",
            "  batch 6 loss: 1.0305877229868388\n",
            "  batch 8 loss: 1.1756580564693726\n",
            "LOSS train 1.1756580564693726\n",
            "EPOCH 630:\n",
            "  batch 2 loss: 1.0985151588057396\n",
            "  batch 4 loss: 0.9390486809116916\n",
            "  batch 6 loss: 0.5320285915094811\n",
            "  batch 8 loss: 1.0359680059605905\n",
            "LOSS train 1.0359680059605905\n",
            "EPOCH 631:\n",
            "  batch 2 loss: 0.9301106788975932\n",
            "  batch 4 loss: 0.893709054157189\n",
            "  batch 6 loss: 0.9222772969807043\n",
            "  batch 8 loss: 0.5630836682273717\n",
            "LOSS train 0.5630836682273717\n",
            "EPOCH 632:\n",
            "  batch 2 loss: 0.7004503971106631\n",
            "  batch 4 loss: 0.5454914136900916\n",
            "  batch 6 loss: 0.779812146654708\n",
            "  batch 8 loss: 1.0421365877009\n",
            "LOSS train 1.0421365877009\n",
            "EPOCH 633:\n",
            "  batch 2 loss: 0.9525822759962371\n",
            "  batch 4 loss: 1.0666053654486645\n",
            "  batch 6 loss: 1.0109574333871063\n",
            "  batch 8 loss: 0.744283209377395\n",
            "LOSS train 0.744283209377395\n",
            "EPOCH 634:\n",
            "  batch 2 loss: 0.6835157480480845\n",
            "  batch 4 loss: 1.1115249172252955\n",
            "  batch 6 loss: 0.704370211962287\n",
            "  batch 8 loss: 1.0860037745988635\n",
            "LOSS train 1.0860037745988635\n",
            "EPOCH 635:\n",
            "  batch 2 loss: 0.9695265297825573\n",
            "  batch 4 loss: 0.9608599001166571\n",
            "  batch 6 loss: 0.7650266359748497\n",
            "  batch 8 loss: 0.9142158959568658\n",
            "LOSS train 0.9142158959568658\n",
            "EPOCH 636:\n",
            "  batch 2 loss: 0.9574179370591349\n",
            "  batch 4 loss: 0.5935675802419961\n",
            "  batch 6 loss: 1.1217431517360876\n",
            "  batch 8 loss: 0.7703810684269656\n",
            "LOSS train 0.7703810684269656\n",
            "EPOCH 637:\n",
            "  batch 2 loss: 1.0493659736392138\n",
            "  batch 4 loss: 1.0178209724439953\n",
            "  batch 6 loss: 1.075177254425122\n",
            "  batch 8 loss: 0.8847830129313804\n",
            "LOSS train 0.8847830129313804\n",
            "EPOCH 638:\n",
            "  batch 2 loss: 0.6779288479269688\n",
            "  batch 4 loss: 1.062791774823891\n",
            "  batch 6 loss: 0.8295551586364378\n",
            "  batch 8 loss: 0.7541987417981596\n",
            "LOSS train 0.7541987417981596\n",
            "EPOCH 639:\n",
            "  batch 2 loss: 1.0133790772507592\n",
            "  batch 4 loss: 1.0405853461684165\n",
            "  batch 6 loss: 0.8643510159822405\n",
            "  batch 8 loss: 0.9456070458228464\n",
            "LOSS train 0.9456070458228464\n",
            "EPOCH 640:\n",
            "  batch 2 loss: 0.8829295237934592\n",
            "  batch 4 loss: 0.7153646108018066\n",
            "  batch 6 loss: 1.0040376054169684\n",
            "  batch 8 loss: 1.0758502824841192\n",
            "LOSS train 1.0758502824841192\n",
            "EPOCH 641:\n",
            "  batch 2 loss: 1.0757493245561172\n",
            "  batch 4 loss: 0.7119531611616272\n",
            "  batch 6 loss: 0.89376662120031\n",
            "  batch 8 loss: 0.5798336904588235\n",
            "LOSS train 0.5798336904588235\n",
            "EPOCH 642:\n",
            "  batch 2 loss: 1.0350541806964142\n",
            "  batch 4 loss: 0.7263380250660835\n",
            "  batch 6 loss: 1.0635578104202972\n",
            "  batch 8 loss: 0.6768155564786587\n",
            "LOSS train 0.6768155564786587\n",
            "EPOCH 643:\n",
            "  batch 2 loss: 0.9570787818485528\n",
            "  batch 4 loss: 1.0951929378272633\n",
            "  batch 6 loss: 0.7240701461007671\n",
            "  batch 8 loss: 0.6632812407706409\n",
            "LOSS train 0.6632812407706409\n",
            "EPOCH 644:\n",
            "  batch 2 loss: 0.8630447484602009\n",
            "  batch 4 loss: 0.9707315012104633\n",
            "  batch 6 loss: 1.089420993047732\n",
            "  batch 8 loss: 1.0238951558533649\n",
            "LOSS train 1.0238951558533649\n",
            "EPOCH 645:\n",
            "  batch 2 loss: 0.46719157474561945\n",
            "  batch 4 loss: 0.8997737823742878\n",
            "  batch 6 loss: 1.0502488504118177\n",
            "  batch 8 loss: 0.9497536045659505\n",
            "LOSS train 0.9497536045659505\n",
            "EPOCH 646:\n",
            "  batch 2 loss: 0.7309174138747166\n",
            "  batch 4 loss: 0.590775180881639\n",
            "  batch 6 loss: 0.6921241007413246\n",
            "  batch 8 loss: 1.1944614060900642\n",
            "LOSS train 1.1944614060900642\n",
            "EPOCH 647:\n",
            "  batch 2 loss: 0.9869309003654007\n",
            "  batch 4 loss: 0.7157883241919958\n",
            "  batch 6 loss: 0.8361449125206832\n",
            "  batch 8 loss: 1.0248360300847057\n",
            "LOSS train 1.0248360300847057\n",
            "EPOCH 648:\n",
            "  batch 2 loss: 0.8349387440778941\n",
            "  batch 4 loss: 0.4727371366484474\n",
            "  batch 6 loss: 0.7465842730173491\n",
            "  batch 8 loss: 0.5628272868439108\n",
            "LOSS train 0.5628272868439108\n",
            "EPOCH 649:\n",
            "  batch 2 loss: 1.137209992749631\n",
            "  batch 4 loss: 1.1565204622438818\n",
            "  batch 6 loss: 0.759570536598432\n",
            "  batch 8 loss: 0.9378194856141149\n",
            "LOSS train 0.9378194856141149\n",
            "EPOCH 650:\n",
            "  batch 2 loss: 0.7723790150185847\n",
            "  batch 4 loss: 0.8097809805796226\n",
            "  batch 6 loss: 0.9841263664773527\n",
            "  batch 8 loss: 1.1326395700540257\n",
            "LOSS train 1.1326395700540257\n",
            "EPOCH 651:\n",
            "  batch 2 loss: 0.957900202737355\n",
            "  batch 4 loss: 0.6194038716632072\n",
            "  batch 6 loss: 0.7588854003584518\n",
            "  batch 8 loss: 1.1933122139793042\n",
            "LOSS train 1.1933122139793042\n",
            "EPOCH 652:\n",
            "  batch 2 loss: 0.7920656511633342\n",
            "  batch 4 loss: 0.5957807337935948\n",
            "  batch 6 loss: 0.7166462430981286\n",
            "  batch 8 loss: 1.4464375191968826\n",
            "LOSS train 1.4464375191968826\n",
            "EPOCH 653:\n",
            "  batch 2 loss: 1.4212914942365207\n",
            "  batch 4 loss: 0.7027082919702927\n",
            "  batch 6 loss: 1.189583474043355\n",
            "  batch 8 loss: 0.6607003120144195\n",
            "LOSS train 0.6607003120144195\n",
            "EPOCH 654:\n",
            "  batch 2 loss: 0.6963171388361695\n",
            "  batch 4 loss: 1.1476148906840726\n",
            "  batch 6 loss: 1.0503482933737278\n",
            "  batch 8 loss: 0.6348283357402402\n",
            "LOSS train 0.6348283357402402\n",
            "EPOCH 655:\n",
            "  batch 2 loss: 0.6198002158003738\n",
            "  batch 4 loss: 0.9566202323179793\n",
            "  batch 6 loss: 0.8210372627472643\n",
            "  batch 8 loss: 1.0241191390948996\n",
            "LOSS train 1.0241191390948996\n",
            "EPOCH 656:\n",
            "  batch 2 loss: 0.6140123787275917\n",
            "  batch 4 loss: 0.6629080190247911\n",
            "  batch 6 loss: 0.741662146138184\n",
            "  batch 8 loss: 0.766233809280633\n",
            "LOSS train 0.766233809280633\n",
            "EPOCH 657:\n",
            "  batch 2 loss: 1.104157873777504\n",
            "  batch 4 loss: 1.0903768692489022\n",
            "  batch 6 loss: 0.9089169683805376\n",
            "  batch 8 loss: 0.8278513254421187\n",
            "LOSS train 0.8278513254421187\n",
            "EPOCH 658:\n",
            "  batch 2 loss: 0.9067498255720672\n",
            "  batch 4 loss: 0.8672376364326868\n",
            "  batch 6 loss: 0.6978970406044643\n",
            "  batch 8 loss: 0.6811690306440812\n",
            "LOSS train 0.6811690306440812\n",
            "EPOCH 659:\n",
            "  batch 2 loss: 0.9986318756690606\n",
            "  batch 4 loss: 0.9732850685399268\n",
            "  batch 6 loss: 1.2160710152938576\n",
            "  batch 8 loss: 0.9189253638898163\n",
            "LOSS train 0.9189253638898163\n",
            "EPOCH 660:\n",
            "  batch 2 loss: 0.5891724675968947\n",
            "  batch 4 loss: 0.8051231114124697\n",
            "  batch 6 loss: 0.8128814958502928\n",
            "  batch 8 loss: 0.9538209935031665\n",
            "LOSS train 0.9538209935031665\n",
            "EPOCH 661:\n",
            "  batch 2 loss: 0.6628853995080206\n",
            "  batch 4 loss: 0.6468110312770936\n",
            "  batch 6 loss: 0.9287678420738356\n",
            "  batch 8 loss: 0.6447262331600354\n",
            "LOSS train 0.6447262331600354\n",
            "EPOCH 662:\n",
            "  batch 2 loss: 0.7580259829129313\n",
            "  batch 4 loss: 1.0586067644976687\n",
            "  batch 6 loss: 1.1056269908022112\n",
            "  batch 8 loss: 0.7932330540990682\n",
            "LOSS train 0.7932330540990682\n",
            "EPOCH 663:\n",
            "  batch 2 loss: 0.9481727016473952\n",
            "  batch 4 loss: 0.9879189731115834\n",
            "  batch 6 loss: 0.9011177215394559\n",
            "  batch 8 loss: 1.0429466893313597\n",
            "LOSS train 1.0429466893313597\n",
            "EPOCH 664:\n",
            "  batch 2 loss: 0.7998016700414626\n",
            "  batch 4 loss: 0.7502555646601323\n",
            "  batch 6 loss: 0.8436626708199989\n",
            "  batch 8 loss: 0.5393050907748663\n",
            "LOSS train 0.5393050907748663\n",
            "EPOCH 665:\n",
            "  batch 2 loss: 0.8443609158708936\n",
            "  batch 4 loss: 0.9020095694061621\n",
            "  batch 6 loss: 0.49713554679425737\n",
            "  batch 8 loss: 0.6659543011720434\n",
            "LOSS train 0.6659543011720434\n",
            "EPOCH 666:\n",
            "  batch 2 loss: 0.6079568914981835\n",
            "  batch 4 loss: 0.7666334847739791\n",
            "  batch 6 loss: 0.9559472650815037\n",
            "  batch 8 loss: 1.0543135787969415\n",
            "LOSS train 1.0543135787969415\n",
            "EPOCH 667:\n",
            "  batch 2 loss: 0.7458385749519416\n",
            "  batch 4 loss: 0.9746122249903045\n",
            "  batch 6 loss: 0.8776803593882977\n",
            "  batch 8 loss: 1.1841058379019602\n",
            "LOSS train 1.1841058379019602\n",
            "EPOCH 668:\n",
            "  batch 2 loss: 0.4534605725306903\n",
            "  batch 4 loss: 0.6384010084356627\n",
            "  batch 6 loss: 0.9062535511326881\n",
            "  batch 8 loss: 1.043031822534007\n",
            "LOSS train 1.043031822534007\n",
            "EPOCH 669:\n",
            "  batch 2 loss: 0.6685283853516495\n",
            "  batch 4 loss: 0.9050647844529107\n",
            "  batch 6 loss: 0.7531178592292576\n",
            "  batch 8 loss: 1.0227095097976615\n",
            "LOSS train 1.0227095097976615\n",
            "EPOCH 670:\n",
            "  batch 2 loss: 0.5131213705216865\n",
            "  batch 4 loss: 0.427088402999854\n",
            "  batch 6 loss: 0.31914382240921957\n",
            "  batch 8 loss: 0.8610962295142293\n",
            "LOSS train 0.8610962295142293\n",
            "EPOCH 671:\n",
            "  batch 2 loss: 0.9967968904757463\n",
            "  batch 4 loss: 0.2881704029603599\n",
            "  batch 6 loss: 0.6474646440112698\n",
            "  batch 8 loss: 0.7509704583304765\n",
            "LOSS train 0.7509704583304765\n",
            "EPOCH 672:\n",
            "  batch 2 loss: 0.8035683750941771\n",
            "  batch 4 loss: 1.0417594200994187\n",
            "  batch 6 loss: 0.8036900106743408\n",
            "  batch 8 loss: 0.7175366708108181\n",
            "LOSS train 0.7175366708108181\n",
            "EPOCH 673:\n",
            "  batch 2 loss: 1.1280001714136847\n",
            "  batch 4 loss: 0.847100782050704\n",
            "  batch 6 loss: 0.848271908907188\n",
            "  batch 8 loss: 0.9869314264281268\n",
            "LOSS train 0.9869314264281268\n",
            "EPOCH 674:\n",
            "  batch 2 loss: 0.7450691284639546\n",
            "  batch 4 loss: 0.6024156842649562\n",
            "  batch 6 loss: 0.8942227896202547\n",
            "  batch 8 loss: 1.2064013021723996\n",
            "LOSS train 1.2064013021723996\n",
            "EPOCH 675:\n",
            "  batch 2 loss: 0.7103455587735821\n",
            "  batch 4 loss: 0.8397639244437248\n",
            "  batch 6 loss: 1.0035465161135975\n",
            "  batch 8 loss: 1.1876928259937503\n",
            "LOSS train 1.1876928259937503\n",
            "EPOCH 676:\n",
            "  batch 2 loss: 0.9698885581325122\n",
            "  batch 4 loss: 0.8424888949335524\n",
            "  batch 6 loss: 0.8459758621130928\n",
            "  batch 8 loss: 0.7842503699057637\n",
            "LOSS train 0.7842503699057637\n",
            "EPOCH 677:\n",
            "  batch 2 loss: 1.2170301011547708\n",
            "  batch 4 loss: 0.6694343293971603\n",
            "  batch 6 loss: 1.0394697406301372\n",
            "  batch 8 loss: 0.7602809299614346\n",
            "LOSS train 0.7602809299614346\n",
            "EPOCH 678:\n",
            "  batch 2 loss: 0.5369397052171994\n",
            "  batch 4 loss: 0.8729538701301057\n",
            "  batch 6 loss: 0.9499935066922401\n",
            "  batch 8 loss: 1.031798568222742\n",
            "LOSS train 1.031798568222742\n",
            "EPOCH 679:\n",
            "  batch 2 loss: 0.7197335099020641\n",
            "  batch 4 loss: 1.0438701044424805\n",
            "  batch 6 loss: 0.9999232676868199\n",
            "  batch 8 loss: 0.8903724309769102\n",
            "LOSS train 0.8903724309769102\n",
            "EPOCH 680:\n",
            "  batch 2 loss: 0.5766130078085631\n",
            "  batch 4 loss: 1.0955439152378545\n",
            "  batch 6 loss: 0.8423431461178243\n",
            "  batch 8 loss: 0.854137011782861\n",
            "LOSS train 0.854137011782861\n",
            "EPOCH 681:\n",
            "  batch 2 loss: 0.9490055679053058\n",
            "  batch 4 loss: 1.0980015166951032\n",
            "  batch 6 loss: 1.0860345252730625\n",
            "  batch 8 loss: 1.0639682096005503\n",
            "LOSS train 1.0639682096005503\n",
            "EPOCH 682:\n",
            "  batch 2 loss: 0.8162714768998207\n",
            "  batch 4 loss: 0.9004379116905433\n",
            "  batch 6 loss: 0.7981032494998077\n",
            "  batch 8 loss: 0.767708883181747\n",
            "LOSS train 0.767708883181747\n",
            "EPOCH 683:\n",
            "  batch 2 loss: 0.6623158933861989\n",
            "  batch 4 loss: 1.01898008269266\n",
            "  batch 6 loss: 1.5180695716214019\n",
            "  batch 8 loss: 0.8160774284900297\n",
            "LOSS train 0.8160774284900297\n",
            "EPOCH 684:\n",
            "  batch 2 loss: 0.9157366211827589\n",
            "  batch 4 loss: 0.9535434987970662\n",
            "  batch 6 loss: 0.3803360531213088\n",
            "  batch 8 loss: 0.785012422031547\n",
            "LOSS train 0.785012422031547\n",
            "EPOCH 685:\n",
            "  batch 2 loss: 0.5548506792085665\n",
            "  batch 4 loss: 0.8536238757758323\n",
            "  batch 6 loss: 0.6912336568168762\n",
            "  batch 8 loss: 0.9357170272439607\n",
            "LOSS train 0.9357170272439607\n",
            "EPOCH 686:\n",
            "  batch 2 loss: 0.8779382682398534\n",
            "  batch 4 loss: 0.8270015610271997\n",
            "  batch 6 loss: 1.4337347740406892\n",
            "  batch 8 loss: 0.6464832504797304\n",
            "LOSS train 0.6464832504797304\n",
            "EPOCH 687:\n",
            "  batch 2 loss: 0.8965605436989073\n",
            "  batch 4 loss: 0.48166751020908016\n",
            "  batch 6 loss: 0.8700900128634239\n",
            "  batch 8 loss: 0.9825738078105849\n",
            "LOSS train 0.9825738078105849\n",
            "EPOCH 688:\n",
            "  batch 2 loss: 0.9425700027916925\n",
            "  batch 4 loss: 1.1758681239939843\n",
            "  batch 6 loss: 0.8073721583956033\n",
            "  batch 8 loss: 0.8343467337234016\n",
            "LOSS train 0.8343467337234016\n",
            "EPOCH 689:\n",
            "  batch 2 loss: 0.812875084920298\n",
            "  batch 4 loss: 0.7698934948129559\n",
            "  batch 6 loss: 0.705317690291732\n",
            "  batch 8 loss: 1.0973459068658094\n",
            "LOSS train 1.0973459068658094\n",
            "EPOCH 690:\n",
            "  batch 2 loss: 1.0462259900351394\n",
            "  batch 4 loss: 0.885398153507144\n",
            "  batch 6 loss: 0.9436822298635947\n",
            "  batch 8 loss: 0.5301313918410115\n",
            "LOSS train 0.5301313918410115\n",
            "EPOCH 691:\n",
            "  batch 2 loss: 0.9798696423227613\n",
            "  batch 4 loss: 1.0378080700667458\n",
            "  batch 6 loss: 0.619214607206049\n",
            "  batch 8 loss: 0.7616663984112043\n",
            "LOSS train 0.7616663984112043\n",
            "EPOCH 692:\n",
            "  batch 2 loss: 0.6203194819742326\n",
            "  batch 4 loss: 0.8570369477253152\n",
            "  batch 6 loss: 1.0390695896065134\n",
            "  batch 8 loss: 0.7091692489203312\n",
            "LOSS train 0.7091692489203312\n",
            "EPOCH 693:\n",
            "  batch 2 loss: 0.7149516933276641\n",
            "  batch 4 loss: 0.4354482843539876\n",
            "  batch 6 loss: 0.5882121653611496\n",
            "  batch 8 loss: 1.0471441985679029\n",
            "LOSS train 1.0471441985679029\n",
            "EPOCH 694:\n",
            "  batch 2 loss: 0.7431850053416462\n",
            "  batch 4 loss: 0.6895719743706856\n",
            "  batch 6 loss: 0.7582008599822184\n",
            "  batch 8 loss: 0.8380499803749146\n",
            "LOSS train 0.8380499803749146\n",
            "EPOCH 695:\n",
            "  batch 2 loss: 1.0925725522977947\n",
            "  batch 4 loss: 0.4196723673300208\n",
            "  batch 6 loss: 0.8420891935293582\n",
            "  batch 8 loss: 0.8608788054893781\n",
            "LOSS train 0.8608788054893781\n",
            "EPOCH 696:\n",
            "  batch 2 loss: 0.8067978670322992\n",
            "  batch 4 loss: 0.9090586641188041\n",
            "  batch 6 loss: 0.5692881049102299\n",
            "  batch 8 loss: 0.7071439280017089\n",
            "LOSS train 0.7071439280017089\n",
            "EPOCH 697:\n",
            "  batch 2 loss: 0.45676755767105054\n",
            "  batch 4 loss: 1.1025550368606154\n",
            "  batch 6 loss: 1.1621031812816889\n",
            "  batch 8 loss: 0.6920463809669919\n",
            "LOSS train 0.6920463809669919\n",
            "EPOCH 698:\n",
            "  batch 2 loss: 0.9595172228444926\n",
            "  batch 4 loss: 1.0533940686695864\n",
            "  batch 6 loss: 0.8408593139056549\n",
            "  batch 8 loss: 1.2024054766982628\n",
            "LOSS train 1.2024054766982628\n",
            "EPOCH 699:\n",
            "  batch 2 loss: 0.6568129052855458\n",
            "  batch 4 loss: 0.9240632275302763\n",
            "  batch 6 loss: 0.9483843831680177\n",
            "  batch 8 loss: 0.7383185003588442\n",
            "LOSS train 0.7383185003588442\n",
            "EPOCH 700:\n",
            "  batch 2 loss: 0.1162121335108103\n",
            "  batch 4 loss: 0.7071401664096962\n",
            "  batch 6 loss: 0.8067997515911456\n",
            "  batch 8 loss: 0.8408842500817649\n",
            "LOSS train 0.8408842500817649\n",
            "EPOCH 701:\n",
            "  batch 2 loss: 1.1435281190623638\n",
            "  batch 4 loss: 0.8234937923023543\n",
            "  batch 6 loss: 0.826519587457697\n",
            "  batch 8 loss: 0.9500359878250921\n",
            "LOSS train 0.9500359878250921\n",
            "EPOCH 702:\n",
            "  batch 2 loss: 0.9187762165422932\n",
            "  batch 4 loss: 0.7724698978444355\n",
            "  batch 6 loss: 1.0088123377975151\n",
            "  batch 8 loss: 0.8194624718658945\n",
            "LOSS train 0.8194624718658945\n",
            "EPOCH 703:\n",
            "  batch 2 loss: 0.37862877778594484\n",
            "  batch 4 loss: 0.9788621272788962\n",
            "  batch 6 loss: 0.8277011073901817\n",
            "  batch 8 loss: 0.699611411044976\n",
            "LOSS train 0.699611411044976\n",
            "EPOCH 704:\n",
            "  batch 2 loss: 0.5272119108165805\n",
            "  batch 4 loss: 0.7799054092411362\n",
            "  batch 6 loss: 0.6426891375913141\n",
            "  batch 8 loss: 0.5392613761307928\n",
            "LOSS train 0.5392613761307928\n",
            "EPOCH 705:\n",
            "  batch 2 loss: 0.7950937069971803\n",
            "  batch 4 loss: 0.9061385562462725\n",
            "  batch 6 loss: 0.2201518750380647\n",
            "  batch 8 loss: 0.7403541100666435\n",
            "LOSS train 0.7403541100666435\n",
            "EPOCH 706:\n",
            "  batch 2 loss: 0.8937203442695449\n",
            "  batch 4 loss: 0.40750926041431157\n",
            "  batch 6 loss: 1.0402341974100429\n",
            "  batch 8 loss: 0.5339299211248171\n",
            "LOSS train 0.5339299211248171\n",
            "EPOCH 707:\n",
            "  batch 2 loss: 0.7314053551504265\n",
            "  batch 4 loss: 1.1028049852270034\n",
            "  batch 6 loss: 1.3714553708600286\n",
            "  batch 8 loss: 0.7581473935022398\n",
            "LOSS train 0.7581473935022398\n",
            "EPOCH 708:\n",
            "  batch 2 loss: 0.7794304715301584\n",
            "  batch 4 loss: 1.0321934988261987\n",
            "  batch 6 loss: 1.245315879609857\n",
            "  batch 8 loss: 0.6190678409122397\n",
            "LOSS train 0.6190678409122397\n",
            "EPOCH 709:\n",
            "  batch 2 loss: 1.2660650778379074\n",
            "  batch 4 loss: 0.8765471559578553\n",
            "  batch 6 loss: 0.7903428237720069\n",
            "  batch 8 loss: 1.2697903714295167\n",
            "LOSS train 1.2697903714295167\n",
            "EPOCH 710:\n",
            "  batch 2 loss: 1.2234873379655433\n",
            "  batch 4 loss: 0.767826421245083\n",
            "  batch 6 loss: 0.5082698187895949\n",
            "  batch 8 loss: 0.32305219861403556\n",
            "LOSS train 0.32305219861403556\n",
            "EPOCH 711:\n",
            "  batch 2 loss: 0.6785283280401095\n",
            "  batch 4 loss: 0.980232926912088\n",
            "  batch 6 loss: 1.154752701547482\n",
            "  batch 8 loss: 1.10381317289591\n",
            "LOSS train 1.10381317289591\n",
            "EPOCH 712:\n",
            "  batch 2 loss: 1.0149330109397148\n",
            "  batch 4 loss: 0.7573318972313946\n",
            "  batch 6 loss: 0.710683959590442\n",
            "  batch 8 loss: 0.6870943250185235\n",
            "LOSS train 0.6870943250185235\n",
            "EPOCH 713:\n",
            "  batch 2 loss: 0.9980554906256556\n",
            "  batch 4 loss: 1.2900450778181192\n",
            "  batch 6 loss: 0.9010018108200417\n",
            "  batch 8 loss: 1.24037600169383\n",
            "LOSS train 1.24037600169383\n",
            "EPOCH 714:\n",
            "  batch 2 loss: 0.9330011772570266\n",
            "  batch 4 loss: 0.9823301844835135\n",
            "  batch 6 loss: 1.1229324312130537\n",
            "  batch 8 loss: 0.5587128035870104\n",
            "LOSS train 0.5587128035870104\n",
            "EPOCH 715:\n",
            "  batch 2 loss: 1.1633833710212593\n",
            "  batch 4 loss: 0.9747726166244116\n",
            "  batch 6 loss: 0.7470123100188938\n",
            "  batch 8 loss: 0.8997941145454587\n",
            "LOSS train 0.8997941145454587\n",
            "EPOCH 716:\n",
            "  batch 2 loss: 0.6151655667690215\n",
            "  batch 4 loss: 0.7729128708686677\n",
            "  batch 6 loss: 1.0410181860543082\n",
            "  batch 8 loss: 0.21075478763635933\n",
            "LOSS train 0.21075478763635933\n",
            "EPOCH 717:\n",
            "  batch 2 loss: 0.8857178640225162\n",
            "  batch 4 loss: 0.7578220212496979\n",
            "  batch 6 loss: 0.9675226684938282\n",
            "  batch 8 loss: 0.6907021693047953\n",
            "LOSS train 0.6907021693047953\n",
            "EPOCH 718:\n",
            "  batch 2 loss: 1.0255689388814448\n",
            "  batch 4 loss: 0.48960249843212633\n",
            "  batch 6 loss: 1.144061453294018\n",
            "  batch 8 loss: 0.8258937110771261\n",
            "LOSS train 0.8258937110771261\n",
            "EPOCH 719:\n",
            "  batch 2 loss: 1.1890700980876754\n",
            "  batch 4 loss: 0.6014619341378677\n",
            "  batch 6 loss: 0.9028612491976322\n",
            "  batch 8 loss: 0.8644210027079294\n",
            "LOSS train 0.8644210027079294\n",
            "EPOCH 720:\n",
            "  batch 2 loss: 0.6857170933496559\n",
            "  batch 4 loss: 0.31621377833027453\n",
            "  batch 6 loss: 0.7506721616806477\n",
            "  batch 8 loss: 0.9347386093857497\n",
            "LOSS train 0.9347386093857497\n",
            "EPOCH 721:\n",
            "  batch 2 loss: 0.946986624489881\n",
            "  batch 4 loss: 1.087910929185169\n",
            "  batch 6 loss: 0.8765541385439769\n",
            "  batch 8 loss: 1.067350804520919\n",
            "LOSS train 1.067350804520919\n",
            "EPOCH 722:\n",
            "  batch 2 loss: 0.7239859013207856\n",
            "  batch 4 loss: 0.8245977263374545\n",
            "  batch 6 loss: 0.5960830421559835\n",
            "  batch 8 loss: 0.4231669549102822\n",
            "LOSS train 0.4231669549102822\n",
            "EPOCH 723:\n",
            "  batch 2 loss: 0.8319122328595066\n",
            "  batch 4 loss: 1.3550622759155035\n",
            "  batch 6 loss: 0.39507807402848527\n",
            "  batch 8 loss: 1.1665959323121808\n",
            "LOSS train 1.1665959323121808\n",
            "EPOCH 724:\n",
            "  batch 2 loss: 0.8779835665392277\n",
            "  batch 4 loss: 0.5183003565007802\n",
            "  batch 6 loss: 1.1500839596274088\n",
            "  batch 8 loss: 1.024686648987415\n",
            "LOSS train 1.024686648987415\n",
            "EPOCH 725:\n",
            "  batch 2 loss: 0.7260692885088297\n",
            "  batch 4 loss: 0.9002793183401729\n",
            "  batch 6 loss: 0.5033528369234872\n",
            "  batch 8 loss: 0.9163477660331028\n",
            "LOSS train 0.9163477660331028\n",
            "EPOCH 726:\n",
            "  batch 2 loss: 0.6390857384814872\n",
            "  batch 4 loss: 1.135922338852418\n",
            "  batch 6 loss: 0.8553285860245263\n",
            "  batch 8 loss: 0.790958035434822\n",
            "LOSS train 0.790958035434822\n",
            "EPOCH 727:\n",
            "  batch 2 loss: 0.8389791773270965\n",
            "  batch 4 loss: 0.5097295869926279\n",
            "  batch 6 loss: 1.1214126426910305\n",
            "  batch 8 loss: 0.6339950371676011\n",
            "LOSS train 0.6339950371676011\n",
            "EPOCH 728:\n",
            "  batch 2 loss: 0.5811556122949237\n",
            "  batch 4 loss: 0.699254950619806\n",
            "  batch 6 loss: 1.1219224318612082\n",
            "  batch 8 loss: 1.1538650824013876\n",
            "LOSS train 1.1538650824013876\n",
            "EPOCH 729:\n",
            "  batch 2 loss: 0.8293165913344547\n",
            "  batch 4 loss: 0.8163345897362532\n",
            "  batch 6 loss: 1.15844474813545\n",
            "  batch 8 loss: 0.9530011542379431\n",
            "LOSS train 0.9530011542379431\n",
            "EPOCH 730:\n",
            "  batch 2 loss: 0.5501842609184883\n",
            "  batch 4 loss: 1.0165785121016908\n",
            "  batch 6 loss: 0.6685324742222769\n",
            "  batch 8 loss: 1.082987163805171\n",
            "LOSS train 1.082987163805171\n",
            "EPOCH 731:\n",
            "  batch 2 loss: 0.7107584068726055\n",
            "  batch 4 loss: 1.1189969969602467\n",
            "  batch 6 loss: 0.3916778216956623\n",
            "  batch 8 loss: 0.8713094519192566\n",
            "LOSS train 0.8713094519192566\n",
            "EPOCH 732:\n",
            "  batch 2 loss: 0.9676641840342444\n",
            "  batch 4 loss: 0.8013454579938358\n",
            "  batch 6 loss: 0.9690054070437394\n",
            "  batch 8 loss: 0.8973219035256419\n",
            "LOSS train 0.8973219035256419\n",
            "EPOCH 733:\n",
            "  batch 2 loss: 1.0877986343629753\n",
            "  batch 4 loss: 0.7545131762767976\n",
            "  batch 6 loss: 0.8781188383925052\n",
            "  batch 8 loss: 0.8117411922106499\n",
            "LOSS train 0.8117411922106499\n",
            "EPOCH 734:\n",
            "  batch 2 loss: 0.43981031838746565\n",
            "  batch 4 loss: 0.8383698360336926\n",
            "  batch 6 loss: 1.1348588741206704\n",
            "  batch 8 loss: 0.601357595779819\n",
            "LOSS train 0.601357595779819\n",
            "EPOCH 735:\n",
            "  batch 2 loss: 1.2378602916830699\n",
            "  batch 4 loss: 1.0517499360754534\n",
            "  batch 6 loss: 1.0816447216526575\n",
            "  batch 8 loss: 0.7723945535110829\n",
            "LOSS train 0.7723945535110829\n",
            "EPOCH 736:\n",
            "  batch 2 loss: 0.8105010690018812\n",
            "  batch 4 loss: 1.1200275872270347\n",
            "  batch 6 loss: 0.9367847559426192\n",
            "  batch 8 loss: 1.1012592383831377\n",
            "LOSS train 1.1012592383831377\n",
            "EPOCH 737:\n",
            "  batch 2 loss: 1.0278576288635741\n",
            "  batch 4 loss: 0.8720041781112458\n",
            "  batch 6 loss: 0.7553299805338114\n",
            "  batch 8 loss: 0.6640087482073245\n",
            "LOSS train 0.6640087482073245\n",
            "EPOCH 738:\n",
            "  batch 2 loss: 1.1798668117748392\n",
            "  batch 4 loss: 1.008314701331445\n",
            "  batch 6 loss: 0.9486049830597054\n",
            "  batch 8 loss: 1.1552834654530093\n",
            "LOSS train 1.1552834654530093\n",
            "EPOCH 739:\n",
            "  batch 2 loss: 0.6846256119743213\n",
            "  batch 4 loss: 0.9065132368249742\n",
            "  batch 6 loss: 0.8030744785185987\n",
            "  batch 8 loss: 1.0772644701197027\n",
            "LOSS train 1.0772644701197027\n",
            "EPOCH 740:\n",
            "  batch 2 loss: 0.7340934663066072\n",
            "  batch 4 loss: 0.8986674862165696\n",
            "  batch 6 loss: 0.992263810943355\n",
            "  batch 8 loss: 0.663593600572602\n",
            "LOSS train 0.663593600572602\n",
            "EPOCH 741:\n",
            "  batch 2 loss: 0.5615124149321437\n",
            "  batch 4 loss: 0.8566696127274085\n",
            "  batch 6 loss: 0.6568759745698902\n",
            "  batch 8 loss: 0.46891897906203905\n",
            "LOSS train 0.46891897906203905\n",
            "EPOCH 742:\n",
            "  batch 2 loss: 1.3245388346706357\n",
            "  batch 4 loss: 1.0548109266367272\n",
            "  batch 6 loss: 1.0345697570766421\n",
            "  batch 8 loss: 0.5537960901071309\n",
            "LOSS train 0.5537960901071309\n",
            "EPOCH 743:\n",
            "  batch 2 loss: 0.9904746919776821\n",
            "  batch 4 loss: 0.9293763074669616\n",
            "  batch 6 loss: 0.7811978701806663\n",
            "  batch 8 loss: 0.7818171643024074\n",
            "LOSS train 0.7818171643024074\n",
            "EPOCH 744:\n",
            "  batch 2 loss: 1.7145562547006197\n",
            "  batch 4 loss: 0.6060486098967622\n",
            "  batch 6 loss: 0.9661954311596948\n",
            "  batch 8 loss: 0.6327226601618472\n",
            "LOSS train 0.6327226601618472\n",
            "EPOCH 745:\n",
            "  batch 2 loss: 0.8431922219328936\n",
            "  batch 4 loss: 0.8174223747755094\n",
            "  batch 6 loss: 0.7490755387610821\n",
            "  batch 8 loss: 0.713324176525483\n",
            "LOSS train 0.713324176525483\n",
            "EPOCH 746:\n",
            "  batch 2 loss: 0.6726196799042538\n",
            "  batch 4 loss: 1.2716959717757883\n",
            "  batch 6 loss: 0.9000654682489276\n",
            "  batch 8 loss: 0.9127080505554359\n",
            "LOSS train 0.9127080505554359\n",
            "EPOCH 747:\n",
            "  batch 2 loss: 0.8267858385888178\n",
            "  batch 4 loss: 1.0832054181417652\n",
            "  batch 6 loss: 0.7734766757452499\n",
            "  batch 8 loss: 0.8244835261284595\n",
            "LOSS train 0.8244835261284595\n",
            "EPOCH 748:\n",
            "  batch 2 loss: 0.8002882311495784\n",
            "  batch 4 loss: 1.1099644926100414\n",
            "  batch 6 loss: 0.5013063568463617\n",
            "  batch 8 loss: 0.8458991729763576\n",
            "LOSS train 0.8458991729763576\n",
            "EPOCH 749:\n",
            "  batch 2 loss: 0.985568915665971\n",
            "  batch 4 loss: 0.9608139532546945\n",
            "  batch 6 loss: 0.5225942338677072\n",
            "  batch 8 loss: 0.6079438118851106\n",
            "LOSS train 0.6079438118851106\n",
            "EPOCH 750:\n",
            "  batch 2 loss: 0.996523968662901\n",
            "  batch 4 loss: 0.8098221455454071\n",
            "  batch 6 loss: 0.3417238585975962\n",
            "  batch 8 loss: 1.0303112891018795\n",
            "LOSS train 1.0303112891018795\n",
            "EPOCH 751:\n",
            "  batch 2 loss: 1.2851504370108682\n",
            "  batch 4 loss: 0.6921399868656635\n",
            "  batch 6 loss: 0.8281372189001179\n",
            "  batch 8 loss: 0.5105711538753579\n",
            "LOSS train 0.5105711538753579\n",
            "EPOCH 752:\n",
            "  batch 2 loss: 0.8506010298280896\n",
            "  batch 4 loss: 0.8012498242066557\n",
            "  batch 6 loss: 0.894047181405656\n",
            "  batch 8 loss: 0.7694336755707396\n",
            "LOSS train 0.7694336755707396\n",
            "EPOCH 753:\n",
            "  batch 2 loss: 0.8339734673928998\n",
            "  batch 4 loss: 0.8675526575090716\n",
            "  batch 6 loss: 0.7029175091556115\n",
            "  batch 8 loss: 0.6286437583816058\n",
            "LOSS train 0.6286437583816058\n",
            "EPOCH 754:\n",
            "  batch 2 loss: 1.0119865599130071\n",
            "  batch 4 loss: 0.8779265467655859\n",
            "  batch 6 loss: 0.8886241942813875\n",
            "  batch 8 loss: 1.1339106678830209\n",
            "LOSS train 1.1339106678830209\n",
            "EPOCH 755:\n",
            "  batch 2 loss: 1.032529219366006\n",
            "  batch 4 loss: 1.0508335470947263\n",
            "  batch 6 loss: 0.6987647845376438\n",
            "  batch 8 loss: 0.8722016040543868\n",
            "LOSS train 0.8722016040543868\n",
            "EPOCH 756:\n",
            "  batch 2 loss: 1.1688901557207052\n",
            "  batch 4 loss: 1.0479696747647678\n",
            "  batch 6 loss: 0.7134073299823251\n",
            "  batch 8 loss: 0.7418897514779716\n",
            "LOSS train 0.7418897514779716\n",
            "EPOCH 757:\n",
            "  batch 2 loss: 0.6739158809710514\n",
            "  batch 4 loss: 0.6051512981872627\n",
            "  batch 6 loss: 0.5057390446069098\n",
            "  batch 8 loss: 0.6924240389373775\n",
            "LOSS train 0.6924240389373775\n",
            "EPOCH 758:\n",
            "  batch 2 loss: 1.2017189192574986\n",
            "  batch 4 loss: 0.7984152493070811\n",
            "  batch 6 loss: 0.6927634644802455\n",
            "  batch 8 loss: 0.7173307293519674\n",
            "LOSS train 0.7173307293519674\n",
            "EPOCH 759:\n",
            "  batch 2 loss: 0.9819573698360445\n",
            "  batch 4 loss: 1.204113277918341\n",
            "  batch 6 loss: 1.1242622997788643\n",
            "  batch 8 loss: 0.6732366589989872\n",
            "LOSS train 0.6732366589989872\n",
            "EPOCH 760:\n",
            "  batch 2 loss: 0.9479207712366409\n",
            "  batch 4 loss: 0.9788560797686948\n",
            "  batch 6 loss: 0.8907342435598631\n",
            "  batch 8 loss: 1.0302790422292394\n",
            "LOSS train 1.0302790422292394\n",
            "EPOCH 761:\n",
            "  batch 2 loss: 0.8536751977750174\n",
            "  batch 4 loss: 1.188605992593011\n",
            "  batch 6 loss: 0.6231421525569594\n",
            "  batch 8 loss: 0.8108120331731121\n",
            "LOSS train 0.8108120331731121\n",
            "EPOCH 762:\n",
            "  batch 2 loss: 1.2045141660835612\n",
            "  batch 4 loss: 0.690381370614437\n",
            "  batch 6 loss: 0.7923057149874911\n",
            "  batch 8 loss: 0.7598505975876755\n",
            "LOSS train 0.7598505975876755\n",
            "EPOCH 763:\n",
            "  batch 2 loss: 0.8810800853983021\n",
            "  batch 4 loss: 0.9161584813181332\n",
            "  batch 6 loss: 0.7874202807099366\n",
            "  batch 8 loss: 0.9500137922001941\n",
            "LOSS train 0.9500137922001941\n",
            "EPOCH 764:\n",
            "  batch 2 loss: 0.6486447486229149\n",
            "  batch 4 loss: 1.1871652584022598\n",
            "  batch 6 loss: 0.8184673977197378\n",
            "  batch 8 loss: 1.2695133110452548\n",
            "LOSS train 1.2695133110452548\n",
            "EPOCH 765:\n",
            "  batch 2 loss: 1.0170287414788224\n",
            "  batch 4 loss: 1.0223050607694213\n",
            "  batch 6 loss: 0.8183667043186718\n",
            "  batch 8 loss: 1.1095956399969764\n",
            "LOSS train 1.1095956399969764\n",
            "EPOCH 766:\n",
            "  batch 2 loss: 0.5929274749444549\n",
            "  batch 4 loss: 0.6792115282091866\n",
            "  batch 6 loss: 1.1658557249347496\n",
            "  batch 8 loss: 0.8536931074488501\n",
            "LOSS train 0.8536931074488501\n",
            "EPOCH 767:\n",
            "  batch 2 loss: 0.5370655726379623\n",
            "  batch 4 loss: 1.0580082998798348\n",
            "  batch 6 loss: 0.5815870493145179\n",
            "  batch 8 loss: 0.9675969015233228\n",
            "LOSS train 0.9675969015233228\n",
            "EPOCH 768:\n",
            "  batch 2 loss: 0.7418049646712546\n",
            "  batch 4 loss: 1.129200401142229\n",
            "  batch 6 loss: 0.8514508374444024\n",
            "  batch 8 loss: 0.5571242407454842\n",
            "LOSS train 0.5571242407454842\n",
            "EPOCH 769:\n",
            "  batch 2 loss: 0.6714273970173379\n",
            "  batch 4 loss: 0.9173820346313071\n",
            "  batch 6 loss: 0.7487818064413649\n",
            "  batch 8 loss: 1.0878329044061363\n",
            "LOSS train 1.0878329044061363\n",
            "EPOCH 770:\n",
            "  batch 2 loss: 0.8399868228889322\n",
            "  batch 4 loss: 1.062486203933835\n",
            "  batch 6 loss: 1.3283076087272603\n",
            "  batch 8 loss: 1.0322457652498798\n",
            "LOSS train 1.0322457652498798\n",
            "EPOCH 771:\n",
            "  batch 2 loss: 1.1588810587881535\n",
            "  batch 4 loss: 0.4486474492672945\n",
            "  batch 6 loss: 0.7219696226146617\n",
            "  batch 8 loss: 0.8825273688020798\n",
            "LOSS train 0.8825273688020798\n",
            "EPOCH 772:\n",
            "  batch 2 loss: 1.2105559455425439\n",
            "  batch 4 loss: 0.6424951117366593\n",
            "  batch 6 loss: 0.8417836630309412\n",
            "  batch 8 loss: 0.7315618052844518\n",
            "LOSS train 0.7315618052844518\n",
            "EPOCH 773:\n",
            "  batch 2 loss: 0.9652171408312956\n",
            "  batch 4 loss: 1.0596810823418543\n",
            "  batch 6 loss: 0.4747055587511423\n",
            "  batch 8 loss: 0.9438529534636775\n",
            "LOSS train 0.9438529534636775\n",
            "EPOCH 774:\n",
            "  batch 2 loss: 1.0837093454451732\n",
            "  batch 4 loss: 1.0877287253353694\n",
            "  batch 6 loss: 1.050520227747832\n",
            "  batch 8 loss: 0.5674442448285296\n",
            "LOSS train 0.5674442448285296\n",
            "EPOCH 775:\n",
            "  batch 2 loss: 0.7792046035325867\n",
            "  batch 4 loss: 0.7606469756589826\n",
            "  batch 6 loss: 0.6437513490898255\n",
            "  batch 8 loss: 0.9052978111183301\n",
            "LOSS train 0.9052978111183301\n",
            "EPOCH 776:\n",
            "  batch 2 loss: 0.5600305374193028\n",
            "  batch 4 loss: 0.8941841189974621\n",
            "  batch 6 loss: 1.0717890707290691\n",
            "  batch 8 loss: 0.913886490953268\n",
            "LOSS train 0.913886490953268\n",
            "EPOCH 777:\n",
            "  batch 2 loss: 0.6971079930182428\n",
            "  batch 4 loss: 0.5628834372998959\n",
            "  batch 6 loss: 0.9091334760824276\n",
            "  batch 8 loss: 0.9266315331607364\n",
            "LOSS train 0.9266315331607364\n",
            "EPOCH 778:\n",
            "  batch 2 loss: 1.0773388824587422\n",
            "  batch 4 loss: 0.7049611697118165\n",
            "  batch 6 loss: 0.3651734176541136\n",
            "  batch 8 loss: 0.8286408297238836\n",
            "LOSS train 0.8286408297238836\n",
            "EPOCH 779:\n",
            "  batch 2 loss: 1.0742469607753384\n",
            "  batch 4 loss: 0.834066591332018\n",
            "  batch 6 loss: 0.810400953504731\n",
            "  batch 8 loss: 0.695619848321438\n",
            "LOSS train 0.695619848321438\n",
            "EPOCH 780:\n",
            "  batch 2 loss: 0.847045501660798\n",
            "  batch 4 loss: 0.945205887645487\n",
            "  batch 6 loss: 0.5315180083501867\n",
            "  batch 8 loss: 0.794521784396348\n",
            "LOSS train 0.794521784396348\n",
            "EPOCH 781:\n",
            "  batch 2 loss: 0.8687776430938392\n",
            "  batch 4 loss: 1.1065256718718017\n",
            "  batch 6 loss: 1.0746962176646626\n",
            "  batch 8 loss: 0.8021200324528743\n",
            "LOSS train 0.8021200324528743\n",
            "EPOCH 782:\n",
            "  batch 2 loss: 0.7593472213199717\n",
            "  batch 4 loss: 0.6158714410223138\n",
            "  batch 6 loss: 1.2676625832917674\n",
            "  batch 8 loss: 0.548233188674934\n",
            "LOSS train 0.548233188674934\n",
            "EPOCH 783:\n",
            "  batch 2 loss: 0.5782855908818041\n",
            "  batch 4 loss: 1.0201112993945847\n",
            "  batch 6 loss: 1.0979307943064995\n",
            "  batch 8 loss: 0.8888211061449028\n",
            "LOSS train 0.8888211061449028\n",
            "EPOCH 784:\n",
            "  batch 2 loss: 1.013738088705478\n",
            "  batch 4 loss: 1.0820456263250797\n",
            "  batch 6 loss: 0.9654950790489933\n",
            "  batch 8 loss: 0.8943229640000397\n",
            "LOSS train 0.8943229640000397\n",
            "EPOCH 785:\n",
            "  batch 2 loss: 0.8268128563577004\n",
            "  batch 4 loss: 0.6546175816066322\n",
            "  batch 6 loss: 1.2209642573266235\n",
            "  batch 8 loss: 0.7298425773039807\n",
            "LOSS train 0.7298425773039807\n",
            "EPOCH 786:\n",
            "  batch 2 loss: 0.844639694429321\n",
            "  batch 4 loss: 0.916080308106858\n",
            "  batch 6 loss: 1.2102674195966352\n",
            "  batch 8 loss: 1.1276232497593837\n",
            "LOSS train 1.1276232497593837\n",
            "EPOCH 787:\n",
            "  batch 2 loss: 1.1616459225030016\n",
            "  batch 4 loss: 0.7895862900054407\n",
            "  batch 6 loss: 0.8594531001699052\n",
            "  batch 8 loss: 1.0370490951750788\n",
            "LOSS train 1.0370490951750788\n",
            "EPOCH 788:\n",
            "  batch 2 loss: 1.131608398660275\n",
            "  batch 4 loss: 0.4048495818459209\n",
            "  batch 6 loss: 0.3069715372972095\n",
            "  batch 8 loss: 0.5621728802574517\n",
            "LOSS train 0.5621728802574517\n",
            "EPOCH 789:\n",
            "  batch 2 loss: 1.0546578950060002\n",
            "  batch 4 loss: 0.8269920779063907\n",
            "  batch 6 loss: 1.15225115036238\n",
            "  batch 8 loss: 1.1189267292406897\n",
            "LOSS train 1.1189267292406897\n",
            "EPOCH 790:\n",
            "  batch 2 loss: 1.0286483221985288\n",
            "  batch 4 loss: 1.041115204771348\n",
            "  batch 6 loss: 0.8354317416574852\n",
            "  batch 8 loss: 0.7506766819627128\n",
            "LOSS train 0.7506766819627128\n",
            "EPOCH 791:\n",
            "  batch 2 loss: 0.5939224867816475\n",
            "  batch 4 loss: 0.965730346479607\n",
            "  batch 6 loss: 0.9026191739797461\n",
            "  batch 8 loss: 1.131896770754614\n",
            "LOSS train 1.131896770754614\n",
            "EPOCH 792:\n",
            "  batch 2 loss: 1.149352919981861\n",
            "  batch 4 loss: 0.9306186837179345\n",
            "  batch 6 loss: 0.7908322551033229\n",
            "  batch 8 loss: 0.8917158947274169\n",
            "LOSS train 0.8917158947274169\n",
            "EPOCH 793:\n",
            "  batch 2 loss: 0.9669235467890769\n",
            "  batch 4 loss: 1.1224231666384692\n",
            "  batch 6 loss: 0.6064721655029368\n",
            "  batch 8 loss: 0.9912816969274528\n",
            "LOSS train 0.9912816969274528\n",
            "EPOCH 794:\n",
            "  batch 2 loss: 0.7381401674655832\n",
            "  batch 4 loss: 1.0871348687971838\n",
            "  batch 6 loss: 0.9153950191085674\n",
            "  batch 8 loss: 0.8454818046042384\n",
            "LOSS train 0.8454818046042384\n",
            "EPOCH 795:\n",
            "  batch 2 loss: 0.863974217901075\n",
            "  batch 4 loss: 0.6028303923378184\n",
            "  batch 6 loss: 1.1008324556469906\n",
            "  batch 8 loss: 1.0627523643893042\n",
            "LOSS train 1.0627523643893042\n",
            "EPOCH 796:\n",
            "  batch 2 loss: 1.1903517008836149\n",
            "  batch 4 loss: 1.0975257252755979\n",
            "  batch 6 loss: 0.5851979795703961\n",
            "  batch 8 loss: 0.9692653612709955\n",
            "LOSS train 0.9692653612709955\n",
            "EPOCH 797:\n",
            "  batch 2 loss: 0.8731583212046654\n",
            "  batch 4 loss: 0.7349734463213126\n",
            "  batch 6 loss: 0.6992602505621371\n",
            "  batch 8 loss: 0.7171702668103161\n",
            "LOSS train 0.7171702668103161\n",
            "EPOCH 798:\n",
            "  batch 2 loss: 0.8652518973662626\n",
            "  batch 4 loss: 0.9221966265642707\n",
            "  batch 6 loss: 0.5537425057121962\n",
            "  batch 8 loss: 1.1892268478807777\n",
            "LOSS train 1.1892268478807777\n",
            "EPOCH 799:\n",
            "  batch 2 loss: 0.7804801223171797\n",
            "  batch 4 loss: 0.5861164521335508\n",
            "  batch 6 loss: 0.8073467414856383\n",
            "  batch 8 loss: 0.756774029060334\n",
            "LOSS train 0.756774029060334\n",
            "EPOCH 800:\n",
            "  batch 2 loss: 0.6581775224807289\n",
            "  batch 4 loss: 0.690770097558426\n",
            "  batch 6 loss: 0.6800181778891307\n",
            "  batch 8 loss: 0.8269577588825141\n",
            "LOSS train 0.8269577588825141\n",
            "EPOCH 801:\n",
            "  batch 2 loss: 0.5068845973738306\n",
            "  batch 4 loss: 0.9640540272923819\n",
            "  batch 6 loss: 0.8875700028044753\n",
            "  batch 8 loss: 0.2416452503458757\n",
            "LOSS train 0.2416452503458757\n",
            "EPOCH 802:\n",
            "  batch 2 loss: 0.5003591700221584\n",
            "  batch 4 loss: 1.0435340182586261\n",
            "  batch 6 loss: 1.1603094359887751\n",
            "  batch 8 loss: 0.44136266290825915\n",
            "LOSS train 0.44136266290825915\n",
            "EPOCH 803:\n",
            "  batch 2 loss: 1.1476151297045694\n",
            "  batch 4 loss: 1.121722902600347\n",
            "  batch 6 loss: 0.9826847614118186\n",
            "  batch 8 loss: 1.07153264087396\n",
            "LOSS train 1.07153264087396\n",
            "EPOCH 804:\n",
            "  batch 2 loss: 0.5993432210386231\n",
            "  batch 4 loss: 0.6305161534706462\n",
            "  batch 6 loss: 0.7769439103538365\n",
            "  batch 8 loss: 1.1081449206306273\n",
            "LOSS train 1.1081449206306273\n",
            "EPOCH 805:\n",
            "  batch 2 loss: 0.8366241305300868\n",
            "  batch 4 loss: 0.7481545227543345\n",
            "  batch 6 loss: 0.7472814678604636\n",
            "  batch 8 loss: 0.9643353161374904\n",
            "LOSS train 0.9643353161374904\n",
            "EPOCH 806:\n",
            "  batch 2 loss: 0.7309734233499955\n",
            "  batch 4 loss: 1.0244642245906805\n",
            "  batch 6 loss: 1.1138963960417643\n",
            "  batch 8 loss: 1.1266306977784848\n",
            "LOSS train 1.1266306977784848\n",
            "EPOCH 807:\n",
            "  batch 2 loss: 0.7846022585858323\n",
            "  batch 4 loss: 1.0654631141116158\n",
            "  batch 6 loss: 0.9481068485727598\n",
            "  batch 8 loss: 1.1002346205497795\n",
            "LOSS train 1.1002346205497795\n",
            "EPOCH 808:\n",
            "  batch 2 loss: 0.637318600259006\n",
            "  batch 4 loss: 0.7260767689464782\n",
            "  batch 6 loss: 0.902162514241647\n",
            "  batch 8 loss: 0.5425923155259514\n",
            "LOSS train 0.5425923155259514\n",
            "EPOCH 809:\n",
            "  batch 2 loss: 0.8410347781798231\n",
            "  batch 4 loss: 1.0186175292667723\n",
            "  batch 6 loss: 0.8306978018341923\n",
            "  batch 8 loss: 0.7521401260407201\n",
            "LOSS train 0.7521401260407201\n",
            "EPOCH 810:\n",
            "  batch 2 loss: 0.578399043585102\n",
            "  batch 4 loss: 0.9299458572597732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #if training got interrupted manually, save:\n",
        "# torch.save({'epoch': n_epochs,\n",
        "#                'model_state_dict': model.state_dict(),\n",
        "#                'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                'loss': avg_loss\n",
        "#                }, f'boundary_pinn_interrupt.pt')"
      ],
      "metadata": {
        "id": "F6UaRYiBDnSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Visualizing the training"
      ],
      "metadata": {
        "id": "JrWOrOIiJP62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization for training losses across curriculum steps\n",
        "if nb_curriculum < 3:\n",
        "    # If the number of curricula is less than 3, plot only the last curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.loglog(losses[-1], label=f'Adam({nb_curriculum}/{nb_curriculum})', color='C0')\n",
        "    plt.loglog(losses_lbfgs[-1], label=f'L-BFGS ({nb_curriculum}/{nb_curriculum})', color='C1')\n",
        "    plt.title(f'Training Loss for Curriculum PINN', fontsize=16)\n",
        "    plt.xlabel('Epochs', fontsize=14)\n",
        "    plt.ylabel('Loss (log scale)', fontsize=14)\n",
        "    plt.legend(fontsize=12, loc='upper right')\n",
        "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    # If there are 3 or more curricula, plot the 1st, middle, and last curves\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Determine which curricula to plot: first, middle, last\n",
        "    curricula_to_plot = [nb_curriculum // 3, 2 * nb_curriculum // 3, nb_curriculum - 1]\n",
        "    titles = [\n",
        "        f'Curriculum: {nb_curriculum // 3}/{nb_curriculum}',\n",
        "        f'Curriculum: {2 * nb_curriculum // 3}/{nb_curriculum}',\n",
        "        f'Final Curriculum: {nb_curriculum}/{nb_curriculum}'\n",
        "    ]\n",
        "\n",
        "    # Loop over the subplots to plot the chosen curricula\n",
        "    for i, ax in enumerate(axs):\n",
        "        ax.loglog(losses[curricula_to_plot[i]], label=\"Adam: \" + titles[i], color=f'C{i}')\n",
        "        ax.loglog(losses_lbfgs[curricula_to_plot[i]], label=\"L-BFGS: \" + titles[i], color=f'C{i+3}')\n",
        "        ax.set_title(titles[i], fontsize=14)\n",
        "        ax.set_xlabel('Epochs', fontsize=12)\n",
        "        ax.set_ylabel('Loss (log scale)', fontsize=12)\n",
        "        ax.legend(fontsize=10, loc='upper right')\n",
        "        ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5, alpha=0.7)\n",
        "\n",
        "    # Adjust layout for better spacing\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "fHEYUG0FTjQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6bis) Training for longer if convergence isn't attained [optional]"
      ],
      "metadata": {
        "id": "edNcmpgJ1oRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #reloading the previously saved checkpoint\n",
        "# checkpoint = torch.load(f'boundary_pinn_{method}.pt')\n",
        "\n",
        "# #intialization of the model and optimizer\n",
        "# model = BoundaryPINN(power=power, width=width, depth=depth)\n",
        "\n",
        "# extra_epochs = 10_000\n",
        "# n_decreases = 100\n",
        "# learning_rate_adam = 1e-4\n",
        "# damping = 1e-2\n",
        "# gamma = damping**(1/n_decreases)\n",
        "\n",
        "# if method == 'Adam':\n",
        "#         optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate_adam)\n",
        "#         scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\\\n",
        "#                                                     step_size=extra_epochs//n_decreases,\\\n",
        "#                                                     gamma=gamma)\n",
        "\n",
        "\n",
        "# #reloading of parameters from end of previous training\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "# start_epoch = checkpoint['epoch']\n",
        "# loss = checkpoint['loss']\n",
        "\n",
        "# #retrain for 'extra_epochs'\n",
        "# for epoch in range(start_epoch+1, start_epoch + extra_epochs):\n",
        "#         #epoch_number += 1\n",
        "#         print('EPOCH {}:'.format(epoch))\n",
        "\n",
        "#         # Make sure gradient tracking is on, and do a pass over the data\n",
        "#         model.train(True)\n",
        "#         avg_loss = train_one_epoch(model, optimizer, zeta, eps, losses[i])\n",
        "#         scheduler.step()\n",
        "#         print('LOSS train {}'.format(avg_loss))\n",
        "\n",
        "# #save the further trained model and reset optimizer\n",
        "# torch.save({'epoch': start_epoch + extra_epochs,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'loss': avg_loss\n",
        "#             }, f'boundary_pinn_{method}.pt')"
      ],
      "metadata": {
        "id": "82v6XX_y1n-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Visualizing the results"
      ],
      "metadata": {
        "id": "oPXpOpNEVnK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#load the model to plot\n",
        "interrupt = False\n",
        "n_plot = nb_curriculum\n",
        "\n",
        "if os.path.isfile('boundary_pinn_interrupt.pt'):\n",
        "    interrupt = True\n",
        "    final_checkpoint = torch.load('boundary_pinn_interrupt.pt')\n",
        "    model.load_state_dict(final_checkpoint['model_state_dict'])\n",
        "\n",
        "else:\n",
        "    n_plot = nb_curriculum #<= nb_curriculum\n",
        "    final_checkpoint = torch.load(f'boundary_pinn_curriculum_{n_plot}_lbfgs.pt')\n",
        "    model.load_state_dict(final_checkpoint['model_state_dict'])\n",
        "\n",
        "#evaluate the model on a uniform grid\n",
        "n_points = 150\n",
        "tt = np.linspace(-1, 1, n_points) * radius\n",
        "xx, yy = np.meshgrid(tt, tt)  # create unit square grid\n",
        "xx, yy = np.where(xx**2 + yy**2 <= radius**2, xx, 0), np.where(xx**2 + yy**2 <= radius**2 , yy, 0) #(https://stackoverflow.com/questions/15733530/)\n",
        "#zz_true = true_solution_vectorized(xx,yy)\n",
        "\n",
        "input = torch.from_numpy(np.vstack((xx.ravel(),yy.ravel())).T).double()#.requires_grad_(False)\n",
        "learned_sol = model(input)\n",
        "\n",
        "#plot\n",
        "learned_sol_np = learned_sol.detach().numpy().reshape(xx.shape)\n",
        "#learned_sol_smooth = ndimage.gaussian_filter(learned_sol_np, sigma=0.1, order=0) #smoothing for visualization\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(cx, cy,'k-', alpha=.2)\n",
        "contour = ax.contourf(xx, yy, learned_sol_np, levels=200)\n",
        "ax.set_title(f'Boundary PINN solution: Curriculum {n_plot}/{nb_curriculum} (interrupt={interrupt})')\n",
        "cb = fig.colorbar(contour, ax=ax)"
      ],
      "metadata": {
        "id": "KcwmkdaoHXgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verify that the model is zero on the boundary\n",
        "circle = torch.from_numpy(np.vstack((cx.ravel(),cy.ravel())).T).double().requires_grad_(False)\n",
        "torch.sum(model(circle)**2) #== 0"
      ],
      "metadata": {
        "id": "LzSvYg2oXAXq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}